<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head><meta name="generator" content="Hexo 3.8.0">

    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="google-site-verification" content="Mxm9E-j9Gv7WuZQYUJ6BytlCEOcioYX-OQuCqPdBp6I">

    <!--Description-->
    
        <meta name="description" content="簡介
這次作業主要實作以下演算法：

k-Nearest Neighbor (kNN)
Support Vector Machine (SVM)
Softmax classifier
Two-Layer Neural Network
Higher Level Representations: Image Features">
    

    <!--Author-->
    
        <meta name="author" content="Qoo">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="CS231n assignment 1">
    

    <!--Open Graph Description-->
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="Qoo&#39;s Blog">

    <!--Type page-->
    
        <meta property="og:type" content="article">
    

    <!--Page Cover-->
    

        <meta name="twitter:card" content="summary">
    

    <!-- Title -->
    
    <title>CS231n assignment 1 - Qoo&#39;s Blog</title>

    <!-- Bootstrap Core CSS -->
    <link href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/style.css">

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet">

    <!-- Google Analytics -->
    


    <!-- favicon -->
    
    <link rel="icon" href="/img/favicon.ico"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    
	
</head>


<body>

    <!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- Menu -->
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Qoo's Blog</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                
                    <li>
                        <a href="/">
                            
                                Home
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/archives">
                            
                                Archives
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/tags">
                            
                                Tags
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/categories">
                            
                                Categories
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="https://github.com/p61402">
                            
                                <i class="fa fa-github fa-stack-2x"></i>
                            
                        </a>
                    </li>
                
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>

    <!-- Main Content -->
    <!-- Page Header -->
<!-- Set your background image for this header in your post front-matter: cover -->

<header class="intro-header" style="background-image: url('/img/banner.jpg')">
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <h1>CS231n assignment 1</h1>
                    
                    <span class="meta">
                        <!-- Date and Author -->
                        
                        
                            2019-04-20
                        
                    </span>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

            <!-- Tags and categories -->
           
                <div class="col-lg-4 col-lg-offset-2 col-md-5 col-md-offset-1 post-tags">
                    
                        


<a href="/tags/Deep-Learning/">#Deep Learning</a> <a href="/tags/Computer-Vision/">#Computer Vision</a>


                    
                </div>
                <div class="col-lg-4 col-md-5 post-categories">
                    
                        

<a href="/categories/course/">學校課程</a>/ <a href="/categories/course/圖像辨識/">圖像辨識</a>

                    
                </div>
            

            <!-- Gallery -->
            

            <!-- Post Main Content -->
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <h1 id="簡介">簡介</h1>
<p>這次作業主要實作以下演算法：</p>
<ul>
<li>k-Nearest Neighbor (kNN)</li>
<li>Support Vector Machine (SVM)</li>
<li>Softmax classifier</li>
<li>Two-Layer Neural Network</li>
<li>Higher Level Representations: Image Features</li>
</ul>
<a id="more"></a>
<h1 id="k-nearest-neighbor-knn">k-Nearest Neighbor (kNN)</h1>
<p>在<code>knn.ipynb</code>中已經將資料載入完成，使用 CIFAR-10 圖片集中的 5000 筆當作訓練，500 筆當作測試。每張圖片的大小都是 (32, 32, 3)，3 代表 RGB 三個通道。</p>
<figure>
<img src="/images/CS231n_assignment_1/cifar10.png" alt="cifar-10 資料集的10種類別"><figcaption>cifar-10 資料集的10種類別</figcaption>
</figure>
<p>我們要實作三個版本的 kNN，分別是使用雙迴圈、單迴圈、無迴圈的版本，實作的程式碼在<code>cs231n/classifiers/k_nearest_neighbor.py</code>。</p>
<h2 id="compute_distances_two_loops">compute_distances_two_loops</h2>
<p>首先是雙迴圈的版本，<code>X</code>是輸入的 test data，大小為 (num_test, D)，輸出<code>dists</code>為一個大小為 (num_test, num_train) 的 numpy array，元素<code>dists[i,j]</code>代表第 i 個 test data point 與第 j 個 train data point 的歐幾里得距離。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_distances_two_loops</span><span class="params">(self, X)</span>:</span></span><br><span class="line">    num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">    num_train = self.X_train.shape[<span class="number">0</span>]</span><br><span class="line">    dists = np.zeros((num_test, num_train))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_test):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(num_train):</span><br><span class="line">            dists[i][j] = np.sqrt(np.sum((X[i] - self.X_train[j]) ** <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dists</span><br></pre></td></tr></table></figure>
<h2 id="predict_labels">predict_labels</h2>
<p>接著實作函式<code>predict_labels</code>，先找出前 k 個與測試資料最接近的點，再透過 majority vote 的方式選出最有可能的類別。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_labels</span><span class="params">(self, dists, k=<span class="number">1</span>)</span>:</span></span><br><span class="line">    num_test = dists.shape[<span class="number">0</span>]</span><br><span class="line">    y_pred = np.zeros(num_test)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_test):</span><br><span class="line">        <span class="comment"># A list of length k storing the labels of the k nearest neighbors to</span></span><br><span class="line">        <span class="comment"># the ith test point.</span></span><br><span class="line">        closest_y = []</span><br><span class="line"></span><br><span class="line">        indices = np.argsort(dists[i])[:k]</span><br><span class="line">        closest_y = self.y_train[indices]</span><br><span class="line"></span><br><span class="line">        y_count = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> closest_y:</span><br><span class="line">            y_count[y] = y_count.get(y, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">        max_value = max(y_count.values())</span><br><span class="line">        candidates = [y <span class="keyword">for</span> y, v <span class="keyword">in</span> y_count.items() <span class="keyword">if</span> v == max_value]</span><br><span class="line">        y_pred[i] = min(candidates)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> y_pred</span><br></pre></td></tr></table></figure>
<p>下半部其實可以用一行程式碼解決：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_pred[i] = np.bincount(closest_y).argmax()</span><br></pre></td></tr></table></figure>
<p>利用剛剛得到的<code>dists</code>，可以計算出 test data 的預測結果，再將預測結果與正確答案比較就可以算出準確率。</p>
<p>k = 1 時的準確率： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Got 137 / 500 correct =&gt; accuracy: 0.274000</span><br></pre></td></tr></table></figure></p>
<p>k = 5 時的準確率： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Got 139 / 500 correct =&gt; accuracy: 0.278000</span><br></pre></td></tr></table></figure></p>
<h2 id="compute_distances_one_loop">compute_distances_one_loop</h2>
<p>接著實作單迴圈版本的 kNN：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_distances_one_loop</span><span class="params">(self, X)</span>:</span></span><br><span class="line">    num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">    num_train = self.X_train.shape[<span class="number">0</span>]</span><br><span class="line">    dists = np.zeros((num_test, num_train))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_test):</span><br><span class="line">        dists[i] = np.sqrt(np.sum((X[i] - self.X_train) ** <span class="number">2</span>, axis=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dists</span><br></pre></td></tr></table></figure>
<h2 id="compute_distances_no_loops">compute_distances_no_loops</h2>
<p>無迴圈的概念就是將兩點間的距離以平方差公式展開：<span class="math inline">\((x-y)^2=x^2+y^2-2xy\)</span>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_distances_no_loops</span><span class="params">(self, X)</span>:</span></span><br><span class="line">    num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">    num_train = self.X_train.shape[<span class="number">0</span>]</span><br><span class="line">    dists = np.zeros((num_test, num_train))</span><br><span class="line">    dists += np.sqrt(np.sum(self.X_train ** <span class="number">2</span>, axis=<span class="number">1</span>) + np.sum(X ** <span class="number">2</span>, axis=<span class="number">1</span>)[:, np.newaxis] \</span><br><span class="line">        - <span class="number">2</span> * np.dot(X, self.X_train.T))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dists</span><br></pre></td></tr></table></figure>
<p>執行<code>time_function</code>可以觀察到無迴圈的版本執行速度，遠遠將其他兩個版本甩在後頭。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Two loop version took 29.963571 seconds</span><br><span class="line">One loop version took 23.200013 seconds</span><br><span class="line">No loop version took 0.140244 seconds</span><br></pre></td></tr></table></figure>
<h2 id="cross-validation-交叉驗證">Cross Validation (交叉驗證)</h2>
<p>實作交叉驗證來進行 hyperparameter 的搜索，要找的超參數為 k 值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">num_folds = <span class="number">5</span></span><br><span class="line">k_choices = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">15</span>, <span class="number">20</span>, <span class="number">50</span>, <span class="number">100</span>]</span><br><span class="line"></span><br><span class="line">X_train_folds = []</span><br><span class="line">y_train_folds = []</span><br><span class="line"></span><br><span class="line">X_train_folds = np.array_split(X_train, num_folds)</span><br><span class="line">y_train_folds = np.array_split(y_train, num_folds)</span><br><span class="line"></span><br><span class="line">k_to_accuracies = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> k_choices:</span><br><span class="line">    k_to_accuracies[k] = []</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> range(num_folds):</span><br><span class="line">        classifier.train(np.concatenate(X_train_folds[:n] + X_train_folds[n+<span class="number">1</span>:]), np.concatenate(y_train_folds[:n] + y_train_folds[n+<span class="number">1</span>:]))</span><br><span class="line">        pred = classifier.predict(X_train_folds[n], k=k)</span><br><span class="line">        num_correct = np.sum(pred == y_train_folds[n])</span><br><span class="line">        k_to_accuracies[k].append(float(num_correct) / len(pred))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print out the computed accuracies</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> sorted(k_to_accuracies):</span><br><span class="line">    <span class="keyword">for</span> accuracy <span class="keyword">in</span> k_to_accuracies[k]:</span><br><span class="line">        print(<span class="string">'k = %d, accuracy = %f'</span> % (k, accuracy))</span><br></pre></td></tr></table></figure>
<p>執行此段程式碼會輸出每個 k 值在每個 fold 的表現。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">k = 1, accuracy = 0.263000</span><br><span class="line">k = 1, accuracy = 0.257000</span><br><span class="line">k = 1, accuracy = 0.264000</span><br><span class="line">k = 1, accuracy = 0.278000</span><br><span class="line">k = 1, accuracy = 0.266000</span><br><span class="line">k = 3, accuracy = 0.239000</span><br><span class="line">k = 3, accuracy = 0.249000</span><br><span class="line">k = 3, accuracy = 0.240000</span><br><span class="line">...</span><br><span class="line">k = 100, accuracy = 0.256000</span><br><span class="line">k = 100, accuracy = 0.270000</span><br><span class="line">k = 100, accuracy = 0.263000</span><br><span class="line">k = 100, accuracy = 0.256000</span><br><span class="line">k = 100, accuracy = 0.263000</span><br></pre></td></tr></table></figure>
<h2 id="視覺化">視覺化</h2>
<p>將結果視覺化，可以觀察到在 k = 10 的時候會在訓練集得到最好的準確率。</p>
<figure>
<img src="/images/CS231n_assignment_1/cv.png" alt="cross validation result"><figcaption>cross validation result</figcaption>
</figure>
<p>將 k 值設為 10 之後，在測試集的準確率確實有提升一些。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Got 141 / 500 correct =&gt; accuracy: 0.282000</span><br></pre></td></tr></table></figure>
<h1 id="support-vector-machine-svm">Support Vector Machine (SVM)</h1>
<h2 id="svm_loss_naive">svm_loss_naive</h2>
<p>首先實作迴圈版本的 svm loss function。</p>
<p>輸入的資料維度是 D，總共有 C 種類別，每個 minibatch 有 N 筆資料。</p>
<p>參數<code>W</code>是大小為 (D,C) 的權重，<code>X</code>是大小為 (N,D) 的 minibatch data，<code>y</code>大小為 (N,1) 代表 training labels。</p>
<p>SVM 的 loss function 如下：</p>
<p><span class="math inline">\(L_i=\sum_{j\neq y_i}max(0,s_j-s_{y_i}+\Delta)\)</span></p>
<p>Loss function 的 gradient 如下：</p>
<p><span class="math inline">\(\nabla_{w_{y_i}}L_i=-(\sum_{j\neq y_i} 1(w_j^Tx_i-w_{y_i}^Tx_i+\Delta&gt;0))x_i\)</span></p>
<p><span class="math inline">\(\nabla_{w_j}L_i=1(w_j^Tx_i-w_{y_i}^Tx_i+\Delta&gt;0)x_i\)</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">svm_loss_naive</span><span class="params">(W, X, y, reg)</span>:</span></span><br><span class="line">    dW = np.zeros(W.shape) <span class="comment"># initialize the gradient as zero</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the loss and the gradient</span></span><br><span class="line">    num_classes = W.shape[<span class="number">1</span>]</span><br><span class="line">    num_train = X.shape[<span class="number">0</span>]</span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_train):</span><br><span class="line">        scores = X[i].dot(W)</span><br><span class="line">        correct_class_score = scores[y[i]]</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(num_classes):</span><br><span class="line">            <span class="keyword">if</span> j == y[i]:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            margin = scores[j] - correct_class_score + <span class="number">1</span> <span class="comment"># note delta = 1</span></span><br><span class="line">            <span class="keyword">if</span> margin &gt; <span class="number">0</span>:</span><br><span class="line">                loss += margin</span><br><span class="line">                dW[:, j] += X[i]</span><br><span class="line">                dW[:, y[i]] -= X[i]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Right now the loss is a sum over all training examples, but we want it</span></span><br><span class="line">    <span class="comment"># to be an average instead so we divide by num_train.</span></span><br><span class="line">    loss /= num_train</span><br><span class="line">    dW /= num_train</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add regularization to the loss.</span></span><br><span class="line">    loss += reg * np.sum(W * W)</span><br><span class="line">    dW += reg * W</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, dW</span><br></pre></td></tr></table></figure>
<h2 id="svm_loss_vectorized">svm_loss_vectorized</h2>
<p>利用<code>numpy</code> vectorized 的計算方式提升運算速度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">svm_loss_vectorized</span><span class="params">(W, X, y, reg)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Structured SVM loss function, vectorized implementation.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs and outputs are the same as svm_loss_naive.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    dW = np.zeros(W.shape) <span class="comment"># initialize the gradient as zero</span></span><br><span class="line"></span><br><span class="line">    num_train = X.shape[<span class="number">0</span>]</span><br><span class="line">    scores = X.dot(W)</span><br><span class="line">    correct_class_score = scores[np.arange(num_train), y].reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">    margin = scores - correct_class_score + <span class="number">1</span></span><br><span class="line">    margin[np.arange(num_train), y] = <span class="number">0</span></span><br><span class="line">    loss += margin[margin &gt; <span class="number">0</span>].sum() / num_train</span><br><span class="line">    loss += reg * np.sum(W * W)</span><br><span class="line"></span><br><span class="line">    counts = (margin &gt; <span class="number">0</span>).astype(int)</span><br><span class="line">    counts[range(num_train), y] = - np.sum(counts, axis = <span class="number">1</span>)</span><br><span class="line">    dW += np.dot(X.T, counts) / num_train + reg * W</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, dW</span><br></pre></td></tr></table></figure>
<h2 id="stochastic-gradient-descent-sgd">Stochastic Gradient Descent (SGD)</h2>
<p><code>cs331n/linear_classifier.py</code></p>
<p>隨機梯度下降，每次更新<code>W</code>時只利用一部分的資料來計算 loss 及 gradient，能夠減少運算量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, X, y, learning_rate=<span class="number">1e-3</span>, reg=<span class="number">1e-5</span>, num_iters=<span class="number">100</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">            batch_size=<span class="number">200</span>, verbose=False)</span>:</span></span><br><span class="line">    num_train, dim = X.shape</span><br><span class="line">    num_classes = np.max(y) + <span class="number">1</span> <span class="comment"># assume y takes values 0...K-1 where K is number of classes</span></span><br><span class="line">    <span class="keyword">if</span> self.W <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="comment"># lazily initialize W</span></span><br><span class="line">        self.W = <span class="number">0.001</span> * np.random.randn(dim, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Run stochastic gradient descent to optimize W</span></span><br><span class="line">    loss_history = []</span><br><span class="line">    <span class="keyword">for</span> it <span class="keyword">in</span> range(num_iters):</span><br><span class="line">        X_batch = <span class="keyword">None</span></span><br><span class="line">        y_batch = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">        idx = np.random.choice(np.arange(num_train), batch_size)</span><br><span class="line">        X_batch, y_batch = X[idx], y[idx]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># evaluate loss and gradient</span></span><br><span class="line">        loss, grad = self.loss(X_batch, y_batch, reg)</span><br><span class="line">        loss_history.append(loss)</span><br><span class="line"></span><br><span class="line">        self.W -= learning_rate * grad</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> verbose <span class="keyword">and</span> it % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'iteration %d / %d: loss %f'</span> % (it, num_iters, loss))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss_history</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">    y_pred = np.zeros(X.shape[<span class="number">0</span>])</span><br><span class="line">    y_pred = X.dot(self.W).argmax(axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> y_pred</span><br></pre></td></tr></table></figure>
<h2 id="parameter-tuning">Parameter Tuning</h2>
<p>使用 Grid Search 的方法尋找超參數。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">learning_rates = [<span class="number">1e-7</span>, <span class="number">5e-5</span>]</span><br><span class="line">regularization_strengths = [<span class="number">2.5e4</span>, <span class="number">5e4</span>]</span><br><span class="line"></span><br><span class="line">results = &#123;&#125;</span><br><span class="line">best_val = <span class="number">-1</span>   <span class="comment"># The highest validation accuracy that we have seen so far.</span></span><br><span class="line">best_svm = <span class="keyword">None</span> <span class="comment"># The LinearSVM object that achieved the highest validation rate.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> lr <span class="keyword">in</span> learning_rates:</span><br><span class="line">    <span class="keyword">for</span> reg <span class="keyword">in</span> regularization_strengths:</span><br><span class="line">        print(<span class="string">"hyperparameter tuning: lr=&#123;&#125;, reg=&#123;&#125;"</span>.format(lr, reg))</span><br><span class="line">        svm = LinearSVM()</span><br><span class="line">        tic = time.time()</span><br><span class="line">        loss_hist = svm.train(X_train, y_train, learning_rate=lr, reg=reg,</span><br><span class="line">                              num_iters=<span class="number">1500</span>, verbose=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">        y_train_pred = svm.predict(X_train)</span><br><span class="line">        train_acc = np.mean(y_train == y_train_pred)</span><br><span class="line">        y_val_pred = svm.predict(X_val)</span><br><span class="line">        val_acc = np.mean(y_val == y_val_pred)</span><br><span class="line"></span><br><span class="line">        results[(lr, reg)] = (train_acc, val_acc)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> val_acc &gt; best_val:</span><br><span class="line">            best_val = val_acc</span><br><span class="line">            best_svm = svm</span><br><span class="line"></span><br><span class="line">        print(<span class="string">'-'</span>*<span class="number">40</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Print out results.</span></span><br><span class="line"><span class="keyword">for</span> lr, reg <span class="keyword">in</span> sorted(results):</span><br><span class="line">    train_accuracy, val_accuracy = results[(lr, reg)]</span><br><span class="line">    print(<span class="string">'lr %e reg %e train accuracy: %f val accuracy: %f'</span> % (</span><br><span class="line">                lr, reg, train_accuracy, val_accuracy))</span><br><span class="line">    </span><br><span class="line">print(<span class="string">'best validation accuracy achieved during cross-validation: %f'</span> % best_val)</span><br></pre></td></tr></table></figure>
<h2 id="將學習到的權重視覺化">將學習到的權重視覺化</h2>
<figure>
<img src="/images/CS231n_assignment_1/learned_weight.png" alt="learned_weight"><figcaption>learned_weight</figcaption>
</figure>
<h1 id="softmax-classifier">Softmax classifier</h1>
<p>與 svm 相同都是要實作兩種方法：</p>
<h2 id="softmax_loss_naive">softmax_loss_naive</h2>
<p>模型的<code>W</code>,<code>X</code>,<code>y</code>都與 SVM 相同，唯一不同的點是 softmax classifier 使用的 loss function 不是 hinge loss，而是 cross-entropy loss：</p>
<p><span class="math inline">\(L_i=-log(\dfrac{e^{f_{y_i}}}{\sum_j e^{f_j}})\)</span></p>
<p>也可以寫成</p>
<p><span class="math inline">\(L_i＝-f_{y_i}+log\sum_j e^{f_j}\)</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_naive</span><span class="params">(W, X, y, reg)</span>:</span></span><br><span class="line">    <span class="comment"># Initialize the loss and gradient to zero.</span></span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    dW = np.zeros_like(W)</span><br><span class="line"></span><br><span class="line">    num_classes = W.shape[<span class="number">1</span>]</span><br><span class="line">    num_train = X.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_train):</span><br><span class="line">        scores = X[i].dot(W)</span><br><span class="line">        scores -= np.max(scores)</span><br><span class="line">        correct_class_score = scores[y[i]]</span><br><span class="line">        exp_sum = np.sum(np.exp(scores))</span><br><span class="line">        loss += -np.log(np.exp(correct_class_score) / exp_sum)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(num_classes):</span><br><span class="line">            dW[:, j] += (np.exp(scores[j]) / exp_sum - (y[i] == j)) * X[i]</span><br><span class="line"></span><br><span class="line">    loss /= num_train</span><br><span class="line">    dW /= num_train</span><br><span class="line"></span><br><span class="line">    loss += reg * np.sum(W * W)</span><br><span class="line">    dW += reg * W</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, dW</span><br></pre></td></tr></table></figure>
<h2 id="softmax_loss_vectorized">softmax_loss_vectorized</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_vectorized</span><span class="params">(W, X, y, reg)</span>:</span></span><br><span class="line">    <span class="comment"># Initialize the loss and gradient to zero.</span></span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    dW = np.zeros_like(W)</span><br><span class="line"></span><br><span class="line">    num_train = X.shape[<span class="number">0</span>]</span><br><span class="line">    scores = X.dot(W)</span><br><span class="line">    scores -= np.max(scores, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">    prob = np.exp(scores) / np.sum(np.exp(scores), axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">    loss += np.sum(-np.log(prob[np.arange(num_train), y]))</span><br><span class="line"></span><br><span class="line">    ind = np.zeros_like(prob)</span><br><span class="line">    ind[np.arange(num_train), y] = <span class="number">1</span></span><br><span class="line">    dW = X.T.dot(prob - ind)</span><br><span class="line"></span><br><span class="line">    loss = loss / num_train + reg * np.sum(W * W)</span><br><span class="line">    dW = dW / num_train + reg * W</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, dW</span><br></pre></td></tr></table></figure>
<p>接下來也是實作 Grid Search 搜尋超參數，與 SVM 的部份相同，就不再贅述。</p>
<h1 id="two-layer-neural-network">Two-Layer Neural Network</h1>
<h2 id="fordward-pass">Fordward Pass</h2>
<p>實作一個 two-layer 的 NN，使用 ReLU nonlinearity。</p>
<p>計算方法如下：</p>
<p><span class="math inline">\(h_1=ReLU(X\cdot W_1+b_1)\)</span></p>
<p><span class="math inline">\(S=h_1\cdot W_2+b_2\)</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(self, X, y=None, reg=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">    <span class="comment"># Unpack variables from the params dictionary</span></span><br><span class="line">    W1, b1 = self.params[<span class="string">'W1'</span>], self.params[<span class="string">'b1'</span>]</span><br><span class="line">    W2, b2 = self.params[<span class="string">'W2'</span>], self.params[<span class="string">'b2'</span>]</span><br><span class="line">    N, D = X.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the forward pass</span></span><br><span class="line">    scores = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    relu = <span class="keyword">lambda</span> x: np.maximum(<span class="number">0</span>, x)</span><br><span class="line">    h1 = relu(X.dot(W1) + b1)</span><br><span class="line">    scores = h1.dot(W2) + b2</span><br><span class="line"></span><br><span class="line">    <span class="comment"># If the targets are not given then jump out, we're done</span></span><br><span class="line">    <span class="keyword">if</span> y <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">return</span> scores</span><br></pre></td></tr></table></figure>
<h2 id="compute-loss">Compute Loss</h2>
<p>使用 softmax classifier loss：<span class="math inline">\(L_i=-log(\dfrac{e^{f_{y_i}}}{\sum_j e^{f_j}})\)</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Compute the loss</span></span><br><span class="line">loss = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">scores_ = scores - np.max(scores, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">prob = np.exp(scores_) / np.sum(np.exp(scores_), axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">loss = np.sum(-np.log(prob[np.arange(N), y]))</span><br><span class="line">loss = loss / N + <span class="number">0.5</span> * reg * (np.sum(W1 * W1) + np.sum(W2 * W2))</span><br></pre></td></tr></table></figure>
<h2 id="backpropagation">Backpropagation</h2>
<p>計算 Loss 對每個參數的偏微分：<span class="math inline">\(\dfrac{\partial L}{\partial W_2},\dfrac{\partial L}{\partial b_2},\dfrac{\partial L}{\partial W_1},\dfrac{\partial L}{\partial b_1}\)</span></p>
<p>以下進行四個偏微分的推導：</p>
<p><span class="math inline">\(\mathbf{\dfrac{\partial L}{\partial W_2}}=\dfrac{\partial L}{\partial S}\dfrac{\partial S}{\partial W_2}=dS\cdot h_1^\top\)</span></p>
<p>而 <span class="math inline">\(dS\)</span> 為 softmax function 對 score 的偏微分：</p>
<p><span class="math inline">\(dS=\begin{cases} \dfrac{e^{s_i}}{\sum_j e^{s_j}} - 1, &amp; j=y_i \\ \dfrac{e^{s_i}}{\sum_j e^{s_j}}, &amp; j\neq y_i \end{cases}\)</span></p>
<p><span class="math inline">\(\mathbf{\dfrac{\partial L}{\partial b_2}}=\dfrac{\partial L}{\partial S}\dfrac{\partial S}{\partial b_2}=dS\)</span></p>
<p><span class="math inline">\(\mathbf{\dfrac{\partial L}{\partial W_1}}=\dfrac{\partial L}{\partial S}\dfrac{\partial S}{\partial h_1}\dfrac{\partial h_1}{\partial W_1}=dS\cdot W_2^\top\cdot \dfrac{\partial h_1}{\partial W_1}\)</span></p>
<p>而 <span class="math inline">\(\dfrac{\partial h_1}{\partial W_1}=\begin{cases} 0, &amp; h_1=0 \\ x^\top, &amp; h_1 &gt; 0 \end{cases}\)</span></p>
<p>因為 <span class="math inline">\(ReLU(x)=max(0,x),\dfrac{\partial ReLU}{\partial x}=\begin{cases} 0, &amp; x&lt;0 \\ 1, &amp; x&gt;0 \end{cases}\)</span></p>
<p><span class="math inline">\(\mathbf{\dfrac{\partial L}{\partial b_1}}=\dfrac{\partial L}{\partial S}\dfrac{\partial S}{\partial h_1}\dfrac{\partial h_1}{\partial b_1}=dS\cdot W_2^\top\cdot\dfrac{\partial h_1}{\partial b_1}\)</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Backward pass: compute gradients</span></span><br><span class="line">grads = &#123;&#125;</span><br><span class="line"></span><br><span class="line">dS = prob</span><br><span class="line">dS[np.arange(N), y] -= <span class="number">1</span></span><br><span class="line">dS /= N</span><br><span class="line"></span><br><span class="line">grads[<span class="string">'W2'</span>] = h1.T.dot(dS) + reg * W2</span><br><span class="line">grads[<span class="string">'b2'</span>] = np.sum(dS, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">dh1 = dS.dot(W2.T) * (h1 &gt; <span class="number">0</span>)</span><br><span class="line">grads[<span class="string">'W1'</span>] = X.T.dot(dh1) + reg * W1</span><br><span class="line">grads[<span class="string">'b1'</span>] = np.sum(dh1, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> loss, grads</span><br></pre></td></tr></table></figure>
<h1 id="higher-level-representations-image-features">Higher Level Representations: Image Features</h1>
<p>計算圖片的 Histogram of Oriented Gradients (HOG) 當作 feature，丟進模型做訓練。這部分的程式碼都已經先寫好了，我們只要 Tuning 模型的 Hyperparameter 使準確率到預期的值即可。</p>


                
            </div>

            <!-- Comments -->
            
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    


                </div>
            
        </div>
    </div>
</article>

    <!-- Footer -->
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

<hr>
<!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                    

                    

                    

                    

                    

                    
                    <span id="busuanzi_container_site_uv">
                    <i class="fa fa-user"> 訪客數: </i><span id="busuanzi_value_site_uv"></span>
                    </span>
                    <span id="busuanzi_container_page_pv">
                    <i class="fa fa-eye"> 閱讀次數: </i><span id="busuanzi_value_page_pv"></span>
                    </span>
                </ul>
                <p class="copyright text-muted">&copy; 2019 Qoo<br></p>
                <p class="copyright text-muted">Original Theme <a target="_blank" href="http://startbootstrap.com/template-overviews/clean-blog/">Clean Blog</a> from <a href="http://startbootstrap.com/" target="_blank">Start Bootstrap</a></p>
                <p class="copyright text-muted">Adapted for <a target="_blank" href="https://hexo.io/">Hexo</a> by <a href="http://www.codeblocq.com/" target="_blank">Jonathan Klughertz</a></p>
            </div>
        </div>
    </div>
</footer>


    <!-- After footer scripts -->
    
<!-- jQuery -->
<script src="//code.jquery.com/jquery-2.1.4.min.js"></script>

<!-- Bootstrap -->
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>

<!-- Gallery -->
<script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Disqus Comments -->



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>

</html>