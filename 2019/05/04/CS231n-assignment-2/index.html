<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>

    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="google-site-verification" content="Mxm9E-j9Gv7WuZQYUJ6BytlCEOcioYX-OQuCqPdBp6I" />

    <!--Description-->
    
        <meta name="description" content="簡介
這次的作業相較於第一次作業又更深入了一些，這次作業要依序實作 Fully Connected Network、Batch Normalization、Dropout、Convolutional Neural Network 等方法，並將裡頭的步驟模組化。">
    

    <!--Author-->
    
        <meta name="author" content="Qoo">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="CS231n assignment 2"/>
    

    <!--Open Graph Description-->
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="Qoo&#39;s Blog"/>

    <!--Type page-->
    
        <meta property="og:type" content="article" />
    

    <!--Page Cover-->
    

        <meta name="twitter:card" content="summary" />
    

    <!-- Title -->
    
    <title>CS231n assignment 2 - Qoo&#39;s Blog</title>

    <!-- Bootstrap Core CSS -->
    <link href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" rel="stylesheet"/>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/style.css">

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet" />

    <!-- Google Analytics -->
    


    <!-- favicon -->
    
    <link rel="icon" href="/img/favicon.ico"/><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    
	
</head>


<body>

    <!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- Menu -->
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Qoo's Blog</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                
                    <li>
                        <a href="/">
                            
                                Home
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/archives">
                            
                                Archives
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/tags">
                            
                                Tags
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="/categories">
                            
                                Categories
                            
                        </a>
                    </li>
                
                    <li>
                        <a href="https://github.com/p61402" target="_blank" rel="noopener">
                            
                                <i class="fa fa-github fa-stack-2x"></i>
                            
                        </a>
                    </li>
                
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>

    <!-- Main Content -->
    <!-- Page Header -->
<!-- Set your background image for this header in your post front-matter: cover -->

<header class="intro-header" style="background-image: url('/img/banner.jpg')">
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <h1>CS231n assignment 2</h1>
                    
                    <span class="meta">
                        <!-- Date and Author -->
                        
                        
                            2019-05-04
                        
                    </span>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

            <!-- Tags and categories -->
           
                <div class="col-lg-4 col-lg-offset-2 col-md-5 col-md-offset-1 post-tags">
                    
                        


<a href="/tags/Deep-Learning/">#Deep Learning</a> <a href="/tags/Computer-Vision/">#Computer Vision</a>


                    
                </div>
                <div class="col-lg-4 col-md-5 post-categories">
                    
                        

<a href="/categories/course/">學校課程</a>/ <a href="/categories/course/圖像辨識/">圖像辨識</a>

                    
                </div>
            

            <!-- Gallery -->
            

            <!-- Post Main Content -->
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <h1>簡介</h1>
<p>這次的作業相較於第一次作業又更深入了一些，這次作業要依序實作 Fully Connected Network、Batch Normalization、Dropout、Convolutional Neural Network 等方法，並將裡頭的步驟模組化。</p>
<a id="more"></a>
<h1>Fully Connected Network</h1>
<h2 id="modular-network"><a class="header-anchor" href="#modular-network">¶</a>Modular network</h2>
<p>首先要將 FCN 的 forward pass 以及 backward 模組化，包括 affine layer 與 ReLU activation。<br>
在 forward pass 時使用<code>cache</code>將所需變數儲存，以便 backward pass 的時候使用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">affine_forward</span><span class="params">(x, w, b)</span>:</span></span><br><span class="line">    out = <span class="literal">None</span></span><br><span class="line">    out = x.reshape(x.shape[<span class="number">0</span>], <span class="number">-1</span>).dot(w) + b</span><br><span class="line">    cache = (x, w, b)</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">affine_backward</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    x, w, b = cache</span><br><span class="line">    dx, dw, db = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    dx = dout.dot(w.T).reshape(x.shape)</span><br><span class="line">    dw = x.reshape(x.shape[<span class="number">0</span>], <span class="number">-1</span>).T.dot(dout)</span><br><span class="line">    db = np.sum(dout, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dx, dw, db</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu_forward</span><span class="params">(x)</span>:</span></span><br><span class="line">    out = <span class="literal">None</span></span><br><span class="line">    out = np.maximum(x, <span class="number">0</span>)</span><br><span class="line">    cache = x</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu_backward</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    dx, x = <span class="literal">None</span>, cache</span><br><span class="line">    dx = dout * (x &gt; <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure>
<p>利用剛剛完成的模組來建構<code>TwoLayerNet</code>以及可以自訂 size 的<code>FullyConnectedNet</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TwoLayerNet</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim=<span class="number">3</span>*<span class="number">32</span>*<span class="number">32</span>, hidden_dim=<span class="number">100</span>, num_classes=<span class="number">10</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 weight_scale=<span class="number">1e-3</span>, reg=<span class="number">0.0</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">        self.params = &#123;&#125;</span><br><span class="line">        self.reg = reg</span><br><span class="line"></span><br><span class="line">        self.params[<span class="string">'W1'</span>] = np.random.normal(<span class="number">0</span>, weight_scale, (input_dim, hidden_dim))</span><br><span class="line">        self.params[<span class="string">'b1'</span>] = np.zeros((hidden_dim, ))</span><br><span class="line">        self.params[<span class="string">'W2'</span>] = np.random.normal(<span class="number">0</span>, weight_scale, (hidden_dim, num_classes))</span><br><span class="line">        self.params[<span class="string">'b2'</span>] = np.zeros((num_classes, ))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        scores = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        h1, cache_h1 = affine_relu_forward(X, self.params[<span class="string">'W1'</span>], self.params[<span class="string">'b1'</span>])</span><br><span class="line">        scores, cache_scores = affine_forward(h1, self.params[<span class="string">'W2'</span>], self.params[<span class="string">'b2'</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># If y is None then we are in test mode so just return scores</span></span><br><span class="line">        <span class="keyword">if</span> y <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> scores</span><br><span class="line"></span><br><span class="line">        loss, grads = <span class="number">0</span>, &#123;&#125;</span><br><span class="line"></span><br><span class="line">        loss, dS = softmax_loss(scores, y)</span><br><span class="line">        dh1, grads[<span class="string">'W2'</span>], grads[<span class="string">'b2'</span>] = affine_backward(dS, cache_scores)</span><br><span class="line">        dx, grads[<span class="string">'W1'</span>], grads[<span class="string">'b1'</span>] = affine_relu_backward(dh1, cache_h1)</span><br><span class="line"></span><br><span class="line">        loss += <span class="number">0.5</span> * self.reg * (np.sum(self.params[<span class="string">'W2'</span>] ** <span class="number">2</span>) + np.sum(self.params[<span class="string">'W1'</span>] ** <span class="number">2</span>))</span><br><span class="line">        grads[<span class="string">'W1'</span>] += self.reg * self.params[<span class="string">'W1'</span>]</span><br><span class="line">        grads[<span class="string">'W2'</span>] += self.reg * self.params[<span class="string">'W2'</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss, grads</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FullyConnectedNet</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hidden_dims, input_dim=<span class="number">3</span>*<span class="number">32</span>*<span class="number">32</span>, num_classes=<span class="number">10</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout=<span class="number">1</span>, normalization=None, reg=<span class="number">0.0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 weight_scale=<span class="number">1e-2</span>, dtype=np.float32, seed=None)</span>:</span></span><br><span class="line"></span><br><span class="line">        self.normalization = normalization</span><br><span class="line">        self.use_dropout = dropout != <span class="number">1</span></span><br><span class="line">        self.reg = reg</span><br><span class="line">        self.num_layers = <span class="number">1</span> + len(hidden_dims)</span><br><span class="line">        self.dtype = dtype</span><br><span class="line">        self.params = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        self.params[<span class="string">'W1'</span>] = np.random.normal(<span class="number">0</span>, weight_scale, (input_dim, hidden_dims[<span class="number">0</span>]))</span><br><span class="line">        self.params[<span class="string">'b1'</span>] = np.zeros((hidden_dims[<span class="number">0</span>], ))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, self.num_layers - <span class="number">1</span>):</span><br><span class="line">            self.params[<span class="string">'W'</span>+str(i+<span class="number">1</span>)] = np.random.normal(<span class="number">0</span>, weight_scale, (hidden_dims[i<span class="number">-1</span>], hidden_dims[i]))</span><br><span class="line">            self.params[<span class="string">'b'</span>+str(i+<span class="number">1</span>)] = np.zeros((hidden_dims[i], ))</span><br><span class="line"></span><br><span class="line">        self.params[<span class="string">'W'</span>+str(self.num_layers)] = np.random.normal(<span class="number">0</span>, weight_scale, (hidden_dims[<span class="number">-1</span>], num_classes))</span><br><span class="line">        self.params[<span class="string">'b'</span>+str(self.num_layers)] = np.zeros((num_classes, ))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># When using dropout we need to pass a dropout_param dictionary to each</span></span><br><span class="line">        <span class="comment"># dropout layer so that the layer knows the dropout probability and the mode</span></span><br><span class="line">        <span class="comment"># (train / test). You can pass the same dropout_param to each dropout layer.</span></span><br><span class="line">        self.dropout_param = &#123;&#125;</span><br><span class="line">        <span class="keyword">if</span> self.use_dropout:</span><br><span class="line">            self.dropout_param = &#123;<span class="string">'mode'</span>: <span class="string">'train'</span>, <span class="string">'p'</span>: dropout&#125;</span><br><span class="line">            <span class="keyword">if</span> seed <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                self.dropout_param[<span class="string">'seed'</span>] = seed</span><br><span class="line"></span><br><span class="line">        <span class="comment"># With batch normalization we need to keep track of running means and</span></span><br><span class="line">        <span class="comment"># variances, so we need to pass a special bn_param object to each batch</span></span><br><span class="line">        <span class="comment"># normalization layer. You should pass self.bn_params[0] to the forward pass</span></span><br><span class="line">        <span class="comment"># of the first batch normalization layer, self.bn_params[1] to the forward</span></span><br><span class="line">        <span class="comment"># pass of the second batch normalization layer, etc.</span></span><br><span class="line">        self.bn_params = []</span><br><span class="line">        <span class="keyword">if</span> self.normalization==<span class="string">'batchnorm'</span>:</span><br><span class="line">            self.bn_params = [&#123;<span class="string">'mode'</span>: <span class="string">'train'</span>&#125; <span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_layers - <span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">if</span> self.normalization==<span class="string">'layernorm'</span>:</span><br><span class="line">            self.bn_params = [&#123;&#125; <span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_layers - <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Cast all parameters to the correct datatype</span></span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> self.params.items():</span><br><span class="line">            self.params[k] = v.astype(dtype)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        X = X.astype(self.dtype)</span><br><span class="line">        mode = <span class="string">'test'</span> <span class="keyword">if</span> y <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="string">'train'</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set train/test mode for batchnorm params and dropout param since they</span></span><br><span class="line">        <span class="comment"># behave differently during training and testing.</span></span><br><span class="line">        <span class="keyword">if</span> self.use_dropout:</span><br><span class="line">            self.dropout_param[<span class="string">'mode'</span>] = mode</span><br><span class="line">        <span class="keyword">if</span> self.normalization==<span class="string">'batchnorm'</span>:</span><br><span class="line">            <span class="keyword">for</span> bn_param <span class="keyword">in</span> self.bn_params:</span><br><span class="line">                bn_param[<span class="string">'mode'</span>] = mode</span><br><span class="line">        scores = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        h, cache = [<span class="literal">None</span>] * (self.num_layers + <span class="number">1</span>), [<span class="literal">None</span>] * (self.num_layers + <span class="number">1</span>)</span><br><span class="line">        h[<span class="number">0</span>] = X</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_layers - <span class="number">1</span>):</span><br><span class="line">            W, b = self.params[<span class="string">'W'</span>+str(i+<span class="number">1</span>)], self.params[<span class="string">'b'</span>+str(i+<span class="number">1</span>)]</span><br><span class="line">            h[i+<span class="number">1</span>], cache[i+<span class="number">1</span>] = affine_relu_forward(h[i], W, b)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">            W, b = self.params[<span class="string">'W'</span>+str(i+<span class="number">1</span>)], self.params[<span class="string">'b'</span>+str(i+<span class="number">1</span>)]</span><br><span class="line">            h[i+<span class="number">1</span>], cache[i+<span class="number">1</span>] = affine_forward(h[i], W, b)</span><br><span class="line"></span><br><span class="line">        scores = h[<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># If test mode return early</span></span><br><span class="line">        <span class="keyword">if</span> mode == <span class="string">'test'</span>:</span><br><span class="line">            <span class="keyword">return</span> scores</span><br><span class="line"></span><br><span class="line">        loss, grads = <span class="number">0.0</span>, &#123;&#125;</span><br><span class="line"></span><br><span class="line">        loss, dS = softmax_loss(scores, y)</span><br><span class="line">        dh = [<span class="literal">None</span>] * (self.num_layers + <span class="number">1</span>)</span><br><span class="line">        dh[<span class="number">-1</span>] = dS</span><br><span class="line"></span><br><span class="line">        i = self.num_layers</span><br><span class="line">        dh[i<span class="number">-1</span>], grads[<span class="string">'W'</span>+str(i)], grads[<span class="string">'b'</span>+str(i)] = affine_backward(dh[i], cache[i])</span><br><span class="line">        loss += <span class="number">0.5</span> * self.reg * np.sum(self.params[<span class="string">'W'</span>+str(i)] ** <span class="number">2</span>)</span><br><span class="line">        grads[<span class="string">'W'</span>+str(i)] += <span class="number">0.5</span> * self.reg * (self.params[<span class="string">'W'</span>+str(i)] ** <span class="number">2</span>)</span><br><span class="line">        i -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> i &gt; <span class="number">0</span>:</span><br><span class="line">            dh[i<span class="number">-1</span>], grads[<span class="string">'W'</span>+str(i)], grads[<span class="string">'b'</span>+str(i)] = affine_relu_backward(dh[i], cache[i])</span><br><span class="line">            loss += <span class="number">0.5</span> * self.reg * np.sum(self.params[<span class="string">'W'</span>+str(i)] ** <span class="number">2</span>)</span><br><span class="line">            grads[<span class="string">'W'</span>+str(i)] += self.reg * self.params[<span class="string">'W'</span>+str(i)]</span><br><span class="line">            i -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss, grads</span><br></pre></td></tr></table></figure>
<h2 id="sgd-momentum"><a class="header-anchor" href="#sgd-momentum">¶</a>SGD + Momentum</h2>
<p>原始的 Stochastic Gradient Descent：</p>
<p>$x_{t+1}=x_t-\alpha\nabla f(x_t)$</p>
<p>SGD + Momumtum：</p>
<p>$v_{t+1}=\rho v_t-\alpha\nabla f(x_t) \ x_{t+1}=x_t+v_{t+1}$</p>
<p>$v$ 代表目前的方向速度，初始值為 0，如果負梯度與目前方向相同，則速度會越來越快，參數的更新幅度就會變大；反之則越來越慢，參數的更新幅度會變小。</p>
<p>至於 $\rho$ 則是一個 hyperparameter，通常設在 0.9 左右。</p>
<p>使用 SGD + Momentum 通常比 Vanilla SGD 能夠更快收斂。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd_momentum</span><span class="params">(w, dw, config=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> config <span class="keyword">is</span> <span class="literal">None</span>: config = &#123;&#125;</span><br><span class="line">    config.setdefault(<span class="string">'learning_rate'</span>, <span class="number">1e-2</span>)</span><br><span class="line">    config.setdefault(<span class="string">'momentum'</span>, <span class="number">0.9</span>)</span><br><span class="line">    v = config.get(<span class="string">'velocity'</span>, np.zeros_like(w))</span><br><span class="line"></span><br><span class="line">    next_w = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    v = config[<span class="string">'momentum'</span>] * v - config[<span class="string">'learning_rate'</span>] * dw</span><br><span class="line">    next_w = w + v</span><br><span class="line"></span><br><span class="line">    config[<span class="string">'velocity'</span>] = v</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> next_w, config</span><br></pre></td></tr></table></figure>
<h2 id="rmsprop"><a class="header-anchor" href="#rmsprop">¶</a>RMSProp</h2>
<p>$v_t=\rho v_{t-1}+(1-\rho) \times (\nabla f(x_t))^2$</p>
<p>$\Delta x_t=-\dfrac{\alpha}{\sqrt{v_t+\epsilon}} \times \nabla f(x_t)$</p>
<p>$x_{t+1}=x_t+\Delta x_t$</p>
<p>$\rho$ 為 decay rate，通常設為 0.9、0.99、0.999。</p>
<p>$\epsilon$ 是一個很小的值，為了避免除以 0 的情況產生。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rmsprop</span><span class="params">(w, dw, config=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> config <span class="keyword">is</span> <span class="literal">None</span>: config = &#123;&#125;</span><br><span class="line">    config.setdefault(<span class="string">'learning_rate'</span>, <span class="number">1e-2</span>)</span><br><span class="line">    config.setdefault(<span class="string">'decay_rate'</span>, <span class="number">0.99</span>)</span><br><span class="line">    config.setdefault(<span class="string">'epsilon'</span>, <span class="number">1e-8</span>)</span><br><span class="line">    config.setdefault(<span class="string">'cache'</span>, np.zeros_like(w))</span><br><span class="line"></span><br><span class="line">    next_w = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    config[<span class="string">'cache'</span>] = config[<span class="string">'decay_rate'</span>] * config[<span class="string">'cache'</span>] + (<span class="number">1</span> - config[<span class="string">'decay_rate'</span>]) * (dw ** <span class="number">2</span>)</span><br><span class="line">    next_w = w + -config[<span class="string">'learning_rate'</span>] * dw / np.sqrt(config[<span class="string">'cache'</span>] + config[<span class="string">'epsilon'</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> next_w, config</span><br></pre></td></tr></table></figure>
<h2 id="adam"><a class="header-anchor" href="#adam">¶</a>Adam</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adam</span><span class="params">(w, dw, config=None)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> config <span class="keyword">is</span> <span class="literal">None</span>: config = &#123;&#125;</span><br><span class="line">    config.setdefault(<span class="string">'learning_rate'</span>, <span class="number">1e-3</span>)</span><br><span class="line">    config.setdefault(<span class="string">'beta1'</span>, <span class="number">0.9</span>)</span><br><span class="line">    config.setdefault(<span class="string">'beta2'</span>, <span class="number">0.999</span>)</span><br><span class="line">    config.setdefault(<span class="string">'epsilon'</span>, <span class="number">1e-8</span>)</span><br><span class="line">    config.setdefault(<span class="string">'m'</span>, np.zeros_like(w))</span><br><span class="line">    config.setdefault(<span class="string">'v'</span>, np.zeros_like(w))</span><br><span class="line">    config.setdefault(<span class="string">'t'</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    next_w = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    config[<span class="string">'t'</span>] += <span class="number">1</span></span><br><span class="line">    config[<span class="string">'m'</span>] = config[<span class="string">'beta1'</span>] * config[<span class="string">'m'</span>] + (<span class="number">1</span> - config[<span class="string">'beta1'</span>]) * dw</span><br><span class="line">    mt = config[<span class="string">'m'</span>] / (<span class="number">1</span> - config[<span class="string">'beta1'</span>] ** config[<span class="string">'t'</span>])</span><br><span class="line">    config[<span class="string">'v'</span>] = config[<span class="string">'beta2'</span>] * config[<span class="string">'v'</span>] + (<span class="number">1</span> - config[<span class="string">'beta2'</span>]) * (dw ** <span class="number">2</span>)</span><br><span class="line">    vt = config[<span class="string">'v'</span>] / (<span class="number">1</span> - config[<span class="string">'beta2'</span>] ** config[<span class="string">'t'</span>])</span><br><span class="line">    next_w = w + -config[<span class="string">'learning_rate'</span>] * mt / (np.sqrt(vt) + config[<span class="string">'epsilon'</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> next_w, config</span><br></pre></td></tr></table></figure>
<p>比較不同 optimizer 的表現：</p>
<p><img src="/images/CS231n_assignment_2/optimizer_comparison.png" alt="Optimizer Comparison"></p>
<h1>Batch Normalization</h1>
<p>實作過程參考以下論文：</p>
<p><a href="https://arxiv.org/abs/1502.03167" target="_blank" rel="noopener">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></p>
<p>batch normalization 的目的是為了讓每一層的輸出都保持高斯分佈，主要的目的是為了避免 gradient vanishing。做法是將 fordward pass 時用來訓練的批次資料計算 mean 以及 variance，利用 mini-batch 的 mean 及 vairance 來更新整體的 mean 及 variance。</p>
<h2 id="forward-pass"><a class="header-anchor" href="#forward-pass">¶</a>Forward Pass</h2>
<p>論文中具體的實作方法如下：</p>
<p><img src="/images/CS231n_assignment_2/batch_normalization.png" alt="batch_normalization"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_forward</span><span class="params">(x, gamma, beta, bn_param)</span>:</span></span><br><span class="line">    mode = bn_param[<span class="string">'mode'</span>]</span><br><span class="line">    eps = bn_param.get(<span class="string">'eps'</span>, <span class="number">1e-5</span>)</span><br><span class="line">    momentum = bn_param.get(<span class="string">'momentum'</span>, <span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">    N, D = x.shape</span><br><span class="line">    running_mean = bn_param.get(<span class="string">'running_mean'</span>, np.zeros(D, dtype=x.dtype))</span><br><span class="line">    running_var = bn_param.get(<span class="string">'running_var'</span>, np.zeros(D, dtype=x.dtype))</span><br><span class="line"></span><br><span class="line">    out, cache = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">'train'</span>:</span><br><span class="line">        sample_mean = x.mean(axis=<span class="number">0</span>)</span><br><span class="line">        sample_var = x.var(axis=<span class="number">0</span>)</span><br><span class="line">        sqrtvar = np.sqrt(sample_var + eps)</span><br><span class="line">        xmu = x - sample_mean</span><br><span class="line">        ivar = <span class="number">1.</span>/sqrtvar</span><br><span class="line">        x_hat = xmu * ivar</span><br><span class="line">        out = gamma * x_hat + beta</span><br><span class="line"></span><br><span class="line">        cache = (xmu, sample_var, ivar, sqrtvar, x_hat, gamma, eps)</span><br><span class="line"></span><br><span class="line">        running_mean = momentum * running_mean + (<span class="number">1</span> - momentum) * sample_mean</span><br><span class="line">        running_var = momentum * running_var + (<span class="number">1</span> - momentum) * sample_var</span><br><span class="line">    <span class="keyword">elif</span> mode == <span class="string">'test'</span>:</span><br><span class="line">        x_hat = (x - running_mean) / np.sqrt(running_var + eps)</span><br><span class="line">        out = gamma * x_hat + beta</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">'Invalid forward batchnorm mode "%s"'</span> % mode)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Store the updated running means back into bn_param</span></span><br><span class="line">    bn_param[<span class="string">'running_mean'</span>] = running_mean</span><br><span class="line">    bn_param[<span class="string">'running_var'</span>] = running_var</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure>
<h2 id="backward-pass"><a class="header-anchor" href="#backward-pass">¶</a>Backward Pass</h2>
<p>論文中對於計算 BN 的反向傳播也有一些描述：</p>
<p><img src="/images/CS231n_assignment_2/BN_backward.png" alt="BN backward"></p>
<p>真的是滿複雜的，最好還是自己畫過一次計算圖之後再試著去計算 backward pass，這部分的話<a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html" target="_blank" rel="noopener">這篇文章</a>寫得滿不錯的，可以參考一下。</p>
<p><img src="/images/CS231n_assignment_2/BN_comp_graph.png" alt="自己畫的計算圖"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_backward</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    dx, dgamma, dbeta = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    N, D = dout.shape</span><br><span class="line">    xmu, var, ivar, sqrtvar, x_hat, gamma, eps = cache</span><br><span class="line"></span><br><span class="line">    dbeta = np.sum(dout, axis=<span class="number">0</span>)</span><br><span class="line">    dgamma_x = dout</span><br><span class="line">    dgamma = np.sum(dgamma_x * x_hat, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    dx_hat = dgamma_x * gamma</span><br><span class="line">    divar = np.sum(dx_hat * xmu, axis=<span class="number">0</span>)</span><br><span class="line">    dx_mu1 = dx_hat * ivar</span><br><span class="line"></span><br><span class="line">    dsqrtvar = -divar / (sqrtvar ** <span class="number">2</span>)</span><br><span class="line">    dvar = <span class="number">0.5</span> * dsqrtvar / np.sqrt(var + eps)</span><br><span class="line">    dsq = dvar * np.ones((N, D)) / N</span><br><span class="line"></span><br><span class="line">    dx_mu2 = <span class="number">2</span> * xmu * dsq</span><br><span class="line"></span><br><span class="line">    dx1 = dx_mu1 + dx_mu2</span><br><span class="line">    dmu = -np.sum(dx_mu1 + dx_mu2, axis=<span class="number">0</span>)</span><br><span class="line">    dx2 = dmu * np.ones((N, D)) / N</span><br><span class="line"></span><br><span class="line">    dx = dx1 + dx2</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dx, dgamma, dbeta</span><br></pre></td></tr></table></figure>
<p>簡化版：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_backward_alt</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    dx, dgamma, dbeta = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    xmu, var, ivar, sqrtvar, x_hat, gamma, eps = cache</span><br><span class="line">    N, D = dout.shape</span><br><span class="line"></span><br><span class="line">    dbeta = np.sum(dout, axis=<span class="number">0</span>)</span><br><span class="line">    dgamma = np.sum(x_hat * dout, axis=<span class="number">0</span>)</span><br><span class="line">    dx = (gamma * ivar / N) * (N * dout - x_hat * dgamma - dbeta)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dx, dgamma, dbeta</span><br></pre></td></tr></table></figure>
<h2 id="layer-normalization"><a class="header-anchor" href="#layer-normalization">¶</a>Layer Normalization</h2>
<p>batch normalization 使得類神經網路的訓練更有效率，但是對於複雜的網路結構來說，在 batch size 不夠大的時候效果可能不會太好。因此另一個方法是對 feature 進行 normalize，參考論文：<a href="https://arxiv.org/pdf/1607.06450.pdf" target="_blank" rel="noopener">Layer Normalization</a>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layernorm_forward</span><span class="params">(x, gamma, beta, ln_param)</span>:</span></span><br><span class="line">    out, cache = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    eps = ln_param.get(<span class="string">'eps'</span>, <span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line">    x_T = x.T</span><br><span class="line">    sample_mean = np.mean(x_T, axis=<span class="number">0</span>)</span><br><span class="line">    sample_var = np.var(x_T, axis=<span class="number">0</span>)</span><br><span class="line">    x_norm_T = (x_T - sample_mean) / np.sqrt(sample_var + eps)</span><br><span class="line">    x_norm = x_norm_T.T</span><br><span class="line">    out = x_norm * gamma + beta</span><br><span class="line">    cache = (x, x_norm, gamma, sample_mean, sample_var, eps)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layernorm_backward</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    dx, dgamma, dbeta = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    x, x_norm, gamma, sample_mean, sample_var, eps = cache</span><br><span class="line">    x_T = x.T</span><br><span class="line">    dout_T = dout.T</span><br><span class="line">    N = x_T.shape[<span class="number">0</span>]</span><br><span class="line">    dbeta = np.sum(dout, axis=<span class="number">0</span>)</span><br><span class="line">    dgamma = np.sum(x_norm * dout, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    dx_norm = dout_T * gamma[:,np.newaxis]</span><br><span class="line">    dv = ((x_T - sample_mean) * <span class="number">-0.5</span> * (sample_var + eps)**<span class="number">-1.5</span> * dx_norm).sum(axis=<span class="number">0</span>)</span><br><span class="line">    dm = (dx_norm * <span class="number">-1</span> * (sample_var + eps)**<span class="number">-0.5</span>).sum(axis=<span class="number">0</span>) + (dv * (x_T - sample_mean) * <span class="number">-2</span> / N).sum(axis=<span class="number">0</span>)</span><br><span class="line">    dx_T = dx_norm / (sample_var + eps)**<span class="number">0.5</span> + dv * <span class="number">2</span> * (x_T - sample_mean) / N + dm / N</span><br><span class="line">    dx = dx_T.T</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dx, dgamma, dbeta</span><br></pre></td></tr></table></figure>
<h1>Dropout</h1>
<p><a href="http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf" target="_blank" rel="noopener">Dropout: A Simple Way to Prevent Neural Networks from<br>
Overfitting</a></p>
<p>drouput 是一種正規化的方法，在 forward pass 時隨機將某些 neuron 的值丟掉，跟 L1, L2 regularization 一樣，目的都是為了避免 overfitting。</p>
<p><img src="/images/CS231n_assignment_2/dropout.png" alt="dropout"></p>
<p>實作方法是在 training 時根據一個機率 p 來隨機產生一個 mask (值為 True or False)，將 x 乘上 mask 就可以將部分 neuron 的值設為 0， predicting 的時候就直接將 x 乘上 p。</p>
<p>但與其在 predicting 時乘上 p，其實我們可以在 training 的時候就除以 p，這樣就可以減少 predicting 的計算量，因為我們通常比較在意 predicting 時的效率，這個技巧稱為 <strong>inverted dropout</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout_forward</span><span class="params">(x, dropout_param)</span>:</span></span><br><span class="line">    p, mode = dropout_param[<span class="string">'p'</span>], dropout_param[<span class="string">'mode'</span>]</span><br><span class="line">    <span class="keyword">if</span> <span class="string">'seed'</span> <span class="keyword">in</span> dropout_param:</span><br><span class="line">        np.random.seed(dropout_param[<span class="string">'seed'</span>])</span><br><span class="line"></span><br><span class="line">    mask = <span class="literal">None</span></span><br><span class="line">    out = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">'train'</span>:</span><br><span class="line">        mask = (np.random.rand(*x.shape) &lt; p) / p</span><br><span class="line">        out = x * mask</span><br><span class="line">    <span class="keyword">elif</span> mode == <span class="string">'test'</span>:</span><br><span class="line">        out = x</span><br><span class="line"></span><br><span class="line">    cache = (dropout_param, mask)</span><br><span class="line">    out = out.astype(x.dtype, copy=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout_backward</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    dropout_param, mask = cache</span><br><span class="line">    mode = dropout_param[<span class="string">'mode'</span>]</span><br><span class="line"></span><br><span class="line">    dx = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">'train'</span>:</span><br><span class="line">        dx = dout * mask</span><br><span class="line">    <span class="keyword">elif</span> mode == <span class="string">'test'</span>:</span><br><span class="line">        dx = dout</span><br><span class="line">    <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure>
<h1>Convolutional Neural Network</h1>
<h2 id="convolution-layer-forward-pass"><a class="header-anchor" href="#convolution-layer-forward-pass">¶</a>Convolution Layer Forward Pass</h2>
<p>實作 CNN 的 forward pass，輸入 $x$ 的大小為 $(N,C,H,W)$，以及 $F$ 個 filter，合起來成為一個 $(F,C,HH,WW)$ 的矩陣，經過 convolution 的計算後，輸出一個 $(N,F,H<sup>\prime,W</sup>\prime)$ 的矩陣。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_forward_naive</span><span class="params">(x, w, b, conv_param)</span>:</span></span><br><span class="line">    out = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    N, C, H, W = x.shape</span><br><span class="line">    F, C, HH, WW = w.shape</span><br><span class="line">    pad, stride = conv_param[<span class="string">'pad'</span>], conv_param[<span class="string">'stride'</span>]</span><br><span class="line"></span><br><span class="line">    x_pad = np.pad(x, ((<span class="number">0</span>,), (<span class="number">0</span>,), (pad,), (pad,)), <span class="string">'constant'</span>, constant_values=<span class="number">0</span>)</span><br><span class="line">    H_prime = <span class="number">1</span> + (H + <span class="number">2</span> * pad - HH) // stride</span><br><span class="line">    W_prime = <span class="number">1</span> + (W + <span class="number">2</span> * pad - WW) // stride</span><br><span class="line"></span><br><span class="line">    out = np.empty((N, F, H_prime, W_prime))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> range(F):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(H_prime):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(W_prime):</span><br><span class="line">                out[:, f, i, j] = np.sum(x_pad[:, :, i*stride:i*stride+HH, j*stride:j*stride+WW] * w[f], axis=(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">    out += b.reshape(F, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    cache = (x, w, b, conv_param)</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure>
<h2 id="convolution-layer-backward-pass"><a class="header-anchor" href="#convolution-layer-backward-pass">¶</a>Convolution Layer Backward Pass</h2>
<p>計算 convolution layer 的 backpropagation 可以參考<a href="https://becominghuman.ai/back-propagation-in-convolutional-neural-networks-intuition-and-code-714ef1c38199" target="_blank" rel="noopener">這篇文章</a>。因為 forward 時的計算也算是 $x$ 乘上 $w$，因此 backward 時計算 $dx$ 就是用 $dout$ 與 $w$ 做計算；計算 $dw$ 時則是用 $dout$ 與 $x$ 做計算，雖然概念上不難理解，但是要透過<code>numpy</code>實作的話對維度要有一定的掌握才行。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_backward_naive</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    dx, dw, db = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    x, w, b, conv_param = cache</span><br><span class="line"></span><br><span class="line">    N, C, H, W = x.shape</span><br><span class="line">    F, C, HH, WW = w.shape</span><br><span class="line">    pad, stride = conv_param[<span class="string">'pad'</span>], conv_param[<span class="string">'stride'</span>]</span><br><span class="line"></span><br><span class="line">    x_pad = np.pad(x, ((<span class="number">0</span>,), (<span class="number">0</span>,), (pad,), (pad,)), mode=<span class="string">'constant'</span>, constant_values=<span class="number">0</span>)</span><br><span class="line">    H_prime = <span class="number">1</span> + (H + <span class="number">2</span> * pad - HH) // stride</span><br><span class="line">    W_prime = <span class="number">1</span> + (W + <span class="number">2</span> * pad - WW) // stride</span><br><span class="line"></span><br><span class="line">    dx = np.zeros_like(x)</span><br><span class="line">    dx_pad = np.zeros_like(x_pad)</span><br><span class="line">    dw = np.zeros_like(w)</span><br><span class="line">    db = np.sum(dout, axis=(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(H_prime):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(W_prime):</span><br><span class="line">            <span class="keyword">for</span> f <span class="keyword">in</span> range(F):</span><br><span class="line">                dw[f] += np.sum(dout[:, f, i, j].reshape(<span class="number">-1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>) * x_pad[:, :, i*stride:i*stride+HH, j*stride:j*stride+WW], axis=<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">for</span> n <span class="keyword">in</span> range(N):</span><br><span class="line">                dx_pad[n, :, i*stride:i*stride+HH, j*stride:j*stride+WW] += np.sum(w * dout[n, :, i, j].reshape(<span class="number">-1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>), axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    dx = dx_pad[:, :, pad:-pad, pad:-pad]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dx, dw, db</span><br></pre></td></tr></table></figure>
<h2 id="max-pooling-forward-pass"><a class="header-anchor" href="#max-pooling-forward-pass">¶</a>Max Pooling Forward Pass</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_forward_naive</span><span class="params">(x, pool_param)</span>:</span></span><br><span class="line">    out = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    N, C, H, W = x.shape</span><br><span class="line">    pool_height, pool_width, stride = pool_param[<span class="string">'pool_height'</span>], pool_param[<span class="string">'pool_width'</span>], pool_param[<span class="string">'stride'</span>]</span><br><span class="line"></span><br><span class="line">    H_prime = <span class="number">1</span> + (H - pool_height) // stride</span><br><span class="line">    W_prime = <span class="number">1</span> + (W - pool_width) // stride</span><br><span class="line">    out = np.empty((N, C, H_prime, W_prime))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(H_prime):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(W_prime):</span><br><span class="line">                out[:, :, i, j] = np.max(x[:, :, i*stride:i*stride+pool_height, j*stride:j*stride+pool_width], axis=(<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">    cache = (x, pool_param)</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure>
<h2 id="max-pooling-backward-pass"><a class="header-anchor" href="#max-pooling-backward-pass">¶</a>Max Pooling Backward Pass</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_backward_naive</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    dx = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    x, pool_param = cache</span><br><span class="line">    N, C, H, W = x.shape</span><br><span class="line">    pool_height, pool_width, stride = pool_param[<span class="string">'pool_height'</span>], pool_param[<span class="string">'pool_width'</span>], pool_param[<span class="string">'stride'</span>]</span><br><span class="line"></span><br><span class="line">    H_prime = <span class="number">1</span> + (H - pool_height) // stride</span><br><span class="line">    W_prime = <span class="number">1</span> + (W - pool_width) // stride</span><br><span class="line">    dx = np.zeros_like(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(H_prime):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(W_prime):</span><br><span class="line">            arg = np.max(x[:, :, i*stride:i*stride+pool_height, j*stride:j*stride+pool_width], axis=(<span class="number">2</span>,<span class="number">3</span>), keepdims=<span class="literal">True</span>) == \</span><br><span class="line">                x[:, :, i*stride:i*stride+pool_height, j*stride:j*stride+pool_width]</span><br><span class="line">            dx[:, :, i*stride:i*stride+pool_height, j*stride:j*stride+pool_width] += arg * dout[:, :, i, j][:,:,np.newaxis,np.newaxis]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure>
<p>最後還有實作 Spatial Batch Normalization 以及 Group Normalization，但這部分不是很熟所以略過。</p>


                
            </div>

            <!-- Comments -->
            
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    


                </div>
            
        </div>
    </div>
</article>

    <!-- Footer -->
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

<hr />
<!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                    

                    

                    

                    

                    

                    
                    <span id="busuanzi_container_site_uv">
                    <i class="fa fa-user"> 訪客數: </i><span id="busuanzi_value_site_uv"></span>
                    </span>
                    <span id="busuanzi_container_page_pv">
                    <i class="fa fa-eye"> 閱讀次數: </i><span id="busuanzi_value_page_pv"></span>
                    </span>
                </ul>
                <p class="copyright text-muted">&copy; 2019 Qoo<br></p>
                <p class="copyright text-muted">Original Theme <a target="_blank" href="http://startbootstrap.com/template-overviews/clean-blog/">Clean Blog</a> from <a href="http://startbootstrap.com/" target="_blank">Start Bootstrap</a></p>
                <p class="copyright text-muted">Adapted for <a target="_blank" href="https://hexo.io/">Hexo</a> by <a href="http://www.codeblocq.com/" target="_blank">Jonathan Klughertz</a></p>
            </div>
        </div>
    </div>
</footer>


    <!-- After footer scripts -->
    
<!-- jQuery -->
<script src="//code.jquery.com/jquery-2.1.4.min.js"></script>

<!-- Bootstrap -->
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>

<!-- Gallery -->
<script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Disqus Comments -->



<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>

</html>