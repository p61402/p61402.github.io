<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[機器學習經典演算法實作 - Linear Regression]]></title>
    <url>%2F2019%2F06%2F12%2F%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E7%B6%93%E5%85%B8%E6%BC%94%E7%AE%97%E6%B3%95%E5%AF%A6%E4%BD%9C-Linear-Regression%2F</url>
    <content type="text"><![CDATA[Linear Regression 簡介 假設我們有 \(m\) 筆資料，每筆資料都包含 \(n\) 個特徵，記為 \(x\)，每筆資料都有 1 個正確答案，記為 \(y\)。若給我們一筆新的資料，我們希望預測這筆資料的正確答案。 \(x\) 與 \(y\) 之間可能存在著某種關聯性 \(f\)，使得 \(y=f(x)\)，但我們不知道這個 \(f\) 是什麼，因此我們希望利用這些現存的資料找出一個 \(h(x)\)，使得 \(h\) 越接近 \(f\) 越好。 Model Representation Linear Regression (線性迴歸) 就是找出一條直線，使得這條線能夠盡可能的擬合所有的訓練資料。 \[h_\theta(x)=\theta_0+\theta_1 x_1+\theta_2 x_2+...+\theta_n x_n\] \(\theta\) 是這條直線的係數，也是我們要想辦法計算出來的，這個模型的參數。 那麼假設我們找到了一組參數，我們要如何保證在給定這組參數的情況下，模型能夠準確地用來表示這些資料呢？ Cost Function 我們希望資料透過這個模型的計算得出來的結果，與實際的值誤差越小越好，因此我們可以採用 Mean Square Error (MSE) 來計算誤差。 而這個誤差我們稱為 cost function (代價函數)，有些人稱作 loss function (損失函數) 或 objective function (目標函數)，其實指的都是同個意思。 下方的式子就是我們使用的 cost function： \[J(\theta)=\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2\] 這裡有一點需要注意的是，我們多除了一個 \(2\)，這是因為待會的步驟要計算這個 cost function 的微分，微分時平方項會變成 \(\times2\)，剛好可以與多除的 \(2\) 抵銷掉，因此能夠減少計算量。 Gradient Descent 既然我們有了一個 cost function 能夠衡量一組參數的好壞，那我們剩下要做的事情就是想辦法找到一組最好的參數使得 cost function 最小。 我們當然可以用暴力法一一嘗試各種參數的組合，但是這個有點方法不切實際，因為我們不知道參數的範圍在哪裡，同時這個方法的效率也相當差。 我們可以用 Gradient Descent 來逐步更新參數： \[\theta_j:=\theta_j-\alpha\frac{\partial}{\partial \theta_j}J(\theta)\] \(\dfrac{\partial}{\partial \theta_j}J(\theta)\) 是 cost function 對參數 \(\theta_j\) 的偏微分，代表這一點的梯度方向，而由於梯度方向是指向高點，而我們要尋找的是 cost function 的最小值，所以我們會在這一項前面加上一個負號。 我們將這個梯度乘上一個 learning rate \(\alpha\)，這個值通常小於 1，因為我們希望每次參數更新的幅度不要太大，才能夠緩慢的收斂至極值，不過若 learning rate 過小也容易導致收斂在 local optimal。 而 gradient 的計算如下： \[\dfrac{\partial}{\partial \theta_j}J(\theta)=\dfrac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}\] Feature Scaling 若不同 feature 的範圍差距過大，在 gradient descent 的時候收斂的速度會較慢，這時我們可以將 feature 進行正規化，例如將特徵規範在 -1 到 1 之間。 而正規化的方法有很多種，這裡我們使用的是 Mean Normalization： \[x_i=\dfrac{x_i-\mu_i}{s_i}\] 其中 \(s_i=x_{max}-x_{min}\)，結果會使得 \(-1\leq x_i\leq1\)，並且 mean = 0。 Normal Equation 除了用 gradient descent 之外，也可以使用 Normal Equation，這個方法能夠直接得出最佳的 \(\theta\) 值，這邊我們僅列出式子，不探討其背後的原理。 \[\theta=(X^TX)^{-1}X^Ty\] 實作 載入我們唯一需要用到的函式庫：numpy。 1import numpy as np 定義一個類別及所需的變數： 123456789class LinearRegression(): def __init__(self, num_iteration=100, learning_rate=1e-1, feature_scaling=True): self.num_iteration = num_iteration self.learning_rate = learning_rate self.feature_scaling = feature_scaling self.M = 0 # normalize mean self.S = 1 # normalize range self.W = None self.cost_history = np.empty(num_iteration) Gradient Descent 的方法： 為了加快運算的速度以及方便表示資料，我們會把資料向量化，也就是全部的資料都以numpy的矩陣表示。 因為 feature 的數量為 \(n\) 個，因此參數的數量會是 \(n + 1\) 個： \[\theta=\begin{bmatrix} \theta_0 \\ \theta_1 \\ \vdots \\ \theta_n \end{bmatrix}_{(n+1)\times 1} \] 但輸入資料為一個 \(m\times n\) 的矩陣： \[X=\begin{bmatrix} x_1^{(1)} &amp; x_2^{(1)} &amp; \dots &amp; x_n^{(1)} \\ x_1^{(2)} &amp; x_2^{(2)} &amp; \dots &amp; x_n^{(2)} \\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\ x_1^{(m)} &amp; x_2^{(m)} &amp; \dots &amp; x_n^{(m)} \end{bmatrix}_{m\times n} \] 因此需要新增一行全部都為 \(1\) 的 feature 與 \(\theta_0\) 做計算： \[X=\begin{bmatrix} 1 &amp; x_1^{(1)} &amp; x_2^{(1)} &amp; \dots &amp; x_n^{(1)} \\ 1 &amp; x_1^{(2)} &amp; x_2^{(2)} &amp; \dots &amp; x_n^{(2)} \\ \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\ 1 &amp; x_1^{(m)} &amp; x_2^{(m)} &amp; \dots &amp; x_n^{(m)} \end{bmatrix}_{m\times (n+1)} \] 1234567891011121314151617181920212223def fit(self, X, y): # m 為資料筆數，n 為特徵數量 if X.ndim == 1: X = X.reshape(X.shape[0], 1) m, n = X.shape # 是否進行正規化 if self.feature_scaling: X = self.normalize(X) # 在 X 左方加入一行 1 對應到參數 theta 0 X = np.hstack((np.ones((m, 1)), X)) y = y.reshape(y.shape[0], 1) self.W = np.zeros((n+1,1)) # 每個 iteration 逐步更新參數 for i in range(self.num_iteration): y_hat = X.dot(self.W) cost = self.cost_function(y_hat, y, m) self.cost_history[i] = cost self.gradient_descent(X, y_hat, y, m) 實作fit函式用到的方法： 12345678910111213def normalize(self, X): self.M = np.mean(X, axis=0) self.S = np.max(X, axis=0) - np.min(X, axis=0) return (X - self.M) / self.Sdef cost_function(self, y_hat, y, m): return 1/(2*m) * np.sum((y_hat - y)**2)def compute_gradient(self, X, y_hat, y, m): return 1/m * np.sum((y_hat - y) * X, axis=0).reshape(-1,1)def gradient_descent(self, X, y_hat, y, m): self.W -= self.learning_rate * self.compute_gradient(X, y_hat, y, m) 預測： 123456789101112def predict(self, X): if X.ndim == 1: X = X.reshape(X.shape[0], 1) m, n = X.shape if self.normalize: X = (X - self.M) / self.S X = np.hstack((np.ones((m, 1)), X)) y_hat = X.dot(self.W) return y_hat Normal Equation 的方法： 12345678910def normal_equation(self, X, y): if X.ndim == 1: X = X.reshape(X.shape[0], 1) m, n = X.shape X = np.hstack((np.ones((m, 1)), X)) y = y.reshape(y.shape[0], 1) self.W = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y) 實驗 隨機產生 30 筆單一特徵的資料： Data Distribution 不使用 feature scaling 使用 feature scaling 其實可以發現在 univariate 的 feature 上，不論使用 feature scaling 與否，都不會對結果影響太大。 使用 Normal Equation： 隨機產生 100 筆三個特徵的資料，前三個 column 為 feature，最後一個 column 為 label，y 的值透過三個特徵經過某些運算後加上一些隨機值得到： 12345654.31 3.92 -2.06 130.0334.79 3.63 -2.49 91.5687.91 2.99 -2.97 197.7515.88 3.4 -2.22 52.1496.59 2.83 -1.81 209.64... 不進行 feature scaling，則 learning rate 必須設定小一點，否則容易造成梯度爆炸。即便沒有梯度爆炸，也通常需要更多的 iteration 才會收斂。 下圖使用的 learning rate = 1e-5。 若有進行 feature scaling，則 learning rate 可以設定大一點，收斂速度會相對較快。 下圖使用的 learning rate = 0.1。]]></content>
      <categories>
        <category>機器學習</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n assignment 3]]></title>
    <url>%2F2019%2F05%2F29%2FCS231n-assignment-3%2F</url>
    <content type="text"><![CDATA[簡介 實作 RNN 並應用至 Image Captioning 實作 LSTM 並應用至 Image Captioning Neural Network 視覺化 Style Transfer 在藝術畫作上的應用 Generative Adversial Network 實作並應用在 MNIST 資料 Image Captioning with RNNs 資料集 Microsoft COCO 資料集，2014 年的版本，總共有80,000 張的訓練圖片與 40,000 張的驗證圖片，每張圖片都有 5 個 captions。 &lt;START&gt; a group of people fly their kites in a field of flowers &lt;END&gt; &lt;START&gt; a dirt road a wooden bench some grass and trees &lt;END&gt; &lt;START&gt; a bird sitting on a tree &lt;UNK&gt; in a &lt;UNK&gt; &lt;END&gt; Vanilla RNN step forward 原始 RNN 的計算方法如下： \(h_t=tanh(W_h\cdot h_{t-1}+W_x\cdot X_t+b)\) 而 \(tanh(x)=\dfrac{e^{2x}-1}{e^{2x}+1}\) 不過我這邊使用numpy自帶的tanh函數，使用上會較為穩定，試過自定義一個tanh函式，不知為何會導致 gradient exploding 的問題。 1234567def rnn_step_forward(x, prev_h, Wx, Wh, b): next_h, cache = None, None next_h = np.tanh(prev_h.dot(Wh) + x.dot(Wx) + b) cache = x, prev_h, Wx, Wh, b, next_h return next_h, cache step backward 12345678910111213def rnn_step_backward(dnext_h, cache): dx, dprev_h, dWx, dWh, db = None, None, None, None, None x, prev_h, Wx, Wh, b, next_h = cache dtanh = dnext_h * (1 - next_h ** 2) dx = dtanh.dot(Wx.T) dprev_h = dtanh.dot(Wh.T) dWx = x.T.dot(dtanh) dWh = prev_h.T.dot(dtanh) db = np.sum(dtanh, axis=0) return dx, dprev_h, dWx, dWh, db forward 每筆資料都有 T 個 timestamp，所以用迴圈依序建構每個 timestamp 的 rnn layer。 123456789101112def rnn_forward(x, h0, Wx, Wh, b): h, cache = None, None N, T, D = x.shape H = h0.shape[1] h, cache = np.empty((N, T, H)), [None] * T h[:,0,:], cache[0] = rnn_step_forward(x[:,0,:], h0, Wx, Wh, b) for i in range(1, T): h[:,i,:], cache[i] = rnn_step_forward(x[:,i,:], h[:,i-1,:], Wx, Wh, b) return h, cache backward 1234567891011121314def rnn_backward(dh, cache): dx, dh0, dWx, dWh, db = None, None, None, None, None N, T, H = dh.shape D = cache[0][0].shape[1] dx, dprev_h, dWx, dWh, db = np.empty((N, T, D)), np.zeros((N, H)), np.zeros((D, H)), np.zeros((H, H)), np.zeros((H,)) for i in range(T-1,-1,-1): dx[:,i,:], dprev_h, dWx_temp, dWh_temp, db_temp = rnn_step_backward(dh[:,i,:] + dprev_h, cache[i]) dWx, dWh, db = dWx + dWx_temp, dWh + dWh_temp, db + db_temp dh0 = dprev_h return dx, dh0, dWx, dWh, db Word Embedding forward 1234567def word_embedding_forward(x, W): out, cache = None, None out = W[x,:] cache = x, W return out, cache backward 12345678def word_embedding_backward(dout, cache): dW = None x, W = cache dW = np.zeros_like(W) np.add.at(dW, x, dout) return dW RNN for image captioning 12345678910111213141516171819202122232425262728293031323334def loss(self, features, captions): captions_in = captions[:, :-1] captions_out = captions[:, 1:] # You'll need this mask = (captions_out != self._null) # Weight and bias for the affine transform from image features to initial # hidden state W_proj, b_proj = self.params['W_proj'], self.params['b_proj'] # Word embedding matrix W_embed = self.params['W_embed'] # Input-to-hidden, hidden-to-hidden, and biases for the RNN Wx, Wh, b = self.params['Wx'], self.params['Wh'], self.params['b'] # Weight and bias for the hidden-to-vocab transformation. W_vocab, b_vocab = self.params['W_vocab'], self.params['b_vocab'] loss, grads = 0.0, &#123;&#125; feature_out, feature_cache = affine_forward(features, W_proj, b_proj) word_embed_out, word_embed_cache = word_embedding_forward(captions_in, W_embed) rnn_out, rnn_cache = rnn_forward(word_embed_out, feature_out, Wx, Wh, b) score, score_cache = temporal_affine_forward(rnn_out, W_vocab, b_vocab) loss, dout = temporal_softmax_loss(score, captions_out, mask) dscore, grads['W_vocab'], grads['b_vocab'] = temporal_affine_backward(dout, score_cache) drnn, dh0_rnn, grads['Wx'], grads['Wh'], grads['b'] = rnn_backward(dscore, rnn_cache) grads['W_embed'] = word_embedding_backward(drnn, word_embed_cache) dx_feature, grads['W_proj'], grads['b_proj'] = affine_backward(dh0_rnn, feature_cache) return loss, grads Overfit small data Test-time sampling 1234567891011121314151617181920def sample(self, features, max_length=30): N = features.shape[0] captions = self._null * np.ones((N, max_length), dtype=np.int32) # Unpack parameters W_proj, b_proj = self.params['W_proj'], self.params['b_proj'] W_embed = self.params['W_embed'] Wx, Wh, b = self.params['Wx'], self.params['Wh'], self.params['b'] W_vocab, b_vocab = self.params['W_vocab'], self.params['b_vocab'] hidden, _ = affine_forward(features, W_proj, b_proj) cur_word = self._start for n in range(max_length): word_embed, _ = word_embedding_forward(cur_word, W_embed) hidden, _ = rnn_step_forward(word_embed, hidden, Wx, Wh, b) score, _ = affine_forward(hidden, W_vocab, b_vocab) cur_word = np.argmax(score, axis=1) captions[:, n] = cur_word return captions 一些結果： Image Captioning with LSTMs LSTM step forward 修正：右方應為 elementwise multiplication 123456789101112131415def lstm_step_forward(x, prev_h, prev_c, Wx, Wh, b): next_h, next_c, cache = None, None, None N, H = prev_h.shape gates = prev_h.dot(Wh) + x.dot(Wx) + b gates[:,:3*H] = sigmoid(gates[:,:3*H]) gates[:,3*H:] = np.tanh(gates[:,3*H:]) i, f, o, g = gates[:,:H], gates[:,H:2*H], gates[:,2*H:3*H], gates[:,3*H:4*H], next_c = f * prev_c + i * g next_h = o * np.tanh(next_c) cache = x, prev_h, prev_c, Wx, Wh, b, next_h, next_c, i, f, o, g return next_h, next_c, cache step backward 1234567891011121314151617181920def lstm_step_backward(dnext_h, dnext_c, cache): dx, dprev_h, dprev_c, dWx, dWh, db = None, None, None, None, None, None x, prev_h, prev_c, Wx, Wh, b, next_h, next_c, i, f, o, g = cache dnext_c = dnext_c + o * (1 - np.tanh(next_c) ** 2) * dnext_h df = dnext_c * prev_c di = dnext_c * g dg = dnext_c * i do = dnext_h * np.tanh(next_c) dgates = np.hstack((di*i*(1-i), df*f*(1-f), do*o*(1-o), dg*(1-g**2))) dx = dgates.dot(Wx.T) dprev_h = dgates.dot(Wh.T) dprev_c = dnext_c * f dWx = x.T.dot(dgates) dWh = prev_h.T.dot(dgates) db = np.sum(dgates, axis=0) return dx, dprev_h, dprev_c, dWx, dWh, db forward 123456789101112def lstm_forward(x, h0, Wx, Wh, b): h, cache = None, None N, T, D = x.shape H = h0.shape[1] h, c, cache = np.zeros((N, T, H)), np.zeros((N, H)), [None] * T h[:,0,:], c, cache[0] = lstm_step_forward(x[:,0,:], h0, c, Wx, Wh, b) for i in range(1, T): h[:,i,:], c, cache[i] = lstm_step_forward(x[:,i,:], h[:,i-1,:], c, Wx, Wh, b) return h, cache backward 1234567891011121314def lstm_backward(dh, cache): dx, dh0, dWx, dWh, db = None, None, None, None, None N, D = cache[0][0].shape T, H = dh.shape[1:] dx, dprev_h, dWx, dWh, db, dc = np.zeros((N, T, D)), np.zeros((N, H)), np.zeros((D, 4*H)), np.zeros((H, 4*H)), np.zeros((4*H,)), np.zeros((N, H)) for i in range(T-1, -1, -1): dx[:,i,:], dprev_h, dc, dWx_temp, dWh_temp, db_temp = lstm_step_backward(dh[:,i,:] + dprev_h, dc, cache[i]) dWx, dWh, db= dWx + dWx_temp, dWh + dWh_temp, db + db_temp dh0 = dprev_h return dx, dh0, dWx, dWh, db Overfit small data LSTM test-time sampling Network Visualization Saliency Maps Saliency Map 的概念就是透過計算 \(loss\) 對 \(X\) 的梯度 \(dx\)，觀察圖片的哪一個部分對分類的影響較為顯著。 假設圖片的 channel 數為 3，Saloency Map 就是將 \(dx\) 取絕對值後，取 3 個 channel 之中的最大值。 1234567891011121314151617def compute_saliency_maps(X, y, model): saliency = None x = tf.convert_to_tensor(X, dtype=X.dtype) y = tf.convert_to_tensor(y, dtype=tf.int32) N = x.shape[0] with tf.GradientTape() as t: t.watch(x) score = model.call(x) correct_class = tf.gather_nd(score, tf.stack((tf.range(N), y), axis=1)) dx = t.gradient(score, x) dx = tf.abs(dx) saliency = tf.reduce_max(dx, axis=3) return saliency Fooling Images 既然這個梯度可以用來表示這張圖片中影響分類的關鍵部分，那麼我們是否能利用這個梯度來更新原始圖片，產生出一個看起來與原始圖片差不多，但是卻分類錯誤的圖片呢？ 因此我們使用一個錯誤的類別當作正確答案來計算 loss，並利用這個 loss 計算對於原始圖片的梯度，再用這個梯度對原始圖片進行更新，看看是否能產生一張以假亂真的圖片。 順帶一提，更新的方法稱為 Gradient Ascent。 123456789101112131415161718192021222324def make_fooling_image(X, target_y, model): # Make a copy of the input that we will modify X_fooling = X.copy() # Step size for the update learning_rate = 1 X_fooling = tf.convert_to_tensor(X_fooling) for i in range(100): with tf.GradientTape() as t: t.watch(X_fooling) score = model.call(X_fooling) pred_y = np.argmax(score[0]) loss = score[0, target_y] g = t.gradient(loss, X_fooling) dx = learning_rate * g / tf.norm(g) X_fooling += dx if pred_y == target_y: break return X_fooling Class visualization 那麼反過來說若給我們一張隨機的圖片，我們是否能夠透過同樣的方法，使得這張圖片被模型分類為某一個類別呢？ 若 \(I\) 是一張圖片，而 \(y\) 是目標類別，模型對於 \(I\) 在類別 \(y\) 的分數為 \(s_y(I)\)。 我們希望產生一張圖片 \(I^*\) 使得他在類別 \(y\) 的分數越高越好： \[ I^* = {\arg\max}_I (s_y(I) - R(I)) \] 其中 \(R\) 是一個 L2 reguralization term： \[ R(I) = \lambda \|I\|_2^2 \] 1234567with tf.GradientTape() as tape: tape.watch(X) score = model.call(X) loss = score[0, target_y]g = tape.gradient(loss, X) + l2_reg * X * XX += learning_rate * g iteration 1/100 iteration 25/100 iteration 50/100 iteration 75/100 iteration 100/100 Style Transfer 這個部分要利用 Image Style Transfer Using Convolutional Neural Networks 這篇 paper 的概念來實作 style transfer。 Loss function 為 content loss、style loss、total variation loss 三個的加權總和。 值得注意的是在這裡的 gradient descent 不是用來更新模型的參數，而是 output image 的 pixel value。 Content Loss 我們只需要計算一個 layer 的 content loss，假設這個 layer 的 feature maps \(A^\ell \in \mathbb{R}^{1 \times H_\ell \times W_\ell \times C_\ell}\)。 \(C_\ell\) 是 layer \(\ell\) 的 filter/channel 數量，\(H^\ell\) 與 \(W^\ell\) 是圖片的長跟寬。 \(F^\ell \in \mathbb{R}^{M_\ell \times C_\ell}\) 是目前圖片的 feature map，而\(P^\ell \in \mathbb{R}^{M_\ell \times C_\ell}\) 是 content 圖片的 feature map，而 \(M_\ell=H_\ell\times W_\ell\) 是每個 feature map 的 pixel 數量，最後\(w_c\)則是 content loss 這一項的權重。 content loss 定義如下： \(L_c = w_c \times \sum_{i,j} (F_{ij}^{\ell} - P_{ij}^{\ell})^2\) 12345def content_loss(content_weight, content_current, content_original): content_current = tf.reshape(content_current, [-1, tf.shape(content_current)[-1]]) content_original = tf.reshape(content_original, [-1, tf.shape(content_original)[-1]]) return content_weight * tf.reduce_sum((content_current - content_original)**2) Style Loss 建立 Gram Matrix 來表示 image 的 texture。 給定一個 feature map \(F^\ell\)，大小為 \(M_\ell \times C_\ell\)，Gram Matrix 的大小為 \(C_\ell \times C_\ell\)，定義如下： \[G_{ij}^\ell = \sum_k F^{\ell}_{ki} F^{\ell}_{kj}\] 假設 \(G^\ell\) 為由目前圖片的 feature map 所計算的 Gram Matrix，而 \(A^\ell\) 為由原始 style image 的 feature map 所計算的 Gram Matrix，\(w_\ell\) 是這一項的權重，則第 \(\ell\) 層的 style loss 可以透過計算這兩個 gram matrix 的 euclidean distance 來表示： \[L_s^\ell = w_\ell \sum_{i, j} \left(G^\ell_{ij} - A^\ell_{ij}\right)^2\] 我們通常會將每一層的 loss 相加來表示最終的 loss： \[L_s = \sum_{\ell \in \mathcal{L}} L_s^\ell\] 1234567891011def gram_matrix(features, normalize=True): _, H, W, C = tf.shape(features) features = tf.reshape(features, [-1, C]) gram = tf.matmul(tf.transpose(features), features) if normalize: gram /= tf.cast(H * W * C, tf.float32) return gram 1234567def style_loss(feats, style_layers, style_targets, style_weights): style_loss = 0 for i, layer in enumerate(style_layers): style_loss += style_weights[i] * tf.reduce_sum((gram_matrix(feats[layer]) - style_targets[i])**2) return style_loss Total-variation regularization 為了增加圖片的平滑度，我們會加入一個正規項來懲罰所謂的 wiggle 或是 total variation。 方法是計算相鄰 pixel 的平方差(水平與垂直方向，3個channel(RGB))並將所有結果相加。 \(L_{tv} = w_t \times \left(\sum_{c=1}^3\sum_{i=1}^{H-1}\sum_{j=1}^{W} (x_{i+1,j,c} - x_{i,j,c})^2 + \sum_{c=1}^3\sum_{i=1}^{H}\sum_{j=1}^{W - 1} (x_{i,j+1,c} - x_{i,j,c})^2\right)\) 1234def tv_loss(img, tv_weight): horizontal_loss = tf.reduce_sum((img[:,1:,:,:] - img[:,:-1,:,:])**2) vertical_loss = tf.reduce_sum((img[:,:,1:,:] - img[:,:,:-1,:])**2) return tv_weight * (horizontal_loss + vertical_loss) 結果 iteration 0 iteration 100 iteration 199 Generative Adversarial Networks (GANs) 生成對抗網路(GAN)由兩個 neural network 組成：Discriminator 與 Generator。 Discriminator 就是一個二元分類器，目標是判斷輸入的圖片是 real (來自 training set) 或是 fake (不來自 training set)。 Generator 的目標是產生與 training set 相似的圖片，混淆 Discriminator 使其將產生的圖片判斷為 real。 我們可以想像這是一個 Discriminator \((D)\) 與 Generator \((G)\) 來回對抗的遊戲。遊戲剛開始時，Discriminator 能夠輕易地判斷輸入圖片的真實性，但隨著遊戲的進行，Generator 漸漸地能產生一些 Discriminator 判斷錯誤的圖片，這時 Discriminator 也會慢慢進步，重複這樣的流程，最後當 Discriminator 已經無法判斷輸入圖片的真實性時，就代表 Generator 已經能夠產生非常接近真實的圖片了。 \[\underset{G}{\text{minimize}}\; \underset{D}{\text{maximize}}\; \mathbb{E}_{x \sim p_\text{data}}\left[\log D(x)\right] + \mathbb{E}_{z \sim p(z)}\left[\log \left(1-D(G(z))\right)\right]\] 我們也可以把這個流程想像為一個 minmax game，重複以下兩個步驟： Gradient descent 更新 generator \((G)\) 最小化 discriminator 做出正確決定 的機率 Gradient ascent 更新 discriminator \((D)\) 最大化 discriminator 做出正確決定 的機率 但是這種方法在實作上不太可行，因為若當 discriminator 已經變得很強的時候，generator 會產生 gradient vanishing 的問題。 因此我們通常會稍微修改下 generator 的更新方式：最大化 discriminator 做出錯誤決定的機率。 因此新的遊戲玩法如下： Gradient ascent 更新 generator \((G)\) 最大化 discriminator 做出錯誤決定 的機率 \[\underset{G}{\text{maximize}}\; \mathbb{E}_{z \sim p(z)}\left[\log D(G(z))\right]\] Gradient ascent 更新 discriminator \((D)\) 最大化 discriminator 做出正確決定 的機率 \[\underset{D}{\text{maximize}}\; \mathbb{E}_{x \sim p_\text{data}}\left[\log D(x)\right] + \mathbb{E}_{z \sim p(z)}\left[\log \left(1-D(G(z))\right)\right]\] Vallina GAN Leaky ReLU Leaky ReLU 能夠解決 ReLU 的 dying problem，常被用在 GAN 的實作中。 \[f(x)=max(\alpha x, x)\] 12def leaky_relu(x, alpha=0.01): return tf.maximum(alpha * x, x) Random Noise 隨機產生介於 \([-1,1]\) 的雜訊。 12def sample_noise(batch_size, dim): return tf.random.uniform(shape=(batch_size, dim), minval=-1, maxval=1) Discriminator 123456789def discriminator(): model = tf.keras.models.Sequential([ tf.keras.layers.Dense(256, input_shape=(784,), activation=leaky_relu), tf.keras.layers.Dense(256, activation=leaky_relu), tf.keras.layers.Dense(1) ]) return model Generator 123456789def generator(noise_dim=NOISE_DIM): model = tf.keras.models.Sequential([ tf.keras.layers.Dense(1024, input_shape=(noise_dim,), activation=tf.nn.relu), tf.keras.layers.Dense(1024, activation=tf.nn.relu), tf.keras.layers.Dense(784, activation=tf.nn.tanh) ]) return model GAN Loss Generator Loss: \[\ell_G = -\mathbb{E}_{z \sim p(z)}\left[\log D(G(z))\right]\] Discriminator loss \[ \ell_D = -\mathbb{E}_{x \sim p_\text{data}}\left[\log D(x)\right] - \mathbb{E}_{z \sim p(z)}\left[\log \left(1-D(G(z))\right)\right]\] 1234567891011121314151617181920def discriminator_loss(logits_real, logits_fake): loss = None cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True) real_loss = cross_entropy(tf.ones_like(logits_real), logits_real) fake_loss = cross_entropy(tf.zeros_like(logits_fake), logits_fake) loss = real_loss + fake_loss return lossdef generator_loss(logits_fake): loss = None cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True) loss = cross_entropy(tf.ones_like(logits_fake), logits_fake) return loss Optimizing the loss 12345678def get_solvers(learning_rate=1e-3, beta1=0.5): D_solver = None G_solver = None D_solver = tf.optimizers.Adam(learning_rate=1e-3, beta_1=0.5) G_solver = tf.optimizers.Adam(learning_rate=1e-3, beta_1=0.5) return D_solver, G_solver 結果 Least Squares GAN Least Squares GAN 不同的地方只有在 loss function 的部分。 Generator loss: \[\ell_G = \frac{1}{2}\mathbb{E}_{z \sim p(z)}\left[\left(D(G(z))-1\right)^2\right]\] Discriminator loss: \[ \ell_D = \frac{1}{2}\mathbb{E}_{x \sim p_\text{data}}\left[\left(D(x)-1\right)^2\right] + \frac{1}{2}\mathbb{E}_{z \sim p(z)}\left[ \left(D(G(z))\right)^2\right]\] 12345678910111213141516def ls_discriminator_loss(scores_real, scores_fake): loss = None real_loss = 0.5 * tf.reduce_mean((scores_real - tf.ones_like(scores_real)) ** 2) fake_loss = 0.5 * tf.reduce_mean(scores_fake ** 2) loss = real_loss + fake_loss return lossdef ls_generator_loss(scores_fake): loss = None loss = 0.5 * tf.reduce_mean((scores_fake - tf.ones_like(scores_fake)) ** 2) return loss 結果 Deep Convolutional GANs DCGAN 使用卷積神經網路來實作 Generator 與 Discriminator。 1234567891011121314151617181920def discriminator(): model = tf.keras.models.Sequential([ tf.keras.layers.Reshape((28, 28, 1), input_shape=(784, )), tf.keras.layers.Conv2D(filters=32, kernel_size=(5, 5), strides=(1, 1), padding='valid'), tf.keras.layers.LeakyReLU(alpha=0.01), tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)), tf.keras.layers.Conv2D(filters=64, kernel_size=(5, 5), strides=(1, 1), padding='valid'), tf.keras.layers.LeakyReLU(alpha=0.01), tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)), tf.keras.layers.Flatten(), tf.keras.layers.Dense(4*4*64), tf.keras.layers.LeakyReLU(alpha=0.01), tf.keras.layers.Dense(1) ]) return modelmodel = discriminator()test_discriminator(1102721) 12345678910111213141516def generator(noise_dim=NOISE_DIM): model = tf.keras.models.Sequential([ tf.keras.layers.Dense(1024, use_bias=True, input_shape=(noise_dim, ), activation='relu'), tf.keras.layers.BatchNormalization(), tf.keras.layers.Dense(7*7*128, use_bias=True, activation='relu'), tf.keras.layers.BatchNormalization(), tf.keras.layers.Reshape((7, 7, 128)), tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=(4, 4), strides=(2, 2), padding='same', activation='relu', use_bias=True), tf.keras.layers.BatchNormalization(), tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=(4, 4), strides=(2, 2), padding='same', activation='tanh', use_bias=True), ]) return modeltest_generator(6595521) 結果]]></content>
      <categories>
        <category>學校課程</category>
        <category>圖像辨識</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Computer Vision</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019暑期實習面試心得]]></title>
    <url>%2F2019%2F05%2F27%2F2019%E6%9A%91%E6%9C%9F%E5%AF%A6%E7%BF%92%E9%9D%A2%E8%A9%A6%E5%BF%83%E5%BE%97%2F</url>
    <content type="text"><![CDATA[緣起 大學的時候就很想參加暑期實習，今年終於有機會申請了！我大概是去年 12 月的時候決定要開始找實習，但後來在一下的時候進資策會實習又是另一個故事了，總之我還滿想到北部看看的(純軟公司大概有 90% 都在台北吧我猜XD)，所以還是決定在暑假的時候再找一個實習。 沒有算得很清楚，不過我總共投了大約 10 多間公司，大多為軟體開發、資料分析以及與機器學習有關的職缺。總共拿到 6 個面試機會，有完整參與整個面試流程的有 4 間，最後拿到 2 個 offer。 準備 首先最重要的就是履歷的撰寫，我有寫中文以及英文兩個版本，中文版是使用 CakeResume 撰寫，英文版則是使用 overleaf，以 latex 語法撰寫。 履歷的重點主要放在自己做過的專案及作品，強調使用的技術以及解決的成效，可以用條列式的方法寫，比較能整理出重點。學經歷的話我簡單敘述在資策會實習的貢獻，還有在大學、研究所待的實驗室做了什麼這樣。我認為履歷的內容要與應徵的職缺有符合是一個大重點，甚至可以針對不同的公司製作不同的履歷。 至於程式測驗的話就是靠平常的累積吧，大學的時候 UVa 大概寫了 200 多題，而 Leetcode 大概寫了 100 多題，雖然練得不多但有寫還是有一點差啦，看到題目會多少有一些方向。還有就是我之前有買一本 Cracking the Coding Interview 中譯本回來看，不過大概只看了一半吧。 至於資料科學、機器學習方面的領域： 學校修課：資料探勘、機器學習 線上課程：CS231n: Convolutional Neural Networks for Visual Recognition 以及 CS224n: Natural Language Processing with Deep Learning，都是 Stanford CS 的課程，Youtube 上都有影片可以看。大推 cs231n 的作業，自幹 fcn、cnn、dropout、batch normalization 等可以釐清很多觀念 比賽：打過一些 Kaggle 的比賽以及一個 T-brain 的比賽(趨勢科技辦的) 經驗：在資策會做一些跟電腦視覺有關的應用 面試 ¶趨勢科技 - 軟體開發與測試 履歷投遞：3/7 程式測驗：3/8 面試通知：3/15 現場面試：3/27 電話面試：4/3 ¶程式測驗 投了履歷之後會收到 Codility 的程式測驗，要在一個半小時內完成兩題。可以自由選擇作答的程式語言，我使用的是 Python (Life is short, you need Python.)。 第一題是給一個 list，算有幾種方法，若 remove 其中一個元素，此 list 剩下的元素會是 non-decreasing order。 第二題則是判斷兩個字串要經過多少次 insert 後才能互為 Anagram。Anagram 就是兩個長度相同的字串，組成的字母相同，但順序不同。 這兩題都不會用到特別困難的技巧，應該只是要考你是否具備基本的程式能力，我認為題目如果放在 Leetcode 上大概會是 Easy 的的難度。 大概花了30分鐘寫完，但是完成後不會知道自己的成績。於是就這樣過了一週，完全沒有收到任何通知，有點擔心想說是不是涼了，於是充滿好奇心的我就寫信去問 HR 我有沒有通過測驗，結果火速收到 HR 寄來的通知，恭喜我通過程式測驗，以及與我進行面試的邀約！ ¶現場面試 面試是約在早上十點，於是我起了個大早驅車從台南趕往台北趨勢科技總部。 公司位於台北市大安區，是一棟外表看起來非常氣派的大樓。上樓後被人資帶領到一個小會議室等待主管進來，一會後主管進來，總共有三個人，分別代表 Commercial(Deep Security), SaaS(TMRM), Consumer(IoT Security) 三個 team，簡短自我介紹後，主管們就我做過的專案，以及在資策會實習的工作內容等進行提問，過程還滿愉快的。之後主管們就分別向我介紹每個 team 的工作內容，包括各個 team 正在進行中與 ML 相關的 project。 結束後換下一場，是跟 SaaS 有關的 team，與我面試的是一位工程師，了解到他們的 team 主要是做後端與一些資料分析，還有討論到使用哪些技術。不過感覺上他是被主管叫來面試我，所以好像也不太清楚主管的標準是什麼，所以過程感覺像是聊天而不是在面試哈哈。 最後則是 HR 進來與我確認想進每個 Team 的志願序及實習的時間上是否能配合，也問了我是否有其他公司的面試以及若錄取趨勢後是否會繼續面試其他公司。面試結束後就去在公司附近，而南部已經絕跡的漢堡王吃一波啦！ ¶HR 電話面試 面試完的下一週就收到 HR 寄來的信，通知我已經進入面試的最後階段，跟我安排 30 分鐘的電話面試。電話面試 HR 主要問我一些與人格特質有關的問題，是否有與其他人合作專案的經驗，遇到最大的困難是什麼以及是怎麼解決的，還有確認我參與實習的意願。最後就是恭喜我成為趨勢實習生的一員啦！而且我填的第一個志願就有跟主管 match 到，所以算是進了我最想去的 team，超開心的啦！ 結果：Offer get (4/3) ¶台達研究院 - 大數據分析實習生 履歷投遞：3/13 現場面試：3/27 投完履歷當天就接到人資打來的電話，詢問暑期時間是否能配合以及能否自行解決住宿的問題後，就與我安排了面試的時間。 面試的地點在內湖，法拉利總部的隔壁。當天有三個面試官，首先要我介紹做過有關資料分析的專案，接著針對我履歷上的比賽經歷、修過的課、大學專題等等詢問。 面試官好像特別對我參加的比賽有興趣，問了之前 T-brain 的旅遊訂單成行預測的比賽以及我提到在 Data Mining 課堂上參與的 Gstore 收益預測的 Kaggle 比賽，問了為何選用這些 feature、為何 feature 要這樣處理等等，很多細節我都不太記得，這部分我答的真的不是很好。 面試到後來就覺得應該是沒什麼機會了，但是最後還是詢問了一下工作內容與工作環境如何。有提到他們是做 AOI 與 NLP 以及生醫相關的專案，也有提到他們是因為履歷上寫的經驗有 match 到才會找我來去面試。 整體而言我覺得我的表現非常差，畢竟面試官也只是對我履歷上寫的經歷做詢問，但我由於沒有特別複習我修過的課、做過的專題，其中的細節很多都已經忘記。不過這次的面試經驗也讓我學到了寶貴的一課：任何寫在履歷上的經驗都必須徹底熟悉，而自己不熟悉的部分則可以不提，否則就是在挖洞給自己跳。這次面試結束後算是有點小沮喪，覺得沒有發揮的很好，希望以後還有機會可以再來挑戰。 結果：感謝函 (4/10) ¶台積電 - IT 實習生 履歷投遞：忘了 現場面試：4/2 首先到台積電的官網投遞履歷，看哪個廠區的主管有 match 到你的話就會找你去面試。 邀請我去的是南科廠的 IT 部門，由於之前在資策會實習就在南科，所以大概知道台積電在哪裡。面試流程首先要做一個適性測驗，而由於我英文檢定有通過門檻因此不需做英文測驗，完成後就回到面試休息室等待。接著是一位主管與我面試，請我自我介紹並從中詢問我細節，比起技術上的細節似乎更看重求職者的個性、態度，但是到最後我還是沒有很瞭解工作的內容。 接下來是人資與我面試，詢問我人格特質相關的問題，包括說出三個自己的優缺點，並從說出的優缺點來更進一步追問是否有更具體的實例，也重視團隊合作的經驗。最後提到可能會等所有實習生面試完畢才會通知是否錄取。 結果：感謝函 (6/6) ¶Verizon Media - Summer Intern Multiple Openings 履歷投遞：3月中旬 一面：4/25 二面：5/2 onsite：5/14 傳說中的外商，也就是大家所熟知的 Yahoo! 會知道這個實習主要是因為校園徵才時有逛到這間公司的攤位，之前就有聽說過 Yahoo 被收購後與 AOL 整併成 Oath，沒想到現在居然又改名字了XD。剛好這間公司也有資料分析相關的實習職缺，所以就投投看啦！投遞履歷過了一陣子後收到 HR 來信喬電話面試時間。 ¶一面 (HR Phone Interview) 約定的時間 HR 非常準時的打了電話過來，提到根據我的履歷把我分配到 data team，這個 team 預計會收 1~2 位的實習生，實習過程會做一個有挑戰性的專案。 以下整理還記得的問題： 人格特質與實習意願 為什麼想要應徵 Verizon Media 的暑期實習？ 你期望在暑期實習的過程中能獲得什麼？ 除了我們公司之外還有投遞其他公司嗎？若都錄取會如何做選擇？ 你覺得你有沒有什麼特質能夠凸顯你與其他人的不同？ 技術問題 (ML/Python) 簡述何謂監督式與非監督式學習，以及他們之間的差別？ 哪個方法能夠衡量排名？ (1) NDCG (2) RMSE (3) F1-score 什麼是 loss function？ CNN 中的 convolutional layer 是什麼？ 若一個模型的參數很複雜，學習到很多的雜訊，這個現象稱做什麼？ Python 中 queue 與 stack 的差別？ Python 中哪種資料結構具有順序性？ (1) set (2) list (3) dict Python 中哪些 data type 為 immutable？(多選) (1) int (2) str (3) list (4) tuple 以技術問題來說的話，問的問題並沒有很困難，應該是希望實習生至少能了解基本的 Machine Learning 與 Python 相關知識。所有問題中除了 NDCG 我沒聽過以外，其他問題我認為我自己應該都算是答的還可以，希望有機會能進入下個階段囉～ 後來發現人資姊姊好像是三原的影片出現過的某一個人XD ¶二面 (B Round Phone Interview - Tech Test) data team 工程師的電話面試，簡單自我介紹以及討論我做過的 project。 接著是 online coding 的部分，使用 CoderPad，面試官問了一題 Leetcode 945，題目沒有很難但是因為太緊張所以思路一直打結，最後透過提示才解出來，超懊惱的QQ 不知道是不是 coding 部分表現太差花費太多時間，只考了一題就結束了。然後因為我專案用到很多 CNN，所以面試官最後問了我一些 CNN 的問題，像是 CNN 的原理、訓練的 weight 是什麼、為什麼 CNN 效果這麼好等等。 ¶Onsite Interview 收到面試通知信的時候其實還滿驚訝的，原本還以為會因為 online coding 被刷掉哈哈。公司的地點在南港軟體園區的一棟超大型建築物，先到 2 樓換電梯通行證，再到 14 樓換訪客證後由 HR 帶領到 16 樓的一個小會議室(聽說 10-16 樓都是 Yahoo 的)，跟趨勢還有台達一樣，由我一個人單挑三位面試官。 ¶自我介紹 首先是自我介紹還有簡述之前做過的專案，簡單講了下實驗室做的事情，以及資策會的專案、大學專題還有之前寫的自動登入系統+驗證碼辨識，面試官會要求利用現場的白板描述專案的架構，這部分很快就結束了。 ¶白板題 第二個部分是傳說中的白板題，面試官先詢問我擅長的程式語言後出了一題，給一個長度為 n 的 list，裡頭的值為的範圍是 $0 &lt; x \leq 2$，這樣 list 中一定至少會有一個值會重複，寫一個程式回傳任意一個重複的數字。 例子：輸入為[1, 2, 3, 3, 3]，輸出為3。 首先我給了一個最簡單的方法：統計 list 中每個值出現的次數，出現次數大於 1 的值就是要找的結果。 1234567891011def repeat_num(a, n): d = &#123;&#125; for x in a: if x in d.keys(): d[x] += 1 else: d[x] = 1 for k, v in d.items(): if v &gt; 1: return k 完成後面試官問時間複雜度與空間複雜度，很明顯時間複雜度是 $O(N)$，空間複雜度也是 $O(N)$。接著問能不能改善空間複雜度？ 這部分稍微想了一下，看來要面試官要的是 in-place 的方法。我告訴面試官，因為 list 的值小於 n，因此可以利用 list 的 index 當作 key，接著透過修改對應的值來表示重複，但還沒想到具體的做法。 面試官提示是不是有方法使得 list 的值在改動之後能夠用來表示重複，並且保留原本這個值的特性。忽然間靈光一閃，我可以將值乘上 -1，這樣既可以表示重複，也可以保留原本值的特性。這個方法的時間複雜度為 $O(N)$，空間複雜度為 $O(1)$。 123456def repeat_sum(a, n): for x in a: if a[abs(x)] &lt; 0: return abs(x) else: a[abs(x)] *= -1 最後面試官問若 list 中的值的範圍改為 $0 \leq x \leq n - 2$ 的話要怎麼辦？因為 0 的出現導致乘上 -1 這個方法會失效，不過既然變成負數這個方法可行，要不就就換個方式處理，因為 list 中的每個值一定都小於 n，所以減掉 n 一定可以得到負數，需要原本的值時再加上 n 就可以逆推回去了。 1234567891011def repeat_sum(a, n): for x in a: if x &lt; 0: index = x + n else: index = x if a[index] &lt; 0: return index else: a[index] -= n ¶開放式問題 接著面試官說時間差不多了，可以進入下一個 part，這個階段是開放性的問題，沒有標準答案，主要是想知道我遇到問題會如何思考。 若給我一些圖片，每張圖片都有 label 季節與天氣，利用機器學習預測新的圖片的季節與天氣。我說可以建構兩個 CNN 分別接上 FC layer + softmax，面試官接著問這與建構一個 CNN 並接上兩個 softmax 有何不同？我回答這樣 W 會同時對兩個 loss function 做 optimize，這樣的方法或許不會比分開訓練來得好，但我沒試過也只是推測。 接著問若將每種季節與天氣的組合都當成 label 是否可行，我回答這樣的話 class 會變多，預測的結果未必比分開訓練來得好，但一樣也只是推測。 再來是若訓練資料改成 10 張圖片為一組，label 為一種季節與天氣，測試資料也是直接預測 10 張圖片的季節與天氣。我說可以將 10 張圖片透過某些方式（例如：取平均）得到這 10 張圖片的特徵，再丟進 CNN 做訓練。或是將資料打散，同樣利用輸入為一張圖片的 CNN 做訓練，預測的時候再透過 majority vote 的方式判斷這 10 張圖片的類別。 最後問若這一組 10 張圖片中可能包含一些室內的照片，但是對預測天氣是沒有幫助的，那該怎麼做？我說可以利用 objection detection 找出可能在室內才會有的東西（例如辦公桌、辦公椅等等），將包含室內東西的圖片過濾掉，再用剛剛的方式進行訓練。 這部分我認為與其說是在考我專業能力，我倒覺得比較像是在藉由與面試者的討論，來觀察面試者是如何思考問題、遇到問題時的解決方式、以及與同事間的溝通能力如何。是一個很有趣的面試環節，看起來 data team 的工作也是常常需要腦力激盪一下的啊。 ¶閒聊時間 因為滿好奇 Yahoo 的現況的，所以最後問了一些跟公司有關的問題。最後也提到如果順利錄取的話會跟另一位 data team 的實習生合作完成一個專案，並在實習結束時做發表。 這次真的是耗時最長的一次面試了，從兩次的 phone interview 到最後的 on-site interview，可以看出來他們真的很重視實習生這一塊，可以說外商的 offer 真的不好拿啊！面試的題目也都很有意思，我算是個樂於接受挑戰的人，所以這次的面試算是滿樂在其中的，若有機會到 Yahoo 實習應該會是一個不錯的體驗。 ¶後續通知 過了約莫兩週的時間，HR 寄信說要約電話通知我面試結果。當下覺得應該是錄取了，因為如果要 reject 的話直接寄感謝函就好，不用特地打電話來通知吧XD。 接到電話後果不其然的錄取啦，HR 問了我一些面試結束後的想法，應該是想要確認我的意願如何。因為聽說最後要美國總部批准後才能算真的拿到 offer，所以我也提問到是否會被美國總公司 reject，但是 HR 提到這種狀況機率很低，基本上不太會發生，所以不必太擔心。 結果：Offer Get (5/27) ¶Acer - RD 履歷投遞：3/20 線上測驗：4/17 面試通知：4/26 在 104 投的履歷，投的是 RD 的職缺，看起來 Acer 實習生招的職缺大部分都是與商管、行銷有關，所以似乎也只有 RD 這個選項可以投。 之後收到一個「專業人員適職測驗」的線上測驗信，題目大多為人格特質評估、工作的情境題等等，看起來大部分的題目都沒有標準的答案，所以就有點憑感覺做答了。 完成後大約經過一週就收到了團體面試的通知，面試的時程非常的長，從早上九點到下午六點，由於時間上無法配合，就沒有去參加後續面試了。 結果：主動結束應徵流程 ¶IBM - Data Scientist 履歷投遞：3/26 IPAT 測驗：3/30 面試通知：4/29 到官網投履歷，之後收到問卷調查以及 IPAT 的測驗信，大概就是考數列以及數學的應用問題，試題都是英文的，而且每題限制作答時間很短，感覺有點像是順便考了英文哈哈。 不久之後收到面試的邀請，但是因為不想一直南北奔波，加上當時有拿到其他想去的公司的 offer，所以就放棄了。 結果：主動結束應徵流程 心得 其實這次暑期實習能有這麼多面試的機會真的是讓我滿意外的，可能是我投的職缺都跟資料科學相關，而我的經歷剛好有符合到吧。 在找實習的過程一開始會有點覺得學經歷都不如別人，畢竟競爭對手除了台清交的學生，甚至還有國外的大學(雖然沒去不過 IBM 原本跟我同批面試的人好像還有 NYU 跟 UW 的)。但是後來想想其實也不必這麼沒自信，學歷或許只是拿到面試機會的一道門檻，面試時是否能夠發揮應有的實力才是最重要的。 至於學校成績的話，我認為並不是很重要，沒有公司有問我在校成績相關的問題，有的甚至連成績單都沒看，感覺上相關經驗及作品是公司比較重視的。 整體的面試下來，大概可以總結一下我對各間公司的看法，不過由於應徵的是實習生，面試一定會跟正職會有所差異，而且算是我的個人觀點，所以當作參考就好了。 趨勢科技：工作氣氛感覺不錯、彈性工時、創新、能實際參與到正在進行中的專案(?) 台達研究院：聽面試官說是類似新創的環境，少見有在做生醫及自然語言相關應用的公司 台積電：感覺上是制度分明的公司，工作內容不是非常清楚，但應該不是我有興趣的哈哈 Yahoo：聽說目前靠電商在賺錢、工作環境不錯，會讓人想一直待著、會議室都以台灣的一些景點來命名、非常紫色 最後一定要感謝的是爸媽支持我跑那麼遠去實習，還有感謝老師出國整個暑假都不在(X)，感謝資策會的 Andy 大大幫我寫推薦信，我要暫時北漂兩個月了！]]></content>
      <categories>
        <category>面試</category>
      </categories>
      <tags>
        <tag>Summer Intern</tag>
        <tag>Interview</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n assignment 2]]></title>
    <url>%2F2019%2F05%2F04%2FCS231n-assignment-2%2F</url>
    <content type="text"><![CDATA[簡介 這次的作業相較於第一次作業又更深入了一些，這次作業要依序實作 Fully Connected Network、Batch Normalization、Dropout、Convolutional Neural Network 等方法，並將裡頭的步驟模組化。 Fully Connected Network ¶Modular network 首先要將 FCN 的 forward pass 以及 backward 模組化，包括 affine layer 與 ReLU activation。 在 forward pass 時使用cache將所需變數儲存，以便 backward pass 的時候使用。 12345def affine_forward(x, w, b): out = None out = x.reshape(x.shape[0], -1).dot(w) + b cache = (x, w, b) return out, cache 123456789def affine_backward(dout, cache): x, w, b = cache dx, dw, db = None, None, None dx = dout.dot(w.T).reshape(x.shape) dw = x.reshape(x.shape[0], -1).T.dot(dout) db = np.sum(dout, axis=0) return dx, dw, db 12345def relu_forward(x): out = None out = np.maximum(x, 0) cache = x return out, cache 1234def relu_backward(dout, cache): dx, x = None, cache dx = dout * (x &gt; 0) return dx 利用剛剛完成的模組來建構TwoLayerNet以及可以自訂 size 的FullyConnectedNet。 1234567891011121314151617181920212223242526272829303132333435class TwoLayerNet(object): def __init__(self, input_dim=3*32*32, hidden_dim=100, num_classes=10, weight_scale=1e-3, reg=0.0): self.params = &#123;&#125; self.reg = reg self.params['W1'] = np.random.normal(0, weight_scale, (input_dim, hidden_dim)) self.params['b1'] = np.zeros((hidden_dim, )) self.params['W2'] = np.random.normal(0, weight_scale, (hidden_dim, num_classes)) self.params['b2'] = np.zeros((num_classes, )) def loss(self, X, y=None): scores = None h1, cache_h1 = affine_relu_forward(X, self.params['W1'], self.params['b1']) scores, cache_scores = affine_forward(h1, self.params['W2'], self.params['b2']) # If y is None then we are in test mode so just return scores if y is None: return scores loss, grads = 0, &#123;&#125; loss, dS = softmax_loss(scores, y) dh1, grads['W2'], grads['b2'] = affine_backward(dS, cache_scores) dx, grads['W1'], grads['b1'] = affine_relu_backward(dh1, cache_h1) loss += 0.5 * self.reg * (np.sum(self.params['W2'] ** 2) + np.sum(self.params['W1'] ** 2)) grads['W1'] += self.reg * self.params['W1'] grads['W2'] += self.reg * self.params['W2'] return loss, grads 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596class FullyConnectedNet(object): def __init__(self, hidden_dims, input_dim=3*32*32, num_classes=10, dropout=1, normalization=None, reg=0.0, weight_scale=1e-2, dtype=np.float32, seed=None): self.normalization = normalization self.use_dropout = dropout != 1 self.reg = reg self.num_layers = 1 + len(hidden_dims) self.dtype = dtype self.params = &#123;&#125; self.params['W1'] = np.random.normal(0, weight_scale, (input_dim, hidden_dims[0])) self.params['b1'] = np.zeros((hidden_dims[0], )) for i in range(1, self.num_layers - 1): self.params['W'+str(i+1)] = np.random.normal(0, weight_scale, (hidden_dims[i-1], hidden_dims[i])) self.params['b'+str(i+1)] = np.zeros((hidden_dims[i], )) self.params['W'+str(self.num_layers)] = np.random.normal(0, weight_scale, (hidden_dims[-1], num_classes)) self.params['b'+str(self.num_layers)] = np.zeros((num_classes, )) # When using dropout we need to pass a dropout_param dictionary to each # dropout layer so that the layer knows the dropout probability and the mode # (train / test). You can pass the same dropout_param to each dropout layer. self.dropout_param = &#123;&#125; if self.use_dropout: self.dropout_param = &#123;'mode': 'train', 'p': dropout&#125; if seed is not None: self.dropout_param['seed'] = seed # With batch normalization we need to keep track of running means and # variances, so we need to pass a special bn_param object to each batch # normalization layer. You should pass self.bn_params[0] to the forward pass # of the first batch normalization layer, self.bn_params[1] to the forward # pass of the second batch normalization layer, etc. self.bn_params = [] if self.normalization=='batchnorm': self.bn_params = [&#123;'mode': 'train'&#125; for i in range(self.num_layers - 1)] if self.normalization=='layernorm': self.bn_params = [&#123;&#125; for i in range(self.num_layers - 1)] # Cast all parameters to the correct datatype for k, v in self.params.items(): self.params[k] = v.astype(dtype) def loss(self, X, y=None): X = X.astype(self.dtype) mode = 'test' if y is None else 'train' # Set train/test mode for batchnorm params and dropout param since they # behave differently during training and testing. if self.use_dropout: self.dropout_param['mode'] = mode if self.normalization=='batchnorm': for bn_param in self.bn_params: bn_param['mode'] = mode scores = None h, cache = [None] * (self.num_layers + 1), [None] * (self.num_layers + 1) h[0] = X for i in range(self.num_layers - 1): W, b = self.params['W'+str(i+1)], self.params['b'+str(i+1)] h[i+1], cache[i+1] = affine_relu_forward(h[i], W, b) else: i += 1 W, b = self.params['W'+str(i+1)], self.params['b'+str(i+1)] h[i+1], cache[i+1] = affine_forward(h[i], W, b) scores = h[-1] # If test mode return early if mode == 'test': return scores loss, grads = 0.0, &#123;&#125; loss, dS = softmax_loss(scores, y) dh = [None] * (self.num_layers + 1) dh[-1] = dS i = self.num_layers dh[i-1], grads['W'+str(i)], grads['b'+str(i)] = affine_backward(dh[i], cache[i]) loss += 0.5 * self.reg * np.sum(self.params['W'+str(i)] ** 2) grads['W'+str(i)] += 0.5 * self.reg * (self.params['W'+str(i)] ** 2) i -= 1 while i &gt; 0: dh[i-1], grads['W'+str(i)], grads['b'+str(i)] = affine_relu_backward(dh[i], cache[i]) loss += 0.5 * self.reg * np.sum(self.params['W'+str(i)] ** 2) grads['W'+str(i)] += self.reg * self.params['W'+str(i)] i -= 1 return loss, grads ¶SGD + Momentum 原始的 Stochastic Gradient Descent： $x_{t+1}=x_t-\alpha\nabla f(x_t)$ SGD + Momumtum： $v_{t+1}=\rho v_t-\alpha\nabla f(x_t) \ x_{t+1}=x_t+v_{t+1}$ $v$ 代表目前的方向速度，初始值為 0，如果負梯度與目前方向相同，則速度會越來越快，參數的更新幅度就會變大；反之則越來越慢，參數的更新幅度會變小。 至於 $\rho$ 則是一個 hyperparameter，通常設在 0.9 左右。 使用 SGD + Momentum 通常比 Vanilla SGD 能夠更快收斂。 1234567891011121314def sgd_momentum(w, dw, config=None): if config is None: config = &#123;&#125; config.setdefault('learning_rate', 1e-2) config.setdefault('momentum', 0.9) v = config.get('velocity', np.zeros_like(w)) next_w = None v = config['momentum'] * v - config['learning_rate'] * dw next_w = w + v config['velocity'] = v return next_w, config ¶RMSProp $v_t=\rho v_{t-1}+(1-\rho) \times (\nabla f(x_t))^2$ $\Delta x_t=-\dfrac{\alpha}{\sqrt{v_t+\epsilon}} \times \nabla f(x_t)$ $x_{t+1}=x_t+\Delta x_t$ $\rho$ 為 decay rate，通常設為 0.9、0.99、0.999。 $\epsilon$ 是一個很小的值，為了避免除以 0 的情況產生。 12345678910111213def rmsprop(w, dw, config=None): if config is None: config = &#123;&#125; config.setdefault('learning_rate', 1e-2) config.setdefault('decay_rate', 0.99) config.setdefault('epsilon', 1e-8) config.setdefault('cache', np.zeros_like(w)) next_w = None config['cache'] = config['decay_rate'] * config['cache'] + (1 - config['decay_rate']) * (dw ** 2) next_w = w + -config['learning_rate'] * dw / np.sqrt(config['cache'] + config['epsilon']) return next_w, config ¶Adam 1234567891011121314151617181920def adam(w, dw, config=None): if config is None: config = &#123;&#125; config.setdefault('learning_rate', 1e-3) config.setdefault('beta1', 0.9) config.setdefault('beta2', 0.999) config.setdefault('epsilon', 1e-8) config.setdefault('m', np.zeros_like(w)) config.setdefault('v', np.zeros_like(w)) config.setdefault('t', 0) next_w = None config['t'] += 1 config['m'] = config['beta1'] * config['m'] + (1 - config['beta1']) * dw mt = config['m'] / (1 - config['beta1'] ** config['t']) config['v'] = config['beta2'] * config['v'] + (1 - config['beta2']) * (dw ** 2) vt = config['v'] / (1 - config['beta2'] ** config['t']) next_w = w + -config['learning_rate'] * mt / (np.sqrt(vt) + config['epsilon']) return next_w, config 比較不同 optimizer 的表現： Batch Normalization 實作過程參考以下論文： Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift batch normalization 的目的是為了讓每一層的輸出都保持高斯分佈，主要的目的是為了避免 gradient vanishing。做法是將 fordward pass 時用來訓練的批次資料計算 mean 以及 variance，利用 mini-batch 的 mean 及 vairance 來更新整體的 mean 及 variance。 ¶Forward Pass 論文中具體的實作方法如下： 12345678910111213141516171819202122232425262728293031323334def batchnorm_forward(x, gamma, beta, bn_param): mode = bn_param['mode'] eps = bn_param.get('eps', 1e-5) momentum = bn_param.get('momentum', 0.9) N, D = x.shape running_mean = bn_param.get('running_mean', np.zeros(D, dtype=x.dtype)) running_var = bn_param.get('running_var', np.zeros(D, dtype=x.dtype)) out, cache = None, None if mode == 'train': sample_mean = x.mean(axis=0) sample_var = x.var(axis=0) sqrtvar = np.sqrt(sample_var + eps) xmu = x - sample_mean ivar = 1./sqrtvar x_hat = xmu * ivar out = gamma * x_hat + beta cache = (xmu, sample_var, ivar, sqrtvar, x_hat, gamma, eps) running_mean = momentum * running_mean + (1 - momentum) * sample_mean running_var = momentum * running_var + (1 - momentum) * sample_var elif mode == 'test': x_hat = (x - running_mean) / np.sqrt(running_var + eps) out = gamma * x_hat + beta else: raise ValueError('Invalid forward batchnorm mode "%s"' % mode) # Store the updated running means back into bn_param bn_param['running_mean'] = running_mean bn_param['running_var'] = running_var return out, cache ¶Backward Pass 論文中對於計算 BN 的反向傳播也有一些描述： 真的是滿複雜的，最好還是自己畫過一次計算圖之後再試著去計算 backward pass，這部分的話這篇文章寫得滿不錯的，可以參考一下。 123456789101112131415161718192021222324252627def batchnorm_backward(dout, cache): dx, dgamma, dbeta = None, None, None N, D = dout.shape xmu, var, ivar, sqrtvar, x_hat, gamma, eps = cache dbeta = np.sum(dout, axis=0) dgamma_x = dout dgamma = np.sum(dgamma_x * x_hat, axis=0) dx_hat = dgamma_x * gamma divar = np.sum(dx_hat * xmu, axis=0) dx_mu1 = dx_hat * ivar dsqrtvar = -divar / (sqrtvar ** 2) dvar = 0.5 * dsqrtvar / np.sqrt(var + eps) dsq = dvar * np.ones((N, D)) / N dx_mu2 = 2 * xmu * dsq dx1 = dx_mu1 + dx_mu2 dmu = -np.sum(dx_mu1 + dx_mu2, axis=0) dx2 = dmu * np.ones((N, D)) / N dx = dx1 + dx2 return dx, dgamma, dbeta 簡化版： 12345678910def batchnorm_backward_alt(dout, cache): dx, dgamma, dbeta = None, None, None xmu, var, ivar, sqrtvar, x_hat, gamma, eps = cache N, D = dout.shape dbeta = np.sum(dout, axis=0) dgamma = np.sum(x_hat * dout, axis=0) dx = (gamma * ivar / N) * (N * dout - x_hat * dgamma - dbeta) return dx, dgamma, dbeta ¶Layer Normalization batch normalization 使得類神經網路的訓練更有效率，但是對於複雜的網路結構來說，在 batch size 不夠大的時候效果可能不會太好。因此另一個方法是對 feature 進行 normalize，參考論文：Layer Normalization。 12345678910111213def layernorm_forward(x, gamma, beta, ln_param): out, cache = None, None eps = ln_param.get('eps', 1e-5) x_T = x.T sample_mean = np.mean(x_T, axis=0) sample_var = np.var(x_T, axis=0) x_norm_T = (x_T - sample_mean) / np.sqrt(sample_var + eps) x_norm = x_norm_T.T out = x_norm * gamma + beta cache = (x, x_norm, gamma, sample_mean, sample_var, eps) return out, cache 1234567891011121314151617def layernorm_backward(dout, cache): dx, dgamma, dbeta = None, None, None x, x_norm, gamma, sample_mean, sample_var, eps = cache x_T = x.T dout_T = dout.T N = x_T.shape[0] dbeta = np.sum(dout, axis=0) dgamma = np.sum(x_norm * dout, axis=0) dx_norm = dout_T * gamma[:,np.newaxis] dv = ((x_T - sample_mean) * -0.5 * (sample_var + eps)**-1.5 * dx_norm).sum(axis=0) dm = (dx_norm * -1 * (sample_var + eps)**-0.5).sum(axis=0) + (dv * (x_T - sample_mean) * -2 / N).sum(axis=0) dx_T = dx_norm / (sample_var + eps)**0.5 + dv * 2 * (x_T - sample_mean) / N + dm / N dx = dx_T.T return dx, dgamma, dbeta Dropout Dropout: A Simple Way to Prevent Neural Networks from Overfitting drouput 是一種正規化的方法，在 forward pass 時隨機將某些 neuron 的值丟掉，跟 L1, L2 regularization 一樣，目的都是為了避免 overfitting。 實作方法是在 training 時根據一個機率 p 來隨機產生一個 mask (值為 True or False)，將 x 乘上 mask 就可以將部分 neuron 的值設為 0， predicting 的時候就直接將 x 乘上 p。 但與其在 predicting 時乘上 p，其實我們可以在 training 的時候就除以 p，這樣就可以減少 predicting 的計算量，因為我們通常比較在意 predicting 時的效率，這個技巧稱為 inverted dropout。 123456789101112131415161718def dropout_forward(x, dropout_param): p, mode = dropout_param['p'], dropout_param['mode'] if 'seed' in dropout_param: np.random.seed(dropout_param['seed']) mask = None out = None if mode == 'train': mask = (np.random.rand(*x.shape) &lt; p) / p out = x * mask elif mode == 'test': out = x cache = (dropout_param, mask) out = out.astype(x.dtype, copy=False) return out, cache 12345678910def dropout_backward(dout, cache): dropout_param, mask = cache mode = dropout_param['mode'] dx = None if mode == 'train': dx = dout * mask elif mode == 'test': dx = dout return dx Convolutional Neural Network ¶Convolution Layer Forward Pass 實作 CNN 的 forward pass，輸入 $x$ 的大小為 $(N,C,H,W)$，以及 $F$ 個 filter，合起來成為一個 $(F,C,HH,WW)$ 的矩陣，經過 convolution 的計算後，輸出一個 $(N,F,H\prime,W\prime)$ 的矩陣。 12345678910111213141516171819202122def conv_forward_naive(x, w, b, conv_param): out = None N, C, H, W = x.shape F, C, HH, WW = w.shape pad, stride = conv_param['pad'], conv_param['stride'] x_pad = np.pad(x, ((0,), (0,), (pad,), (pad,)), 'constant', constant_values=0) H_prime = 1 + (H + 2 * pad - HH) // stride W_prime = 1 + (W + 2 * pad - WW) // stride out = np.empty((N, F, H_prime, W_prime)) for f in range(F): for i in range(H_prime): for j in range(W_prime): out[:, f, i, j] = np.sum(x_pad[:, :, i*stride:i*stride+HH, j*stride:j*stride+WW] * w[f], axis=(1,2,3)) out += b.reshape(F, 1, 1) cache = (x, w, b, conv_param) return out, cache ¶Convolution Layer Backward Pass 計算 convolution layer 的 backpropagation 可以參考這篇文章。因為 forward 時的計算也算是 $x$ 乘上 $w$，因此 backward 時計算 $dx$ 就是用 $dout$ 與 $w$ 做計算；計算 $dw$ 時則是用 $dout$ 與 $x$ 做計算，雖然概念上不難理解，但是要透過numpy實作的話對維度要有一定的掌握才行。 12345678910111213141516171819202122232425262728def conv_backward_naive(dout, cache): dx, dw, db = None, None, None x, w, b, conv_param = cache N, C, H, W = x.shape F, C, HH, WW = w.shape pad, stride = conv_param['pad'], conv_param['stride'] x_pad = np.pad(x, ((0,), (0,), (pad,), (pad,)), mode='constant', constant_values=0) H_prime = 1 + (H + 2 * pad - HH) // stride W_prime = 1 + (W + 2 * pad - WW) // stride dx = np.zeros_like(x) dx_pad = np.zeros_like(x_pad) dw = np.zeros_like(w) db = np.sum(dout, axis=(0,2,3)) for i in range(H_prime): for j in range(W_prime): for f in range(F): dw[f] += np.sum(dout[:, f, i, j].reshape(-1,1,1,1) * x_pad[:, :, i*stride:i*stride+HH, j*stride:j*stride+WW], axis=0) for n in range(N): dx_pad[n, :, i*stride:i*stride+HH, j*stride:j*stride+WW] += np.sum(w * dout[n, :, i, j].reshape(-1,1,1,1), axis=0) dx = dx_pad[:, :, pad:-pad, pad:-pad] return dx, dw, db ¶Max Pooling Forward Pass 12345678910111213141516def max_pool_forward_naive(x, pool_param): out = None N, C, H, W = x.shape pool_height, pool_width, stride = pool_param['pool_height'], pool_param['pool_width'], pool_param['stride'] H_prime = 1 + (H - pool_height) // stride W_prime = 1 + (W - pool_width) // stride out = np.empty((N, C, H_prime, W_prime)) for i in range(H_prime): for j in range(W_prime): out[:, :, i, j] = np.max(x[:, :, i*stride:i*stride+pool_height, j*stride:j*stride+pool_width], axis=(2,3)) cache = (x, pool_param) return out, cache ¶Max Pooling Backward Pass 123456789101112131415161718def max_pool_backward_naive(dout, cache): dx = None x, pool_param = cache N, C, H, W = x.shape pool_height, pool_width, stride = pool_param['pool_height'], pool_param['pool_width'], pool_param['stride'] H_prime = 1 + (H - pool_height) // stride W_prime = 1 + (W - pool_width) // stride dx = np.zeros_like(x) for i in range(H_prime): for j in range(W_prime): arg = np.max(x[:, :, i*stride:i*stride+pool_height, j*stride:j*stride+pool_width], axis=(2,3), keepdims=True) == \ x[:, :, i*stride:i*stride+pool_height, j*stride:j*stride+pool_width] dx[:, :, i*stride:i*stride+pool_height, j*stride:j*stride+pool_width] += arg * dout[:, :, i, j][:,:,np.newaxis,np.newaxis] return dx 最後還有實作 Spatial Batch Normalization 以及 Group Normalization，但這部分不是很熟所以略過。]]></content>
      <categories>
        <category>學校課程</category>
        <category>圖像辨識</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Computer Vision</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n assignment 1]]></title>
    <url>%2F2019%2F04%2F20%2FCS231n-assignment-1%2F</url>
    <content type="text"><![CDATA[簡介 這次作業主要實作以下演算法： k-Nearest Neighbor (kNN) Support Vector Machine (SVM) Softmax classifier Two-Layer Neural Network Higher Level Representations: Image Features k-Nearest Neighbor (kNN) 在knn.ipynb中已經將資料載入完成，使用 CIFAR-10 圖片集中的 5000 筆當作訓練，500 筆當作測試。每張圖片的大小都是 (32, 32, 3)，3 代表 RGB 三個通道。 cifar-10 資料集的10種類別 我們要實作三個版本的 kNN，分別是使用雙迴圈、單迴圈、無迴圈的版本，實作的程式碼在cs231n/classifiers/k_nearest_neighbor.py。 compute_distances_two_loops 首先是雙迴圈的版本，X是輸入的 test data，大小為 (num_test, D)，輸出dists為一個大小為 (num_test, num_train) 的 numpy array，元素dists[i,j]代表第 i 個 test data point 與第 j 個 train data point 的歐幾里得距離。 123456789def compute_distances_two_loops(self, X): num_test = X.shape[0] num_train = self.X_train.shape[0] dists = np.zeros((num_test, num_train)) for i in range(num_test): for j in range(num_train): dists[i][j] = np.sqrt(np.sum((X[i] - self.X_train[j]) ** 2)) return dists predict_labels 接著實作函式predict_labels，先找出前 k 個與測試資料最接近的點，再透過 majority vote 的方式選出最有可能的類別。 12345678910111213141516171819def predict_labels(self, dists, k=1): num_test = dists.shape[0] y_pred = np.zeros(num_test) for i in range(num_test): # A list of length k storing the labels of the k nearest neighbors to # the ith test point. closest_y = [] indices = np.argsort(dists[i])[:k] closest_y = self.y_train[indices] y_count = &#123;&#125; for y in closest_y: y_count[y] = y_count.get(y, 0) + 1 max_value = max(y_count.values()) candidates = [y for y, v in y_count.items() if v == max_value] y_pred[i] = min(candidates) return y_pred 下半部其實可以用一行程式碼解決： 1y_pred[i] = np.bincount(closest_y).argmax() 利用剛剛得到的dists，可以計算出 test data 的預測結果，再將預測結果與正確答案比較就可以算出準確率。 k = 1 時的準確率： 1Got 137 / 500 correct =&gt; accuracy: 0.274000 k = 5 時的準確率： 1Got 139 / 500 correct =&gt; accuracy: 0.278000 compute_distances_one_loop 接著實作單迴圈版本的 kNN： 12345678def compute_distances_one_loop(self, X): num_test = X.shape[0] num_train = self.X_train.shape[0] dists = np.zeros((num_test, num_train)) for i in range(num_test): dists[i] = np.sqrt(np.sum((X[i] - self.X_train) ** 2, axis=1)) return dists compute_distances_no_loops 無迴圈的概念就是將兩點間的距離以平方差公式展開：\((x-y)^2=x^2+y^2-2xy\)。 12345678def compute_distances_no_loops(self, X): num_test = X.shape[0] num_train = self.X_train.shape[0] dists = np.zeros((num_test, num_train)) dists += np.sqrt(np.sum(self.X_train ** 2, axis=1) + np.sum(X ** 2, axis=1)[:, np.newaxis] \ - 2 * np.dot(X, self.X_train.T)) return dists 執行time_function可以觀察到無迴圈的版本執行速度，遠遠將其他兩個版本甩在後頭。 123Two loop version took 29.963571 secondsOne loop version took 23.200013 secondsNo loop version took 0.140244 seconds Cross Validation (交叉驗證) 實作交叉驗證來進行 hyperparameter 的搜索，要找的超參數為 k 值。 1234567891011121314151617181920212223num_folds = 5k_choices = [1, 3, 5, 8, 10, 12, 15, 20, 50, 100]X_train_folds = []y_train_folds = []X_train_folds = np.array_split(X_train, num_folds)y_train_folds = np.array_split(y_train, num_folds)k_to_accuracies = &#123;&#125;for k in k_choices: k_to_accuracies[k] = [] for n in range(num_folds): classifier.train(np.concatenate(X_train_folds[:n] + X_train_folds[n+1:]), np.concatenate(y_train_folds[:n] + y_train_folds[n+1:])) pred = classifier.predict(X_train_folds[n], k=k) num_correct = np.sum(pred == y_train_folds[n]) k_to_accuracies[k].append(float(num_correct) / len(pred))# Print out the computed accuraciesfor k in sorted(k_to_accuracies): for accuracy in k_to_accuracies[k]: print('k = %d, accuracy = %f' % (k, accuracy)) 執行此段程式碼會輸出每個 k 值在每個 fold 的表現。 1234567891011121314k = 1, accuracy = 0.263000k = 1, accuracy = 0.257000k = 1, accuracy = 0.264000k = 1, accuracy = 0.278000k = 1, accuracy = 0.266000k = 3, accuracy = 0.239000k = 3, accuracy = 0.249000k = 3, accuracy = 0.240000...k = 100, accuracy = 0.256000k = 100, accuracy = 0.270000k = 100, accuracy = 0.263000k = 100, accuracy = 0.256000k = 100, accuracy = 0.263000 視覺化 將結果視覺化，可以觀察到在 k = 10 的時候會在訓練集得到最好的準確率。 cross validation result 將 k 值設為 10 之後，在測試集的準確率確實有提升一些。 1Got 141 / 500 correct =&gt; accuracy: 0.282000 Support Vector Machine (SVM) svm_loss_naive 首先實作迴圈版本的 svm loss function。 輸入的資料維度是 D，總共有 C 種類別，每個 minibatch 有 N 筆資料。 參數W是大小為 (D,C) 的權重，X是大小為 (N,D) 的 minibatch data，y大小為 (N,1) 代表 training labels。 SVM 的 loss function 如下： \(L_i=\sum_{j\neq y_i}max(0,s_j-s_{y_i}+\Delta)\) Loss function 的 gradient 如下： \(\nabla_{w_{y_i}}L_i=-(\sum_{j\neq y_i} 1(w_j^Tx_i-w_{y_i}^Tx_i+\Delta&gt;0))x_i\) \(\nabla_{w_j}L_i=1(w_j^Tx_i-w_{y_i}^Tx_i+\Delta&gt;0)x_i\) 1234567891011121314151617181920212223242526272829def svm_loss_naive(W, X, y, reg): dW = np.zeros(W.shape) # initialize the gradient as zero # compute the loss and the gradient num_classes = W.shape[1] num_train = X.shape[0] loss = 0.0 for i in range(num_train): scores = X[i].dot(W) correct_class_score = scores[y[i]] for j in range(num_classes): if j == y[i]: continue margin = scores[j] - correct_class_score + 1 # note delta = 1 if margin &gt; 0: loss += margin dW[:, j] += X[i] dW[:, y[i]] -= X[i] # Right now the loss is a sum over all training examples, but we want it # to be an average instead so we divide by num_train. loss /= num_train dW /= num_train # Add regularization to the loss. loss += reg * np.sum(W * W) dW += reg * W return loss, dW svm_loss_vectorized 利用numpy vectorized 的計算方式提升運算速度。 12345678910111213141516171819202122def svm_loss_vectorized(W, X, y, reg): """ Structured SVM loss function, vectorized implementation. Inputs and outputs are the same as svm_loss_naive. """ loss = 0.0 dW = np.zeros(W.shape) # initialize the gradient as zero num_train = X.shape[0] scores = X.dot(W) correct_class_score = scores[np.arange(num_train), y].reshape(-1, 1) margin = scores - correct_class_score + 1 margin[np.arange(num_train), y] = 0 loss += margin[margin &gt; 0].sum() / num_train loss += reg * np.sum(W * W) counts = (margin &gt; 0).astype(int) counts[range(num_train), y] = - np.sum(counts, axis = 1) dW += np.dot(X.T, counts) / num_train + reg * W return loss, dW Stochastic Gradient Descent (SGD) cs331n/linear_classifier.py 隨機梯度下降，每次更新W時只利用一部分的資料來計算 loss 及 gradient，能夠減少運算量。 123456789101112131415161718192021222324252627def train(self, X, y, learning_rate=1e-3, reg=1e-5, num_iters=100, batch_size=200, verbose=False): num_train, dim = X.shape num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes if self.W is None: # lazily initialize W self.W = 0.001 * np.random.randn(dim, num_classes) # Run stochastic gradient descent to optimize W loss_history = [] for it in range(num_iters): X_batch = None y_batch = None idx = np.random.choice(np.arange(num_train), batch_size) X_batch, y_batch = X[idx], y[idx] # evaluate loss and gradient loss, grad = self.loss(X_batch, y_batch, reg) loss_history.append(loss) self.W -= learning_rate * grad if verbose and it % 100 == 0: print('iteration %d / %d: loss %f' % (it, num_iters, loss)) return loss_history 12345def predict(self, X): y_pred = np.zeros(X.shape[0]) y_pred = X.dot(self.W).argmax(axis=1) return y_pred Parameter Tuning 使用 Grid Search 的方法尋找超參數。 123456789101112131415161718192021222324252627282930313233343536learning_rates = [1e-7, 5e-5]regularization_strengths = [2.5e4, 5e4]results = &#123;&#125;best_val = -1 # The highest validation accuracy that we have seen so far.best_svm = None # The LinearSVM object that achieved the highest validation rate.for lr in learning_rates: for reg in regularization_strengths: print("hyperparameter tuning: lr=&#123;&#125;, reg=&#123;&#125;".format(lr, reg)) svm = LinearSVM() tic = time.time() loss_hist = svm.train(X_train, y_train, learning_rate=lr, reg=reg, num_iters=1500, verbose=True) y_train_pred = svm.predict(X_train) train_acc = np.mean(y_train == y_train_pred) y_val_pred = svm.predict(X_val) val_acc = np.mean(y_val == y_val_pred) results[(lr, reg)] = (train_acc, val_acc) if val_acc &gt; best_val: best_val = val_acc best_svm = svm print('-'*40)# Print out results.for lr, reg in sorted(results): train_accuracy, val_accuracy = results[(lr, reg)] print('lr %e reg %e train accuracy: %f val accuracy: %f' % ( lr, reg, train_accuracy, val_accuracy)) print('best validation accuracy achieved during cross-validation: %f' % best_val) 將學習到的權重視覺化 learned_weight Softmax classifier 與 svm 相同都是要實作兩種方法： softmax_loss_naive 模型的W,X,y都與 SVM 相同，唯一不同的點是 softmax classifier 使用的 loss function 不是 hinge loss，而是 cross-entropy loss： \(L_i=-log(\dfrac{e^{f_{y_i}}}{\sum_j e^{f_j}})\) 也可以寫成 \(L_i＝-f_{y_i}+log\sum_j e^{f_j}\) 1234567891011121314151617181920212223def softmax_loss_naive(W, X, y, reg): # Initialize the loss and gradient to zero. loss = 0.0 dW = np.zeros_like(W) num_classes = W.shape[1] num_train = X.shape[0] for i in range(num_train): scores = X[i].dot(W) scores -= np.max(scores) correct_class_score = scores[y[i]] exp_sum = np.sum(np.exp(scores)) loss += -np.log(np.exp(correct_class_score) / exp_sum) for j in range(num_classes): dW[:, j] += (np.exp(scores[j]) / exp_sum - (y[i] == j)) * X[i] loss /= num_train dW /= num_train loss += reg * np.sum(W * W) dW += reg * W return loss, dW softmax_loss_vectorized 12345678910111213141516171819def softmax_loss_vectorized(W, X, y, reg): # Initialize the loss and gradient to zero. loss = 0.0 dW = np.zeros_like(W) num_train = X.shape[0] scores = X.dot(W) scores -= np.max(scores, axis=1, keepdims=True) prob = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True) loss += np.sum(-np.log(prob[np.arange(num_train), y])) ind = np.zeros_like(prob) ind[np.arange(num_train), y] = 1 dW = X.T.dot(prob - ind) loss = loss / num_train + reg * np.sum(W * W) dW = dW / num_train + reg * W return loss, dW 接下來也是實作 Grid Search 搜尋超參數，與 SVM 的部份相同，就不再贅述。 Two-Layer Neural Network Fordward Pass 實作一個 two-layer 的 NN，使用 ReLU nonlinearity。 計算方法如下： \(h_1=ReLU(X\cdot W_1+b_1)\) \(S=h_1\cdot W_2+b_2\) 12345678910111213141516def loss(self, X, y=None, reg=0.0): # Unpack variables from the params dictionary W1, b1 = self.params['W1'], self.params['b1'] W2, b2 = self.params['W2'], self.params['b2'] N, D = X.shape # Compute the forward pass scores = None relu = lambda x: np.maximum(0, x) h1 = relu(X.dot(W1) + b1) scores = h1.dot(W2) + b2 # If the targets are not given then jump out, we're done if y is None: return scores Compute Loss 使用 softmax classifier loss：\(L_i=-log(\dfrac{e^{f_{y_i}}}{\sum_j e^{f_j}})\) 1234567# Compute the lossloss = Nonescores_ = scores - np.max(scores, axis=1, keepdims=True)prob = np.exp(scores_) / np.sum(np.exp(scores_), axis=1, keepdims=True)loss = np.sum(-np.log(prob[np.arange(N), y]))loss = loss / N + 0.5 * reg * (np.sum(W1 * W1) + np.sum(W2 * W2)) Backpropagation 計算 Loss 對每個參數的偏微分：\(\dfrac{\partial L}{\partial W_2},\dfrac{\partial L}{\partial b_2},\dfrac{\partial L}{\partial W_1},\dfrac{\partial L}{\partial b_1}\) 以下進行四個偏微分的推導： \(\mathbf{\dfrac{\partial L}{\partial W_2}}=\dfrac{\partial L}{\partial S}\dfrac{\partial S}{\partial W_2}=dS\cdot h_1^\top\) 而 \(dS\) 為 softmax function 對 score 的偏微分： \(dS=\begin{cases} \dfrac{e^{s_i}}{\sum_j e^{s_j}} - 1, &amp; j=y_i \\ \dfrac{e^{s_i}}{\sum_j e^{s_j}}, &amp; j\neq y_i \end{cases}\) \(\mathbf{\dfrac{\partial L}{\partial b_2}}=\dfrac{\partial L}{\partial S}\dfrac{\partial S}{\partial b_2}=dS\) \(\mathbf{\dfrac{\partial L}{\partial W_1}}=\dfrac{\partial L}{\partial S}\dfrac{\partial S}{\partial h_1}\dfrac{\partial h_1}{\partial W_1}=dS\cdot W_2^\top\cdot \dfrac{\partial h_1}{\partial W_1}\) 而 \(\dfrac{\partial h_1}{\partial W_1}=\begin{cases} 0, &amp; h_1=0 \\ x^\top, &amp; h_1 &gt; 0 \end{cases}\) 因為 \(ReLU(x)=max(0,x),\dfrac{\partial ReLU}{\partial x}=\begin{cases} 0, &amp; x&lt;0 \\ 1, &amp; x&gt;0 \end{cases}\) \(\mathbf{\dfrac{\partial L}{\partial b_1}}=\dfrac{\partial L}{\partial S}\dfrac{\partial S}{\partial h_1}\dfrac{\partial h_1}{\partial b_1}=dS\cdot W_2^\top\cdot\dfrac{\partial h_1}{\partial b_1}\) 123456789101112131415# Backward pass: compute gradientsgrads = &#123;&#125;dS = probdS[np.arange(N), y] -= 1dS /= Ngrads['W2'] = h1.T.dot(dS) + reg * W2grads['b2'] = np.sum(dS, axis=0)dh1 = dS.dot(W2.T) * (h1 &gt; 0)grads['W1'] = X.T.dot(dh1) + reg * W1grads['b1'] = np.sum(dh1, axis=0)return loss, grads Higher Level Representations: Image Features 計算圖片的 Histogram of Oriented Gradients (HOG) 當作 feature，丟進模型做訓練。這部分的程式碼都已經先寫好了，我們只要 Tuning 模型的 Hyperparameter 使準確率到預期的值即可。]]></content>
      <categories>
        <category>學校課程</category>
        <category>圖像辨識</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Computer Vision</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS224n assignment 2]]></title>
    <url>%2F2018%2F12%2F31%2FCS224n-assignment-2%2F</url>
    <content type="text"><![CDATA[CS224n assignment 2 這次的作業主要目的是讓我們實作 Dependency Parsing 以及熟悉 Tensorflow 的運作原理。 1. Tensorflow Softmax In this question, we will implement a linear classifier with loss function \[ J(\mathbf{W}) = CE(\mathbf{y},softmax(\mathbf{xW} + \mathbf{b})) \] Where \(\mathbf{x}\) is a vector of features, \(\mathbf{W}\) is the model’s weight matrix, and \(\mathbf{b}\) is a bias term. We will use TensorFlow’s automatic differentiation capability to fit this model to provided data. (a) 使用 Tensorflow 實作 Softmax Implement the softmax function using TensorFlow in q1_softmax.py. Remember that \[ softmax(x)_i = \dfrac{e^{x_i}}{\sum_j{e^{x_j}}} \] Note that you may not use tf.nn.softmax or related built-in functions. You can run basic (nonexhaustive tests) by running python q1_softmax.py. 1.(a) solution 123def softmax(x): out = tf.exp(x) / tf.reduce_sum(tf.exp(x), axis=1, keepdims=True) return out (b) 使用 Tensorflow 實作 Cross Entropy Loss Implement the cross-entropy loss using TensorFlow in q1_softmax.py. Remember that \[ CE(\mathbf{y},\mathbf{\hat{y}})=-\sum\limits_{i=1}^{N_c} y_i log(\hat{y_i})\] where \(\mathbf{y} \in \mathbb{R}^{N_c}\) is a one-hot label vector and \(Nc\) is the number of classes. This loss is summed over all examples in a minibatch. Note that you may not use TensorFlow’s built-in cross-entropy functions for this question. You can run basic (non-exhaustive tests) by running python q1_softmax.py. 1.(b) solution 123def cross_entropy_loss(y, yhat): out = -tf.reduce_sum(tf.multiply(tf.to_float(y), tf.log(yhat))) return out (c) Placeholder Variable 與 Feed Dictionary Carefully study the Model class in model.py. Briefly explain the purpose of placeholder variables and feed dictionaries in TensorFlow computations. Fill in the implementations for add_placeholders and create_feed_dict in q1_classifier.py. 1.(c) solution Hint: Note that configuration variables are stored in the Config class. You will need to use these configuration variables in the code. 123def add_placeholders(self): self.input_placeholder = tf.placeholder(dtype=tf.float32, shape=(self.config.batch_size, self.config.n_features)) self.labels_placeholder = tf.placeholder(dtype=tf.float32, shape=(self.config.batch_size, self.config.n_classes)) 12345def create_feed_dict(self, inputs_batch, labels_batch=None): feed_dict = &#123;self.input_placeholder: inputs_batch, self.labels_placeholder: labels_batch&#125; return feed_dict (d) 使用 Tensorflow 建立網路架構 Implement the transformation for a softmax classifier in the function add_prediction_op in q1_classifier.py. Add cross-entropy loss in the function add_loss_op in the same file. Use the implementations from the earlier parts of the problem (already imported for you), not TensorFlow built-ins. 1.(d) solution 1234567def add_prediction_op(self): with tf.variable_scope('transformation'): b = tf.Variable(tf.zeros(shape=[self.config.n_classes])) W = tf.Variable(tf.zeros(shape=[self.config.n_features, self.config.n_classes])) z = tf.matmul(self.input_placeholder, W) + b pred = softmax(z) return pred 123def add_loss_op(self, pred): loss = cross_entropy_loss(self.labels_placeholder, pred) return loss (e) 使用 Tensorflow 加入 Optimizer Fill in the implementation for add_training_op in q1_classifier.py. Explain in a few sentences what happens when the model’s train_op is called (what gets computed during forward propagation, what gets computed during backpropagation, and what will have changed after the op has been run?). Verify that your model is able to fit to synthetic data by running python q1_classifier.py and making sure that the tests pass. Hint: Make sure to use the learning rate specified in Config. 1.(e) solution 123def add_training_op(self, loss): train_op = tf.train.GradientDescentOptimizer(learning_rate=self.config.lr).minimize(loss) return train_op 2. Neural Transition-Based Dependency Parsing In this section, you’ll be implementing a neural-network based dependency parser. A dependency parser analyzes the grammatical structure of a sentence, establishing relationships between “head” words and words which modify those heads. Your implementation will be a transition-based parser, which incrementally builds up a parse one step at a time. At every step it maintains a partial parse, which is represented as follows: A stack of words that are currently being processed. A buffer of words yet to be processed. A list of dependencies predicted by the parser. Initially, the stack only contains ROOT, the dependencies lists is empty, and the buffer contains all words of the sentence in order. At each step, the parse applies a transition to the partial parse until its buffer is empty and the stack is size 1. The following transitions can be applied: SHIFT: removes the first word from the buffer and pushes it onto the stack. LEFT-ARC: marks the second (second most recently added) item on the stack as a dependent of the first item and removes the second item from the stack. RIGHT-ARC: marks the first (most recently added) item on the stack as a dependent of the second item and removes the first item from the stack. Your parser will decide among transitions at each state using a neural network classifier. First, you will implement the partial parse representation and transition functions. (a) 試試看 Go through the sequence of transitions needed for parsing the sentence “I parsed this sentence correctly”. The dependency tree for the sentence is shown below. At each step, give the configuration of the stack and buffer, as well as what transition was applied this step and what new dependency was added (if any). The first three steps are provided below as an example. 2.(a) solution stack buffer new dependency transition [ROOT] [I, parsed, this, setence, correctly] Initial Configuration [ROOT, I] [parsed, this, setence, correctly] SHIFT [ROOT, I, parsed] [this, setence, correctly] SHIFT [ROOT, parsed] [this, setence, correctly] parsed -&gt; I LEFT-ARC [ROOT, parsed, this] [setence, correctly] SHIFT [ROOT, parsed, this] [setence, correctly] SHIFT [ROOT, parsed, this, sentence] [correctly] SHIFT [ROOT, parsed, setence] [correctly] sentence -&gt; this LEFT-ARC [ROOT, parsed] [correctly] parse -&gt; sentence RIGHT-ARC [ROOT, parsed, correctly] [] SHFIT [ROOT, parsed] [] parse -&gt; correctly RIGHT-ARC [ROOT] [] ROOT -&gt; parsed RIGHT-ARC (b) 時間複雜度 A sentence containing n words will be parsed in how many steps (in terms of n)? Briefly explain why. 2.(b) solution 每個詞都會進行一次 SHIFT 及 LEFT/RIGHT-ARC，因此共 2n 次。 (c) 實作 Dependency Parsing Implement the __init__ and parse_step functions in the PartialParse class in q2_parser_transitions.py. This implements the transition mechanics your parser will use. You can run basic (not-exhaustive) tests by running python q2_parser_transitions.py. 2.(c) solution 12345678910111213141516def __init__(self, sentence): self.sentence = sentence self.stack = ['ROOT'] self.buffer = sentence[:] self.dependencies = []def parse_step(self, transition): if transition == "S": self.stack.append(self.buffer[0]) self.buffer.pop(0) elif transition == "LA": self.dependencies.append((self.stack[-1], self.stack[-2])) self.stack.pop(-2) else: self.dependencies.append((self.stack[-2], self.stack[-1])) self.stack.pop(-1) (d) Minibatch Dependency Parsing Our network will predict which transition should be applied next to a partial parse. We could use it to parse a single sentence by applying predicted transitions until the parse is complete. However, neural networks run much more efficiently when making predictions about batches of data at a time (i.e., predicting the next transition for a many different partial parses simultaneously). We can parse sentences in minibatches with the following algorithm. Implement this algorithm in the minibatch_parse function in q2_ parser_transitions.py. You can run basic (not-exhaustive) tests by running python q2_parser_transitions.py. Note: You will need minibatch_parse to be correctly implemented to evaluate the model you will build in part (h). However, you do not need it to train the model, so you should be able to complete most of part (h) even if minibatch parse is not implemented yet. 2.(d) solution 123456789101112131415def minibatch_parse(sentences, model, batch_size): partial_parse = [PartialParse(sentence) for sentence in sentences] dependencies = [] while len(partial_parse) &gt; 0: mini_batch = partial_parse[:batch_size] while len(mini_batch) &gt; 0: transitions = model.predict(mini_batch) for i, action in enumerate(transitions): mini_batch[i].parse_step(action) mini_batch = [parse for parse in mini_batch if len(parse.stack) &gt; 1 or len(parse.buffer) &gt; 0] dependencies.extend(p.dependencies for p in partial_parse[:batch_size]) partial_parse = partial_parse[batch_size:] return dependencies (e) Xavier initialization In order to avoid neurons becoming too correlated and ending up in poor local minimina, it is often helpful to randomly initialize parameters. One of the most frequent initializations used is called Xavier initialization. Given a matrix \(A\) of dimension \(m × n\), Xavier initialization selects values \(A_{ij}\) uniformly from \([-\epsilon,\epsilon]\), where \[\epsilon=\dfrac{\sqrt{6}}{\sqrt{m+n}}\] Implement the initialization in xavier weight init in q2 _initialization.py. You can run basic (nonexhaustive tests) by running python q2_initialization.py. This function will be used to initialize \(W\) and \(U\). 2.(e) solution 12345678def xavier_weight_init(): def _xavier_initializer(shape, **kwargs): epsilon = tf.sqrt(6 / tf.cast(np.sum(shape), tf.float32)) out = tf.random_uniform(shape=shape, minval=-epsilon, maxval=epsilon) return out # Returns defined initializer function. return _xavier_initializer (f) Dropout We will regularize our network by applying Dropout. During training this randomly sets units in the hidden layer \(h\) to zero with probability \(p_{drop}\) and then multiplies \(h\) by a constant \(\gamma\) (dropping different units each minibatch). We can write this as \[h_{drop}=\gamma d \circ h\] where \(d ∈ \{0, 1\}^{D_h}\) (\(D_h\) is the size of \(h\)) is a mask vector where each entry is 0 with probability \(p_{drop}\) and 1 with probability (1 − \(p_{drop}\)). \(\gamma\) is chosen such that the value of hdrop in expectation equals \(h\): \[E_{p_{drop}}[h_{drop}]_i=h_i\] for all \(0 &lt; i &lt; D_h\). What must \(\gamma\) equal in terms of \(p_{drop}\)? Briefly justify your answer. 2.(f) solution \(E_p{h_p}_i=E_p[\gamma d_i h_i] = p \times 0 + (1-p) \times \gamma h_i = (1-p)\gamma h_i = h_i\) \(\Rightarrow r = \dfrac{1}{1-p}\) (g) Adam Optmizer We will train our model using the Adam optimizer. Recall that standard SGD uses the update rule \[\theta \leftarrow \theta - \alpha \nabla_\theta J_{minibatch}(\theta)\] where \(\theta\) is a vector containing all of the model parameters, \(J\) is the loss function, \(\nabla_\theta J_{minibatch}(\theta)\) is the gradient of the loss function with respect to the parameters on a minibatch of data, and \(\alpha\) is the learning rate. Adam uses a more sophisticated update rule with two additional steps. First, Adam uses a trick called momentum by keeping track of m, a rolling average of the gradients: \[m \leftarrow \beta_1 m + (1-\beta_1) \nabla_\theta J_{minibatch}(\theta)\] \[\theta \leftarrow \theta - \alpha m\] where \(\beta_1\) is a hyperparameter between 0 and 1 (often set to 0.9). Briefly explain (you don’t need to prove mathematically, just give an intuition) how using \(m\) stops the updates from varying as much. Why might this help with learning? Adam also uses adaptive learning rates by keeping track of v, a rolling average of the magnitudes of the gradients: \[m \leftarrow \beta_1 m + (1-\beta_1) \nabla_\theta J_{minibatch}(\theta)\] \[m \leftarrow \beta_2 v + (1-\beta_2) \nabla_\theta J_{minibatch}(\theta) \circ \nabla_\theta J_{minibatch}(\theta)\] \[\theta \leftarrow \theta - \alpha \circ m / \sqrt{v}\] where \(\circ\) and \(/\) denote elementwise multiplication and division (so \(z \circ z\) is elementwise squaring) and \(\beta_2\) is a hyperparameter between 0 and 1 (often set to 0.99). Since Adam divides the update by \(\sqrt{v}\), which of the model parameters will get larger updates? Why might this help with learning? (h) Parser 模型實作 In q2_parser_model.py implement the neural network classifier governing the dependency parser by filling in the appropriate sections. We will train and evaluate our model on the Penn Treebank (annotated with Universal Dependencies).Run python q2_parser_model.py to train your model and compute predictions on the test data (make sure to turn off debug settings when doing final evaluation). Hints: When debugging, pass the keyword argument debug=True to the main method (it is set to true by default). This will cause the code to run over a small subset of the data, so the training the model won’t take as long. This code should run within 1 hour on a CPU. When running with debug=True, you should be able to get a loss smaller than 0.2 and a UAS larger than 65 on the dev set (although in rare cases your results may be lower, there is some randomness when training). When running with debug=False, you should be able to get a loss smaller than 0.08 on the train set and an Unlabeled Attachment Score larger than 87 on the dev set. For comparison, the model in the original neural dependency parsing paper gets 92.5 UAS. If you want, you can tweak the hyperparameters for your model (hidden layer size, hyperparameters for Adam, number of epochs, etc.) to improve the performance (but you are not required to do so). 2.(h) solution 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144import cPickleimport osimport timeimport tensorflow as tffrom model import Modelfrom q2_initialization import xavier_weight_initfrom utils.parser_utils import minibatches, load_and_preprocess_dataclass Config(object): n_features = 36 n_classes = 3 dropout = 0.5 # (p_drop in the handout) embed_size = 50 hidden_size = 200 batch_size = 1024 n_epochs = 10 lr = 0.0005class ParserModel(Model): def add_placeholders(self): self.input_placeholder = tf.placeholder(dtype=tf.int32, shape=(None, self.config.n_features)) self.labels_placeholder = tf.placeholder(dtype=tf.float32, shape=(None, self.config.n_classes)) self.dropout_placeholder = tf.placeholder(dtype=tf.float32) def create_feed_dict(self, inputs_batch, labels_batch=None, dropout=0): feed_dict = &#123;self.input_placeholder: inputs_batch, self.dropout_placeholder: dropout&#125; if labels_batch is not None: feed_dict[self.labels_placeholder] = labels_batch return feed_dict def add_embedding(self): embedding = tf.Variable(self.pretrained_embeddings) embeddings = tf.reshape(tf.nn.embedding_lookup(embedding, self.input_placeholder), [-1, self.config.n_features * self.config.embed_size]) return embeddings def add_prediction_op(self): x = self.add_embedding() xavier_initializer = xavier_weight_init() W = tf.Variable(xavier_initializer((self.config.n_features * self.config.embed_size, self.config.hidden_size))) b1 = tf.Variable(tf.zeros(self.config.hidden_size)) U = tf.Variable(xavier_initializer((self.config.hidden_size, self.config.n_classes))) b2 = tf.Variable(tf.zeros(self.config.n_classes)) h = tf.nn.relu(tf.matmul(x, W) + b1) h_drop = tf.nn.dropout(h, 1 - self.dropout_placeholder) pred = tf.matmul(h_drop, U) + b2 return pred def add_loss_op(self, pred): loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=self.labels_placeholder)) return loss def add_training_op(self, loss): train_op = tf.train.AdamOptimizer(learning_rate=self.config.lr).minimize(loss) return train_op def train_on_batch(self, sess, inputs_batch, labels_batch): feed = self.create_feed_dict(inputs_batch, labels_batch=labels_batch, dropout=self.config.dropout) _, loss = sess.run([self.train_op, self.loss], feed_dict=feed) return loss def run_epoch(self, sess, parser, train_examples, dev_set): n_minibatches = 1 + len(train_examples) / self.config.batch_size prog = tf.keras.utils.Progbar(target=n_minibatches) for i, (train_x, train_y) in enumerate(minibatches(train_examples, self.config.batch_size)): loss = self.train_on_batch(sess, train_x, train_y) prog.update(i + 1, [("train loss", loss)]) print "Evaluating on dev set", dev_UAS, _ = parser.parse(dev_set) print "- dev UAS: &#123;:.2f&#125;".format(dev_UAS * 100.0) return dev_UAS def fit(self, sess, saver, parser, train_examples, dev_set): best_dev_UAS = 0 for epoch in range(self.config.n_epochs): print "Epoch &#123;:&#125; out of &#123;:&#125;".format(epoch + 1, self.config.n_epochs) dev_UAS = self.run_epoch(sess, parser, train_examples, dev_set) if dev_UAS &gt; best_dev_UAS: best_dev_UAS = dev_UAS if saver: print "New best dev UAS! Saving model in ./data/weights/parser.weights" saver.save(sess, './data/weights/parser.weights') print def __init__(self, config, pretrained_embeddings): self.pretrained_embeddings = pretrained_embeddings self.config = config self.build()def main(debug=True): print 80 * "=" print "INITIALIZING" print 80 * "=" config = Config() parser, embeddings, train_examples, dev_set, test_set = load_and_preprocess_data(debug) if not os.path.exists('./data/weights/'): os.makedirs('./data/weights/') with tf.Graph().as_default() as graph: print "Building model...", start = time.time() model = ParserModel(config, embeddings) parser.model = model init_op = tf.global_variables_initializer() saver = None if debug else tf.train.Saver() print "took &#123;:.2f&#125; seconds\n".format(time.time() - start) graph.finalize() with tf.Session(graph=graph) as session: parser.session = session session.run(init_op) print 80 * "=" print "TRAINING" print 80 * "=" model.fit(session, saver, parser, train_examples, dev_set) if not debug: print 80 * "=" print "TESTING" print 80 * "=" print "Restoring the best model weights found on the dev set" saver.restore(session, './data/weights/parser.weights') print "Final evaluation on test set", UAS, dependencies = parser.parse(test_set) print "- test UAS: &#123;:.2f&#125;".format(UAS * 100.0) print "Writing predictions" with open('q2_test.predicted.pkl', 'w') as f: cPickle.dump(dependencies, f, -1) print "Done!"if __name__ == '__main__': main(False) (i) 加分題 3. Recurrent Neural Networks: Language Modeling]]></content>
      <categories>
        <category>學校課程</category>
        <category>自然語言處理</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS224n assignment 1]]></title>
    <url>%2F2018%2F12%2F05%2FCS224n-assignment-1%2F</url>
    <content type="text"><![CDATA[CS224n assignment 1 這是 Stanford University 的一門開放式課程，叫做 Natural Language Processing with Deep Learning，所指派的第一個作業。這個作業透過以數學推導類神經網路的運算過程，以及實作wor2vec來進行情感分析，為初學者學習自然語言處理打下基礎。 這個作業主要分為 4 個部分： Softmax Neural Network Bascis word2vec Sentiment Analysis 1. Softmax (a) softmax 的性質 Prove that softmax is invariant to constant offsets in the input, that is, for any input vector \(x\) and any constant \(c\), \[softmax(x) = softmax(x + c)\] where \(x + c\) means adding the constant \(c\) to every dimension of \(x\). Remember that \[softmax(x)_i=\dfrac{e^{x_i}}{\sum_j{e^{x_j}}}\] Note: In practice, we make use of this property and choose \(c = − max_i x_i\) when computing softmax probabilities for numerical stability (i.e., subtracting its maximum element from all elements of \(x\)). solution 證明若將 softmax 的輸入 x 的每一項都加上一個常數後，結果會與原本相同。 \(softmax(x+c)_i=\dfrac{e^{x_i+c}}{\sum_j{e^{x_j+c}}}=\dfrac{e^{x_i}e^c}{\sum_j{e^{x_j}e^c}}=\dfrac{e^ce^{x_i}}{e^c\sum_j{e^{x_j}}}=\dfrac{e^{x_i}}{\sum_j{e^{x_j}}}=softmax(x)\) 這個特性在實際計算 softmax 時常被使用，將輸入 x 的每一項都減去 x 中的最大值，可以減少計算量。 (b) 實作 softmax function Given an input matrix of N rows and D columns, compute the softmax prediction for each row using the optimization in part (a). Write your implementation in q1_softmax.py. You may test by executing python q1_softmax.py. Note: The provided tests are not exhaustive. Later parts of the assignment will reference this code so it is important to have a correct implementation. Your implementation should also be efficient and vectorized whenever possible (i.e., use numpy matrix operations rather than for loops). A non-vectorized implementation will not receive full credit! solution 輸入：x -- 一個維度為 N x D 的 numpy 矩陣 輸出：x -- softmax(x)，可以 in-place 修改 x 的值 注意在x的維度為 1 x D 與 N x D (N ≥ 2)時的處理方式不同。 12345678910111213def softmax(x): orig_shape = x.shape if len(x.shape) &gt; 1: # Matrix x = np.apply_along_axis(softmax, 1, x) else: # Vector x -= np.amax(x) x = np.exp(x) / np.sum(np.exp(x)) assert x.shape == orig_shape return x 2. Neural Network Basics (a) 推導 sigmoid function 的 gradient Derive the gradients of the sigmoid function and show that it can be rewritten as a function of the function value (i.e., in some expression where only \(\sigma(x)\), but not \(x\), is present). Assume that the input \(x\) is a scalar for this question. Recall, the sigmoid function is \[\sigma(x)=\dfrac{1}{1+e^{-x}}\] solution \(\dfrac{\partial\sigma(x)}{\partial x}=\dfrac{-1}{(1+e^{-x})^2}\times e^{-x}\times (-1)=\dfrac{e^{-x}}{(1+e^{-x})^2}=\dfrac{1}{1+e^{-x}}\times{\dfrac{1+e^{-x}-1}{1+e^{-x}}}=\sigma(x)(1-\sigma(x))\) (b) 推導 cross entropy loss 的 gradient Derive the gradient with regard to the inputs of a softmax function when cross entropy loss is used for evaluation, i.e., find the gradients with respect to the softmax input vector \(\theta\), when the prediction is made by \(\hat y = softmax(\theta)\). Remember the cross entropy function is \[CE(y,\hat{y})=-\sum_i y_i log(\hat{y_i})\] where \(y\) is the one-hot label vector, and \(\hat{y}\) is the predicted probability vector for all classes. (Hint: you might want to consider the fact many elements of \(y\) are zeros, and assume that only the k-th dimension of \(y\) is one.) solution 計算 \(\dfrac{\partial CE(y,\hat{y})}{\partial \theta}\) 分成兩種情況： 1. m = n \(\dfrac{\partial CE(y_m,\hat{y_m})}{\partial \theta_n}=\dfrac{\partial -y_nlog\hat{y_n}}{\partial \theta_n}=-y_n \times \dfrac{1}{\hat{y_n}}\times \dfrac{\partial \dfrac{e^{\theta_n}}{\sum_j{e^{\theta j}}}}{\partial \theta_n}=-y_n \times \dfrac{1}{\hat{y_n}}\times\dfrac{e^{\theta n}\sum_j{e^{\theta_j}-e^{\theta n}e^{\theta n}}}{(\sum_j{e^{\theta_j}})^2}\) \(=-y_n \times \dfrac{1}{\hat{y_n}} \times \dfrac{e^{\theta n}}{\sum_j{e^{\theta_j}}}\times\dfrac{\sum_j{e^{\theta j}-e^{\theta n}}}{\sum_j{e^{\theta_j}}}=-y_n \times \dfrac{1}{\hat{y_n}} \times \hat{y_n} \times (1-\hat{y_n}) =-y_n(1-\hat{y_n})\) 2. m ≠ n \(\dfrac{\partial CE(y_m,\hat{y_m})}{\partial\theta\_n}=\dfrac{- \sum\_{m \neq n}{y_m log\hat{y_m}}}{\partial\theta\_n}=-\sum_{m \neq n} y_m \times \dfrac{1}{\hat{y_m}} \times \dfrac{\partial \dfrac{e^{\theta_m}}{\sum_j{e^{\theta_j}}}}{\partial \theta\_n}=-\sum_{m \neq n} y_m \times \dfrac{1}{\hat{y_m}} \times \dfrac{0-e^{\theta_n}e^{\theta_m}}{(\sum_j{e^{\theta\_j}})^2}\) \(=-\sum_{m \neq n} y_m \times \dfrac{1}{\hat{y_m}} \times \dfrac{-e^{\theta_n}}{\sum_j{e^{\theta_j}}} \times \dfrac{e^{\theta_m}}{\sum_j{e^{\theta\_j}}}=-\sum_{m \neq n} y_m \times \dfrac{1}{\hat{y_m}} \times (-\hat{y_n}) \times \hat{y\_m}=\sum_{m \neq n}y_m \hat{y_n}\) 因此 \(\dfrac{\partial CE}{\partial \theta_i}=-y_i(1-\hat{y\_i}) + \sum_{k \neq i}y_k \hat{y_i}=-y_i+\hat{y\_i}^2 + \sum_{k \neq i}y_k \hat{y_i}=\sum_k{y_k \hat{y_i}}-y_i=\hat{y_i}\sum_k{y_k}-y_i=\hat{y_i}-y_i\) (c) Backpropagation Derive the gradients with respect to the inputs \(x\) to an one-hidden-layer neural network (that is, find \(\frac{\partial J}{\partial x}\) where \(J = CE(y, \hat{y})\) is the cost function for the neural network). The neural network employs sigmoid activation function for the hidden layer, and softmax for the output layer. Assume the one-hot label vector is \(y\), and cross entropy cost is used. (Feel free to use \(\sigma&#39;(x)\) as the shorthand for sigmoid gradient, and feel free to define any variables whenever you see fit.) Recall that the forward propagation is as follows \[h = sigmoid(xW_1 + b_1), \hat{y} = softmax(hW_2 + b_2)\] Note that here we’re assuming that the input vector (thus the hidden variables and output probabilities) is a row vector to be consistent with the programming assignment. When we apply the sigmoid function to a vector, we are applying it to each of the elements of that vector. \(W_i\) and \(b_i (i = 1, 2)\) are the weights and biases, respectively, of the two layers. solution 令 \(z_1=xW_1+b_1,z_2=hW_2+b_2\) 則 \(\dfrac{\partial J}{\partial x}=\dfrac{\partial J}{\partial z2}\dfrac{\partial z2}{\partial h}\dfrac{\partial h}{\partial z1}\dfrac{\partial z1}{\partial x}=(\hat{y}-y)\times W_2^\top\times h(1-h)\times W_1^\top\) (d) 類神經網路的參數數量 How many parameters are there in this neural network, assuming the input is \(Dx\)-dimensional, the output is \(Dy\)-dimensional, and there are \(H\) hidden units? solution 根據上一題的圖，此神經網路有三個 layer：\(x,h,\hat{y}\)，其中的參數有 \(W_1,b_1,W_2,b_2\) 共四個： \(W_1: D_x \times H\) \(b_1: H\) \(W_2: H \times D_y\) \(b_2: D_y\) 總共有 \((D_x \times H)+H+(H \times D_y)+D_y=(D_x+1)H+(H+1)D_y\) 個參數。 (e) 實作 sigmoid function Fill in the implementation for the sigmoid activation function and its gradient in q2_sigmoid.py. Test your implementation using python q2_sigmoid.py. Again, thoroughly test your code as the provided tests may not be exhaustive. solution 實作出sigmoid以及其梯度sigmoid_grad兩個函式，直接利用(a)小題的結果可以輕鬆實現。 123def sigmoid(x): s = 1 / (1 + np.exp(-x)) return s 123def sigmoid_grad(s): ds = s * (1 - s) return ds (f) Gradient Check To make debugging easier, we will now implement a gradient checker. Fill in the implementation for gradcheck naive in q2_gradcheck.py. Test your code using python q2_gradcheck.py. solution 註解中指定使用 centered difference 方法來實作 gradient check，因為他比 forward / backward difference 方法誤差更小。 Forward difference approximation: \[f&#39;(x)\approx \dfrac{f(x+h)−f(x)}{h}\] Central difference approximations \[f&#39;(x)\approx \dfrac{f(x+h)-f(x-h)}{2h}\] Backward difference approximations: \[f&#39;(x)\approx \dfrac{f(x)−f(x−h)}{h}\] 1234567891011121314151617181920212223242526272829303132333435363738def gradcheck_naive(f, x): rndstate = random.getstate() random.setstate(rndstate) fx, grad = f(x) # Evaluate function value at original point h = 1e-4 # Do not change this! # Iterate over all indexes ix in x to check the gradient. it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite']) while not it.finished: ix = it.multi_index ### YOUR CODE HERE: x_upper, x_lower = x.copy(), x.copy() x_upper[ix] += h random.setstate(rndstate) f_upper = f(x_upper)[0] x_lower[ix] -= h random.setstate(rndstate) f_lower = f(x_lower)[0] numgrad = (f_upper - f_lower) / (2 * h) ### END YOUR CODE # Compare gradients reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix])) if reldiff &gt; 1e-5: print "Gradient check failed." print "First gradient error found at index %s" % str(ix) print "Your gradient: %f \t Numerical gradient: %f" % ( grad[ix], numgrad) return it.iternext() # Step to next dimension print "Gradient check passed!" (g) 實作類神經網路的 Forward 及 Backward Pass Now, implement the forward and backward passes for a neural network with one sigmoid hidden layer. Fill in your implementation in q2_neural.py. Sanity check your implementation with python q2_neural.py. solution 1234567891011121314151617181920212223242526272829303132333435def forward_backward_prop(X, labels, params, dimensions): ### Unpack network parameters (do not modify) ofs = 0 Dx, H, Dy = (dimensions[0], dimensions[1], dimensions[2]) W1 = np.reshape(params[ofs:ofs+ Dx * H], (Dx, H)) ofs += Dx * H b1 = np.reshape(params[ofs:ofs + H], (1, H)) ofs += H W2 = np.reshape(params[ofs:ofs + H * Dy], (H, Dy)) ofs += H * Dy b2 = np.reshape(params[ofs:ofs + Dy], (1, Dy)) # Note: compute cost based on `sum` not `mean`. ### YOUR CODE HERE: forward propagation h = sigmoid(np.dot(X, W1) + b1) y_hat = softmax(np.dot(h, W2) + b2) cost = cost = -np.sum(labels * np.log(y_hat)) ### END YOUR CODE ### YOUR CODE HERE: backward propagation d3 = y_hat - labels gradW2 = np.dot(h.T, d3) gradb2 = np.sum(d3, axis=0) dh = np.dot(d3, W2.T) grad_h = sigmoid_grad(h) * dh gradW1 = np.dot(X.T, grad_h) gradb1 = np.sum(grad_h, axis=0) ### END YOUR CODE ### Stack gradients (do not modify) grad = np.concatenate((gradW1.flatten(), gradb1.flatten(), gradW2.flatten(), gradb2.flatten())) return cost, grad 3. word2vec (a) 計算 Loss Function 對 \(v_c\) 的 gradient Assume you are given a “predicted” word vector \(v_c\) corresponding to the center word \(c\) for Skip-Gram, and word prediction is made with the softmax function found in word2vec models \[\hat{y_o}=p(o|c)=\dfrac{exp(u_o^\top v\_c)}{\sum\limits_{w=1}^V{exp(u_w^\top v_c)}}\] where \(u_w (w = 1, . . . , V )\) are the “output” word vectors for all words in the vocabulary. Assuming cross entropy cost is applied to this prediction and word o is the expected word (the o-th element of the one-hot label vector is one), derive the gradients with respect to \(v_c\). Hint: It will be helpful to use notation from question 2. For instance, letting \(\hat{y}\) be the vector of softmax predictions for every word, \(y\) as the expected word vector, and the loss function \[J_{softmax−CE}(o, v_c, U) = CE(y,\hat{y})\] where \(U = [u_1,u_2, · · · ,u_V ]\) is the matrix of all the output vectors. Make sure you state the orientation of your vectors and matrices. solution 求 \(\dfrac{\partial J}{\partial v_c}\) \(J=-\sum_i y_i log\hat{y_i}=-log \dfrac{exp(u_o^\top v\_c)}{\sum\limits_{w=1}^V exp(u_w^\top v_c)}=-log(exp(u_o^\top v\_c))+log(\sum\limits_{w=1}^V exp(u_w^\top v_c))\) \(\dfrac{\partial J}{\partial v_c}=\dfrac{\partial -log(exp(u_o^\top v_c))}{\partial v\_c}+\dfrac{\partial log(\sum_{w=1}^V exp(u_w^\top v_c))}{\partial v_c}\) \(\dfrac{-1}{exp(u_o^\top v_c)} \times exp(u_o^\top v_c) \times u_o=-u_o\) \(\dfrac{1}{\sum\limits_{w=1}^V exp(u_w^\top v\_c)} \times \sum\limits_{x=1}^V exp(u_x^\top v_c) \times u\_x=\sum\limits\_{x=1}^V \dfrac{exp(u_x^\top v\_c)}{\sum\limits_{w=1}^V exp(u_w^\top v_c)}\times u\_x=\sum\limits\_{x=1}^V \hat{y_x} u_x\) \(\dfrac{\partial J}{\partial v_c}=-u\_o+\sum\limits_{x=1}^V \hat{y_x} u_x=U(\hat{y}-y)\) (b) 計算 Loss Function 對 \(u_k\) 的 Gradient As in the previous part, derive gradients for the “output” word vectors \(u_k\)’s (including \(u_o\)). solution \(\dfrac{\partial J}{\partial u_w}=\dfrac{\partial -log(exp(u_o^\top v_c))}{\partial u\_w}+\dfrac{\partial log(\sum_{w=1}^V exp(u_w^\top v_c))}{\partial u_w}\) \(1. w=o\) \(=-v\_c+\dfrac{1}{\sum_{x=1}^V exp(u_x^\top v\_c)} \times \sum\limits_{w=1}^{V}exp(u_w^\top v_c) \times v_c=v\_c \times \sum\limits_{w=1}^V \dfrac{exp(v_w^\top v\_c)}{\sum\limits_{x=1}^V exp(u_x^\top v\_c)}-v\_c=\sum\limits\_{w=1}^V \hat{y_w}v_c-v_c=v_c(\hat{y}-y)^\top\) \(2. w\neq o\) \(=0+\dfrac{1}{\sum_{x=1}^V exp(u_x^\top v\_c)} \times \sum\limits_{w=1}^{V}exp(u_w^\top v_c) \times v_c=v\_c \times \sum\limits_{w=1}^V \dfrac{exp(v_w^\top v\_c)}{\sum\limits_{x=1}^V exp(u_x^\top v\_c)}=\sum\limits\_{w=1}^V \hat{y_w}v_c\) (c) Negative Sampling Repeat part (a) and (b) assuming we are using the negative sampling loss for the predicted vector \(v_c\), and the expected output word index is \(o\). Assume that \(K\) negative samples (words) are drawn, and they are \(1, 2, ..., K\) respectively for simplicity of notation \((o \notin {1, . . . , K})\). Again, for a given word, \(o\), denote its output vector as \(u_o\). The negative sampling loss function in this case is \[J_{neg-sample}(o,V_c,U)=-log(\sigma(u_o^\top v\_c))-\sum\limits_{k=1}^K log(\sigma(-u_k^\top v_c))\] where \(σ(·)\) is the sigmoid function. After you’ve done this, describe with one sentence why this cost function is much more efficient to compute than the softmax-CE loss (you could provide a speed-up ratio, i.e., the runtime of the softmaxCE loss divided by the runtime of the negative sampling loss). Note: the cost function here is the negative of what Mikolov et al had in their original paper, because we are doing a minimization instead of maximization in our code. solution \(1. \mathbf{\dfrac{\partial J}{\partial v_c}}=-dfrac{1}{\sigma(u_o^\top v_c)}\times \sigma(u_o^\top v_c) \times (1-\sigma(u_o^\top v_c)) \times v\_o - \sum\limits\_{k=1}^K \dfrac{1}{\sigma(-u_k^\top v_c)} \times \sigma(-u_k^\top v_c) \times (1-\sigma(-u_k^\top v_c)) \times (-u_k)\) \(=-u_o(1-\sigma(u_o^\top v\_c))+\sum\limits\_{k=1}^K u_k(1-\sigma(-u_k^\top v_c))\) \(2. \mathbf{\dfrac{\partial J}{\partial u_o}}=-\dfrac{1}{\sigma(u_o^\top v_c)}\times \sigma(u_o^\top v_c) \times (1-\sigma(u_o^\top v_c)) \times v_c\) \(=-v_c(1-\sigma(u_o^\top v_c))=(\sigma(u_o^\top v_c)-1)v_c\) \(3. \mathbf{\dfrac{\partial J}{\partial u_k}}=-\dfrac{1}{\sigma(-u_k^\top v_c)}\times \sigma(-u_k^\top v_c) \times (1-\sigma(-u_k^\top v_c)) \times (-v_c)\) \(=(1-\sigma(-u_k^\top v_c))v_c,k=1,2,...,K\) (d) 計算 Skip-gram 及 CBOW 的 Gradient Suppose the center word is \(c = w\_t\) and the context words are \([w\_{t−m}, . . ., w_{t−1}, w\_t, w\_{t+1},. . ., w_{t+m}]\), where \(m\) is the context size. Derive gradients for all of the word vectors for Skip-Gram and CBOW given the previous parts. Hint: feel free to use \(F(o, v\_c)\) (where o is the expected word) as a placeholder for the \(J_{softmax−CE}(o, v\_c, ...)\) or \(J_{neg−sample}(o, v_c, ...)\) cost functions in this part — you’ll see that this is a useful abstraction for the coding part. That is, your solution may contain terms of the form \(\frac{\partial F(o,v_c)} {\partial ...}\) . Recall that for skip-gram, the cost for a context centered around \(c\) is \[J\_{skip-gram}(w\_{t−m...t+m}) = \sum\_{−m≤j≤m,j\neq 0}F(w_{t+j} , v_c)\] where \(w_{t+j}\) refers to the word at the j-th index from the center. CBOW is slightly different. Instead of using \(v_c\) as the predicted vector, we use \(\hat{v}\) defined below. For (a simpler variant of) CBOW, we sum up the input word vectors in the context \[\hat{v} = \sum\_{−m≤j≤m,j\neq =0}v_{w_t+j}\] then the CBOW cost is \[J\_{CBOW}(w_{c−m...c+m}) = F(w_t, \hat{v})\] Note: To be consistent with the \(\hat{v}\) notation such as for the code portion, for skip-gram \(\hat{v} = v_c\). solution Skip-gram \(\dfrac{\partial J\_{skip-gram}(w\_{t-m...t+m})}{\partial U}=\sum\_{-m≤j≤m,j\neq 0} \dfrac{\partial F(w\_{t+j},v_c)}{\partial U}\) \(\dfrac{\partial J\_{skip-gram}(w\_{t-m...t+m})}{\partial v\_c}=\sum\_{-m≤j≤m,j\neq 0} \dfrac{\partial F(w\_{t+j},v_c)}{\partial v_c}\) \(\dfrac{\partial J\_{skip-gram}(w\_{t-m...t+m})}{\partial v_{t+j}}=0,\forall j \neq c\) CBOW \(\dfrac{\partial J\_{CBOW}(w\_{t-m...t+m})}{\partial U}=\dfrac{\partial F(w_t,\hat{v})}{\partial U}\) \(\dfrac{\partial J\_{CBOW}(w\_{t-m...t+m})}{\partial v\_{w\_{t+j}}}=\dfrac{\partial F(w_t,\hat{v})}{\partial \hat{v}},\forall j \in \{-m,...,-1,1,...,m\}\) \(\dfrac{\partial J\_{CBOW}(w\_{t-m...t+m})}{\partial U}=0,\forall j \notin \{-m,...,-1,1,...,m\}\) (e) 實作 Skip-gram In this part you will implement the word2vec models and train your own word vectors with stochastic gradient descent (SGD). First, write a helper function to normalize rows of a matrix in q3_word2vec.py. In the same file, fill in the implementation for the softmax and negative sampling cost and gradient functions. Then, fill in the implementation of the cost and gradient functions for the skipgram model. When you are done, test your implementation by running python q3_word2vec.py. Note: If you choose not to implement CBOW (part h), simply remove the NotImplementedError so that your tests will complete. solution 實作q3_word2vec.py中的以下函式： normalizeRows softmaxCostAndGradient negSamplingCostAndGradient skipgram normalizeRows 對每個列向量進行正規化，公式：\(\hat{v}=\dfrac{v}{|v|}\)。 1234def normalizeRows(x): length = np.sqrt(np.sum(np.power(x, 2), axis=1)) x /= length[:,None] return x softmaxCostAndGradient predicted: \(v_c\)、target: \(o\)、outputVector: \(U^\top\)、gradz2: \(\hat{y}-y\)、gradPred: \(\dfrac{\partial J}{\partial v_c}\)、grad: \(\dfrac{\partial J}{\partial u_k}\) 利用 (a)(b) 小題的結果： \(\dfrac{\partial J}{\partial v_c}=U(\hat{y}-y)\) \(\dfrac{\partial J}{\partial u_k}=v_c(\hat{y}-y)^\top\) 123456789101112def softmaxCostAndGradient(predicted, target, outputVectors, dataset): ### YOUR CODE HERE y_hat = softmax(np.dot(outputVectors, predicted)) cost = -np.log(y_hat[target]) gradz2 = y_hat.copy() gradz2[target] -= 1.0 gradPred = np.dot(outputVectors.T, gradz2) grad = np.outer(gradz2, predicted) ### END YOUR CODE return cost, gradPred, grad negSamplingCostAndGradient 利用 (c) 小題的結果： gradPred: \(\mathbf{\dfrac{\partial J}{\partial v_c}}=-u_o(1-\sigma(u_o^\top v\_c))+\sum\limits\_{k=1}^K u_k(1-\sigma(-u_k^\top v_c))\) grad: \(\mathbf{\dfrac{\partial J}{\partial u_o}}=(\sigma(u_o^\top v_c)-1)v_c\) grad: \(\mathbf{\dfrac{\partial J}{\partial u_k}}=(1-\sigma(-u_k^\top v_c))v_c,k=1,2,...,K\) cost function 為 \(CE(y,\hat{y})\) 1234567891011121314151617181920212223def negSamplingCostAndGradient(predicted, target, outputVectors, dataset, K=10): # Sampling of indices is done for you. Do not modify this if you # wish to match the autograder and receive points! indices = [target] indices.extend(getNegativeSamples(target, dataset, K)) ### YOUR CODE HERE grad = np.zeros(outputVectors.shape) gradPred = np.zeros(predicted.shape) z = sigmoid(np.dot(outputVectors[target].T, predicted)) cost = -np.log(z) grad[target] += (z - 1.0) * predicted gradPred += (z - 1.0) * outputVectors[target] for k in xrange(1, K + 1): index = indices[k] z = sigmoid(np.dot(-outputVectors[index].T, predicted)) cost -= np.log(z) grad[index] -= (z - 1.0) * predicted gradPred -= (z - 1.0) * outputVectors[index] ### END YOUR CODE return cost, gradPred, grad skipgram 利用 (d) 小題的結果： Skip-gram gradOut:\(\dfrac{\partial J\_{skip-gram}(w\_{t-m...t+m})}{\partial U}=\sum\_{-m≤j≤m,j\neq 0} \dfrac{\partial F(w\_{t+j},v_c)}{\partial U}\) gradIn: \(\dfrac{\partial J\_{skip-gram}(w\_{t-m...t+m})}{\partial v\_c}=\sum\_{-m≤j≤m,j\neq 0} \dfrac{\partial F(w\_{t+j},v_c)}{\partial v_c}\) gradIn: \(\dfrac{\partial J\_{skip-gram}(w\_{t-m...t+m})}{\partial v_{t+j}}=0,\forall j \neq c\) 1234567891011121314151617181920def skipgram(currentWord, C, contextWords, tokens, inputVectors, outputVectors, dataset, word2vecCostAndGradient=softmaxCostAndGradient): cost = 0.0 gradIn = np.zeros(inputVectors.shape) gradOut = np.zeros(outputVectors.shape) ### YOUR CODE HERE current_word_index = tokens[currentWord] vc = inputVectors[current_word_index] for j in contextWords: target_index = tokens[j] c_cost, c_grad_in, c_grad_out = word2vecCostAndGradient(vc, target_index, outputVectors, dataset) cost += c_cost gradIn[current_word_index] += c_grad_in gradOut += c_grad_out ### END YOUR CODE return cost, gradIn, gradOut 執行python q3_word2vec.py，產生以下結果代表成功。 1234567Testing normalizeRows...[[ 0.6 0.8 ] [ 0.4472136 0.89442719]]==== Gradient check for skip-gram ====Gradient check passed!Gradient check passed! (f) Stochastic Gradient Descent Complete the implementation for your SGD optimizer in q3_sgd.py. Test your implementation by running python q3_sgd.py. solution step就是 learning rate，原始程式碼已經給定，直接將其乘上 gradient 即可。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849def sgd(f, x0, step, iterations, postprocessing=None, useSaved=False, PRINT_EVERY=10): # Anneal learning rate every several iterations ANNEAL_EVERY = 20000 if useSaved: start_iter, oldx, state = load_saved_params() if start_iter &gt; 0: x0 = oldx step *= 0.5 ** (start_iter / ANNEAL_EVERY) if state: random.setstate(state) else: start_iter = 0 x = x0 if not postprocessing: postprocessing = lambda x: x expcost = None for iter in xrange(start_iter + 1, iterations + 1): # Don't forget to apply the postprocessing after every iteration! # You might want to print the progress every few iterations. cost = None ### YOUR CODE HERE cost, grad = f(x) x -= step * grad postprocessing(x) ### END YOUR CODE if iter % PRINT_EVERY == 0: if not expcost: expcost = cost else: expcost = .95 * expcost + .05 * cost print "iter %d: %f" % (iter, expcost) if iter % SAVE_PARAMS_EVERY == 0 and useSaved: save_params(iter, x) if iter % ANNEAL_EVERY == 0: step *= 0.5 return x (g) 訓練詞向量 Show time! Now we are going to load some real data and train word vectors with everything you just implemented! We are going to use the Stanford Sentiment Treebank (SST) dataset to train word vectors, and later apply them to a simple sentiment analysis task. You will need to fetch the datasets first. To do this, run sh get datasets.sh. There is no additional code to write for this part; just run python q3_run.py. Note: The training process may take a long time depending on the efficiency of your implementation (an efficient implementation takes approximately an hour). Plan accordingly! When the script finishes, a visualization for your word vectors will appear. It will also be saved as q3_word vectors.png in your project directory. Include the plot in your homework write up. Briefly explain in at most three sentences what you see in the plot. solution 執行sh get datasets.sh下載 Stanford Sentiment Treebank (SST) 資料集。 接著再執行python q3_run.py開始訓練，預設iteration=40000，每 10 次 iteration 會印出一次當前的 cost，沒意外的話 cost 應該要隨著訓練的過程遞減。 12345678iter 39950: 9.435311iter 39960: 9.492463iter 39970: 9.520291iter 39980: 9.524589iter 39990: 9.550077iter 40000: 9.577164sanity check: cost at convergence should be around or below 10training took 4038 seconds 訓練時間約為 67 分鐘，訓練使用的處理器為 i7-8750H，可以參考一下。 訓練完成後會產生一張圖q3_word_vectors.png： q3_word_vectors.png 在q3_run.py中可以發現在原始程式碼中，挑選了一些詞來進行視覺化，透過 SVD 降維後使得能夠在二維平面上觀察這些詞的相對距離。 (h) 實作 CBOW Implement the CBOW model in q3_word2vec.py. Note: This part is optional but the gradient derivations for CBOW in part (d) are not!. solution 同樣利用 (d) 小題的結果： gradOut: \(\dfrac{\partial J\_{CBOW}(w\_{t-m...t+m})}{\partial U}=\dfrac{\partial F(w_t,\hat{v})}{\partial U}\) gradIn: \(\dfrac{\partial J\_{CBOW}(w\_{t-m...t+m})}{\partial v\_{w\_{t+j}}}=\dfrac{\partial F(w_t,\hat{v})}{\partial \hat{v}},\forall j \in \{-m,...,-1,1,...,m\}\) gradIn: \(\dfrac{\partial J\_{CBOW}(w\_{t-m...t+m})}{\partial U}=0,\forall j \notin \{-m,...,-1,1,...,m\}\) 1234567891011121314151617def cbow(currentWord, C, contextWords, tokens, inputVectors, outputVectors, dataset, word2vecCostAndGradient=softmaxCostAndGradient): cost = 0.0 gradIn = np.zeros(inputVectors.shape) gradOut = np.zeros(outputVectors.shape) ### YOUR CODE HERE indices = [tokens[contextWord] for contextWord in contextWords] v_hat = np.sum(inputVectors[indices], axis=0) target_index = tokens[currentWord] cost, grad_in, gradOut = word2vecCostAndGradient(v_hat, target_index, outputVectors, dataset) for j in indices: gradIn[j] += grad_in ### END YOUR CODE return cost, gradIn, gradOut 執行python q3_word2vec.py，在完成 (e)(h) 小題後的結果應如下： 1234567891011Testing normalizeRows...[[ 0.6 0.8 ] [ 0.4472136 0.89442719]]==== Gradient check for skip-gram ====Gradient check passed!Gradient check passed!==== Gradient check for CBOW ====Gradient check passed!Gradient check passed! 4. Sentiment Analysis Now, with the word vectors you trained, we are going to perform a simple sentiment analysis. For each sentence in the Stanford Sentiment Treebank dataset, we are going to use the average of all the word vectors in that sentence as its feature, and try to predict the sentiment level of the said sentence. The sentiment level of the phrases are represented as real values in the original dataset, here we’ll just use five classes: “very negative (−−)”, “negative (−)”, “neutral”, “positive (+)”, “very positive (++)” which are represented by 0 to 4 in the code, respectively. For this part, you will learn to train a softmax classifier, and perform train/dev validation to improve generalization. (a) 以特徵向量來表示一個句子 Implement a sentence featurizer. A simple way of representing a sentence is taking the average of the vectors of the words in the sentence. Fill in the implementation in q4_sentiment.py. solution 將句子中所有詞的詞向量進行平均，將得到的向量用來表示整個句子。 Inputs: tokens: 一個 dictionary，key 為詞，value 為該詞的索引 wordVectors: 詞向量 sentence: 一個句子 Output: sentVector: 句子的特徵向量 12345678910111213def getSentenceFeatures(tokens, wordVectors, sentence): sentVector = np.zeros((wordVectors.shape[1],)) ### YOUR CODE HERE for word in sentence: index = tokens[word] sentVector += wordVectors[index] sentVector /= float(len(sentence)) ### END YOUR CODE assert sentVector.shape == (wordVectors.shape[1],) return sentVector (b) 正規化 (Regularization) Explain in at most two sentences why we want to introduce regularization when doing classification (in fact, most machine learning tasks). solution 正規化可以避免 overfitting，使得模型更加 generalization。 (c) 尋找超參數 (hyperparameter) Fill in the hyperparameter selection code in q4_sentiment.py to search for the “optimal” regularization parameter. You need to implement both getRegularizationValues and chooseBestModel. Attach your code for chooseBestModel to your written write-up. You should be able to attain at least 36.5% accuracy on the dev and test sets using the pretrained vectors in part (d). solution 參考其他人的做法，使用 log function 產生遞增的值。 123456def getRegularizationValues(): values = None # Assign a list of floats in the block below ### YOUR CODE HERE values = np.logspace(-5, 2, num=100, base=10) ### END YOUR CODE return sorted(values) 12345678def chooseBestModel(results): bestResult = None ### YOUR CODE HERE bestResult = max(results, key=lambda x: x["dev"]) ### END YOUR CODE return bestResult (d) 模型比較 Run python q4_sentiment.py --yourvectors to train a model using your word vectors from q3. Now, run python q4_sentiment.py --pretrained to train a model using pretrained GloVe vectors (on Wikipedia data). Compare and report the best train, dev, and test accuracies. Why do you think the pretrained vectors did better? Be specific and justify with 3 distinct reasons. yourvectors Reg Train Dev Test 1.00E-05 31.004 32.516 30.452 1.15E-04 31.063 32.516 30.362 1.12E-03 31.133 32.516 30.362 1.10E-02 30.922 32.334 29.955 1.07E-01 30.290 31.789 29.864 1.05E+00 28.816 29.609 27.059 12Best regularization value: 2.26E-05Test accuracy (%): 30.316742 pretrained GloVe vectors (on Wikipedia data) Reg Train Dev Test 1.00E-05 39.923 36.421 37.059 1.15E-04 39.958 36.512 37.014 1.12E-03 39.899 36.331 37.014 1.10E-02 39.923 36.331 37.195 1.07E-01 39.782 36.240 37.149 1.05E+00 39.478 36.512 37.330 12Best regularization value: 1.20E+01Test accuracy (%): 37.556561 觀察兩者的表現可以發現使用 pretrained 的模型效果明顯比較好，可能原因如下： 更高維度的詞向量可能包含更多的資訊 GloVe 使用更大的語料庫進行訓練 word2vec 與 GloVe 的差異 (參考資料) (e) Regularization Term 的變化在 train set 及 dev set 的表現差異 Plot the classification accuracy on the train and dev set with respect to the regularization value for the pretrained GloVe vectors, using a logarithmic scale on the x-axis. This should have been done automatically. Include q4_reg acc.png in your homework write up. Briefly explain in at most three sentences what you see in the plot. solution (f) Confusion Matrix We will now analyze errors that the model makes (with pretrained GloVe vectors). When you ran python q4_sentiment.py --pretrained, two files should have been generated. Take a look at q4_dev_conf.png and include it in your homework writeup. Interpret the confusion matrix in at most three sentences. solution 由預測結果與標準答案的數量比較所產生的 confusion matrix，由左上到右下的對角線所經過的格子為 True Positive 的數量。 (g) 為何分類錯誤？ Next, take a look at q4_dev_pred.txt. Choose 3 examples where your classifier made errors and briefly explain the error and what features the classifier would need to classify the example correctly (1 sentence per example). Try to pick examples with different reasons. solution 將每筆分類結果與標準答案的差距取絕對值，統計如下： 差距 0 1 2 3 4 數量 409 444 188 59  1 可以看到其實大部分的誤差都在 1 分以內，若將標準降低為差距 1 分或以下就算分類正確，準確率會從原來的 37% 大幅提高到 77%，因此整體的預測其實還算是準確的。 Answer Predicted Sentence Possible Issue 0 4 a lackluster , unessential sequel to the classic disney adaptation of j.m. barrie 's peter pan . 唯一一筆預測與實際分數差距4分的資料，看起來 lackluster, unessential 都是負面的詞彙，classic 則是偏向正面的詞彙，而 j.m. barrie 's peter pan 則比較算是雜訊，其餘則是稍微中性的詞語，若將專有名詞去掉或許能提升預測的分數 4 1 the draw -lrb- for `` big bad love '' -rrb- is a solid performance by arliss howard . 太多無意義的符號干擾 4 1 it is amusing , and that 's all it needs to be . amusing 應該比較偏向正面的詞彙，但是其餘的詞感覺大部分都是冗詞。]]></content>
      <categories>
        <category>學校課程</category>
        <category>自然語言處理</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode#204 Count Primes]]></title>
    <url>%2F2018%2F09%2F10%2FLeetCode-204-Count-Primes%2F</url>
    <content type="text"><![CDATA[¶問題 經典問題，給一個非負整數 n ，問小於 n 的質數有幾個。 ¶輸入與輸出 Input: 10 Output: 4 Explanation: There are 4 prime numbers less than 10, they are 2, 3, 5, 7. ¶方法 建質數表。 方法一： 逐一檢查 1 ~ n - 1 的整數中有多少個質數，但效率不好，因此採用第二種方法。 方法二： 埃拉托斯特尼篩法，從 2 開始將已知質數的倍數標記成合數，可以減少很多不必要的計算。 123456789101112131415161718int countPrimes(int n)&#123; int i, j, prime_count = 0; bool* primes = malloc(n * sizeof(bool)); memset(primes, true, n * sizeof(bool)); for (i = 2; i &lt; n; i++) &#123; if (primes[i]) &#123; prime_count++; for (j = 2 * i; j &lt; n; j += i) primes[j] = false; &#125; &#125; return prime_count;&#125;]]></content>
      <categories>
        <category>程式解題</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>C</tag>
        <tag>Prime</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode#198 House Robber]]></title>
    <url>%2F2018%2F09%2F10%2FLeetCode-198-House-Robber%2F</url>
    <content type="text"><![CDATA[¶問題 有一個職業盜賊想要在某條街上行竊，這條街上有著一整排的房屋，每棟房屋內都有特定數量的金錢，他希望能夠偷到越多錢越好。但是想在這條街行竊有個限制，每兩棟相鄰的房屋之間都有連結保全系統，若他行竊兩棟相鄰的房屋，保全系統就會自動聯繫警察。 給定一個非負整數的list，表示每間房屋內的金錢數量，請問在不驚動保全系統的前提下，竊賊能夠偷到最多的金錢數量是多少？ ¶輸入與輸出 Input: [1,2,3,1] Output: 4 Explanation: Rob house 1 (money = 1) and then rob house 3 (money = 3). Total amount you can rob = 1 + 3 = 4. Input: [2,7,9,3,1] Output: 12 Explanation: Rob house 1 (money = 2), rob house 3 (money = 9) and rob house 5 (money = 1). Total amount you can rob = 2 + 9 + 1 = 12. ¶方法 使用動態規劃，假設 an 為行竊第 n 棟房屋能夠獲得的金錢數量，可得 an = max(an-2, an-3)。 1234567891011121314151617181920212223242526int max(int a, int b)&#123; if (a &gt; b) return a; else return b;&#125;int rob(int* nums, int numsSize) &#123; if (numsSize == 0) return 0; int i; if (numsSize &gt;= 3) nums[2] += nums[0]; for (i = 3; i &lt; numsSize; i++) nums[i] += max(nums[i - 2], nums[i - 3]); if (numsSize == 1) return nums[0]; else if (numsSize == 2) return max(nums[0], nums[1]); else return max(nums[i - 1], nums[i - 2]);&#125;]]></content>
      <categories>
        <category>程式解題</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>C</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[挑戰LeetCode Weekly Contest 50]]></title>
    <url>%2F2017%2F09%2F23%2F%E6%8C%91%E6%88%B0LeetCode-Weekly-Contest-50%2F</url>
    <content type="text"><![CDATA[¶解題心得 這次的題目有兩題都可以用遞迴來寫，但對我來說腦袋真的轉不過來啊！ 果然如俗話所說：「遞迴只應天上有，凡人應當用迴圈」，描述的真是貼切，但遞迴之美，總是讓我們這些凡人流連忘返，遞迴啊遞迴，究竟何時才能讓願意人們真正看清您的真面目呢？ ¶第1題 680. Valid Palindrome II ¶問題 給一個非空的字串s，在最多只能刪除一個字元的條件下，s是否能成為一個回文字串。 字串只包含小寫字母a-z，字串長度不超過50000。 ¶輸入與輸出 Input: &quot;aba&quot; Output: True Input: &quot;abca&quot; Output: True Explanation: You could delete the character 'c'. ¶方法 使用two pointer的技巧，low與high分別從前面與後面開始走，若遇到不一樣的字元，則檢查在刪除其中任一個字元後是否能滿足回文的條件。 1234567891011121314151617181920212223class Solution: def validPalindrome(self, s): low = 0 high = len(s) - 1 while low &lt; high: if s[low] == s[high]: low += 1 high -= 1 else: if self.isPalindrome(s, low+1, high): return True if self.isPalindrome(s, low, high-1): return True return False return True def isPalindrome(self, s, low, high): while low &lt; high: if s[low] != s[high]: return False low += 1 high -= 1 return True ¶第2題 677. Map Sum Pairs ¶問題 實作一個字典，能夠插入一對(key, value)，其中key為字串，value為整數。 輸入一個字串prefix，輸出字典中所有以prefix為開頭的所有key所對應的value的和。 ¶輸入與輸出 Input: &quot;aba&quot; Output: True Input: &quot;abca&quot; Output: True Explanation: You could delete the character 'c'. ¶方法 直接使用python的內建dict。 1234567891011121314151617class MapSum: def __init__(self): self.d = dict() def insert(self, key, val): self.d[key] = val def sum(self, prefix): summation = 0 length = len(prefix) for k, v in self.d.items(): if k[:length] == prefix: summation += v return summation ¶第3題 678. Valid Parenthesis String ¶問題 輸入一個字串，字串內的元素只包含(,),*三種，判斷此字串是否滿足以下條件： 任何一個左方的(都必須對應到一個右方的) 任何一個右方的)都必須對應到一個左方的( 左方的(必須出現在右方的)之前 *可以是(或)或空字串其中一種 空字串是合法的 ¶輸入與輸出 Input: &quot;()&quot; Output: True Input: &quot;(*)&quot; Output: True Input: &quot;(*))&quot; Output: True ¶方法 使用一個變數p紀錄括號的消長，出現一組(,)便可抵消。 至於出現*的話，因為有三種可能，因此用backtracking分支成三種可能，只要其中一種能夠滿足條件就代表合法。 123456789101112131415161718class Solution: def checkValidString(self, s): return self.backtracking(list(s), 0, 0) def backtracking(self, c, p, index): if p &lt; 0: return False for i in range(index, len(c)): x = c[i] if x == '(': p += 1 elif x == ')': if p &lt;= 0: return False p -= 1 else: return self.backtracking(c, p + 1, i + 1) or self.backtracking(c, p - 1, i + 1) or self.backtracking(c, p, i + 1) return p == 0 以上是使用python的解法，可惜跳出 TLE，因此將同樣的方法改寫成C++的版本，就可以通過了。 123456789101112131415161718192021222324class Solution &#123;public: bool checkValidString(string s) &#123; return backtracking(s, 0, 0); &#125; bool backtracking(string c, int p, int index) &#123; if (p &lt; 0) return false; for (int i = index; i &lt; c.length(); i++) &#123; char x = c[i]; if (x == '(') p++; else if (x == ')') &#123; if (p &lt;= 0) return false; p--; &#125; else return backtracking(c, p+1, i+1) || backtracking(c, p-1, i+1) || backtracking(c, p, i+1); &#125; return p == 0; &#125;&#125;; ¶第4題 679. 24 Game ¶問題 給四個範圍1~9的整數，問是否能經過+,-,*,/,(,)的運算後的得到結果為24。 ¶輸入與輸出 Input: [4, 1, 8, 7] Output: True Explanation: (8-4) * (7-1) = 24 Input: [1, 2, 1, 2] Output: False ¶方法 總共有 4 個數字以及 4 個運算子，括號的部份代表運算的先後順序，因此計算的步驟如下： 從 4 個數字中取 2 個(順序有分)，再從 4 個運算子當中選 1 個，所以有 4 x 5 x 3 種選法 第 1 步驟運算完後數字剩下 3 個，再從 3 個數字中取 2 個(順序有分)，同樣從 4 個運算子當中選 1 個，所以有 3 x 2 x 4 種選法 第 2 步驟運算完後數字剩下 2 個，再從 2 個數字當中取 2 個(順序有分)，從 4 個運算子中選一個，所以有 2 x 4 種選法 因此總共有 4 x 3 x 3 x 3 x 2 x 4 x 2 x 4 = 9216 種組合方法 所以用遞迴枚舉出所有的方法就可以將所有答案計算出來。要注意的是除數不能為 0 ，還有由於浮點數運算的誤差，因此計算結果未必剛好等於 24 ，可能會有非常微小的誤差值，所以要給一個容忍值才行。 1234567891011121314class Solution(object): def judgePoint24(self, A): if len(A) == 1: if abs(A[0] - 24) &lt; 1e-6: return True for i in range(len(A)): for j in range(len(A)): if i != j: B = [A[x] for x in range(len(A)) if x != i and x != j] if self.judgePoint24([A[i] + A[j]] + B) or self.judgePoint24([A[i] - A[j]] + B) or self.judgePoint24([A[i] * A[j]] + B): return True if A[j] and self.judgePoint24([A[i] / A[j]] + B): return True return False]]></content>
      <categories>
        <category>程式解題</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>Python</tag>
        <tag>LeetCode Weekly Contest</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[挑戰LeetCode Weekly Contest 49]]></title>
    <url>%2F2017%2F09%2F10%2F%E6%8C%91%E6%88%B0LeetCode-Weekly-Contest-49%2F</url>
    <content type="text"><![CDATA[¶三度挑戰 這次的題目，前兩題真的簡單，至於後兩題，真心難QQ ，最終成績 753 / 2362。 ¶第1題 674. Longest Continuous Increasing Subsequence ¶問題 給一個未排序的整數陣列，找出最長的連續遞增子序列的長度。 ¶輸入範例 1 [1,3,5,4,7] ¶輸出範例 1 3 ¶輸入範例 2 [2,2,2,2,2] ¶輸出範例 2 1 ¶方法 遞增序列的特性就是較晚出現的元素一定大於較先出現的元素，遍歷整個陣列，因為要求為「連續」，所以只要遇到不符合遞增條件的元素，就要重新計算。 1234567891011121314class Solution(object): def findLengthOfLCIS(self, nums): if not nums: return 0 maximum = 1 current = 1 for i in range(1, len(nums)): if nums[i - 1] &lt; nums[i]: current += 1 else: maximum = max(maximum, current) current = 1 maximum = max(maximum, current) return maximum ¶第2題 676. Implement Magic Dictionary ¶問題 定義一個陣列，由許多字串組成，輸入一個字串，若陣列中任一字串更改一個字母後，與輸入的字串相同，回傳True，反之則回傳False。 ¶輸入與輸出 Input: buildDict([&quot;hello&quot;, &quot;leetcode&quot;]), Output: Null Input: search(&quot;hello&quot;), Output: False Input: search(&quot;hhllo&quot;), Output: True Input: search(&quot;hell&quot;), Output: False Input: search(&quot;leetcoded&quot;), Output: False ¶方法 這題沒有什麼特殊的技巧，只要將輸入字串與陣列內的字串一一比對即可。 1234567891011121314151617181920class MagicDictionary(object): def __init__(self): self.magic = [] def buildDict(self, dic): self.magic = dic def search(self, word): for m in self.magic: modify = 0 if len(m) == len(word): for i in range(len(m)): if m[i] != word[i]: modify += 1 if modify == 1: return True return False ¶第3題 675. Cut Off Trees for Golf Event ¶問題 輸入一個二維陣列，0代表無法通過的障礙物，1代表可以通行的草皮，大於1的數代表一棵樹，同樣可以通行，數字大小代表樹的高度。 今天我們要根據樹的高度由低到高進行修剪，經過修剪後的樹會變成草皮，我們要從(0, 0)出發，依序走訪所有的樹，輸出修剪所有的樹所需經過最短的距離，若有任何一棵樹無法抵達，則輸出-1。 ¶輸入與輸出 Input: [ [1,2,3], [0,0,4], [7,6,5] ] Output: 6 Input: [ [1,2,3], [0,0,0], [7,6,5] ] Output: -1 Input: [ [2,3,4], [0,0,5], [8,7,6] ] Output: 6 Explanation: You started from the point (0,0) and you can cut off the tree in (0,0) directly without walking. ¶方法 我先將所有非0或1的數加入一個陣列中，將其排序後由小至大取出，利用BFS計算從目前位置到各點的最短距離，取出目前位置到欲修剪的距離。 很不幸的我的方法吃了一發 TLE ，應該是做的 BFS 有太多多餘的範圍了，應該只要找出目前位置到目的地的最短距離就好，之後若有時間再想辦法補上新的方法吧。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354class Solution(object): def cutOffTree(self, forest): import math self.forest = forest self.r, self.c = len(self.forest), len(self.forest[0]) f = [col for row in forest for col in row if col != 0 and col != 1] f.sort(reverse=True) total_step = 0 cur_pos = (0, 0) while f: self.num_step = [[math.inf for j in range(self.c)] for i in range(self.r)] self.num_step[cur_pos[0]][cur_pos[1]] = 0 self.min_step(cur_pos) next_cut = f.pop() next_pos = self.find_pos(next_cut) if not self.num_step[next_pos[0]][next_pos[1]] &lt; math.inf: return -1 total_step += self.num_step[next_pos[0]][next_pos[1]] cur_pos = next_pos return total_step def min_step(self, pos): i, j = pos[0], pos[1] # print(i, j, self.num_step[i][j]) if not (0 &lt;= i &lt; self.r and 0 &lt;= j &lt; self.c): return if 0 &lt;= i - 1 &lt; self.r and 0 &lt;= j &lt; self.c and self.forest[i - 1][j]: if self.num_step[i - 1][j] &gt;= self.num_step[i][j] + 1: self.num_step[i - 1][j] = self.num_step[i][j] + 1 self.min_step((i - 1, j)) if 0 &lt;= i + 1 &lt; self.r and 0 &lt;= j &lt; self.c and self.forest[i + 1][j]: if self.num_step[i + 1][j] &gt;= self.num_step[i][j] + 1: self.num_step[i + 1][j] = self.num_step[i][j] + 1 self.min_step((i + 1, j)) if 0 &lt;= i &lt; self.r and 0 &lt;= j - 1 &lt; self.c and self.forest[i][j - 1]: if self.num_step[i][j - 1] &gt;= self.num_step[i][j] + 1: self.num_step[i][j - 1] = self.num_step[i][j] + 1 self.min_step((i, j - 1)) if 0 &lt;= i &lt; self.r and 0 &lt;= j + 1 &lt; self.c and self.forest[i][j + 1]: if self.num_step[i][j + 1] &gt;= self.num_step[i][j] + 1: self.num_step[i][j + 1] = self.num_step[i][j] + 1 self.min_step((i, j + 1)) def find_pos(self, num): for i in range(len(self.forest)): if num in self.forest[i]: return (i, self.forest[i].index(num)) return -1 ¶第4題 673. Number of Longest Increasing Subsequence ¶問題 給一個未排序的整數陣列，找出最長的遞增子序列的數量。 ¶輸入與輸出 Input: [1,3,5,4,7] Output: 2 Explanation: The two longest increasing subsequence are [1, 3, 4, 7] and [1, 3, 5, 7]. Input: [2,2,2,2,2] Output: 5 Explanation: The length of longest continuous increasing subsequence is 1, and there are 5 subsequences' length is 1, so output 5. ¶方法 計算LIS的長度有兩種思維方式，第一種是找出哪些數字能接在nums[i]後面，第二種是找出nums[i]能接在哪些數字後面，這裡採用第二種方法。 這裡需要用到兩個陣列： LIS: LIS[i]代表以nums[i]結束的最長遞增子序列的長度 cnt: cnt[i]代表以nums[i]結束的最長遞增子序列的數量 假設nums[i]能接在nums[j]後面，代表nums[i] &gt; nums[j]，這裡有三種狀況可以討論： LIS[i] &gt; LIS[j] + 1 : nums[i]接在 nums[j]後面比原本還要短，不接 LIS[i] = LIS[j] + 1 : nums[i]接在 nums[j]後面比跟原本一樣長，只要把數量相加就好 LIS[i] &lt; LIS[j] + 1 : nums[i]接在 nums[j]後面比原本還要長，長度加1，繼承前面的數量 最後將所有LIS[i]為最大值所對應的cnt[i]相加即為答案。 123456789101112131415161718class Solution: def findNumberOfLIS(self, nums): if nums == []: return 0 # 初始化陣列，每個數字本身就是長度為1的LIS LIS, cnt = [1] * len(nums), [1] * len(nums) # nums[i]能接在哪些數字後面 for i in range(1, len(nums)): for j in range(0, i): if nums[i] &gt; nums[j]: if LIS[i] == LIS[j] + 1: cnt[i] += cnt[j] elif LIS[i] &lt; LIS[j] + 1: cnt[i] = cnt[j] LIS[i] = LIS[j] + 1 return sum((y for x, y in zip(LIS, cnt) if x == max(LIS)))]]></content>
      <categories>
        <category>程式解題</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>Python</tag>
        <tag>LeetCode Weekly Contest</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二元樹(Binary Tree)]]></title>
    <url>%2F2017%2F09%2F08%2F%E4%BA%8C%E5%85%83%E6%A8%B9-Binary-Tree%2F</url>
    <content type="text"><![CDATA[¶樹 Tree 生活中常見各式各樣的數狀圖，像是族譜、賽程表等等，至於在計算機科學中，tree則是一種抽象資料型別(Abstract Data Type)，是一種具有階層式 (Hierarchical) 的數狀資料集合。 ¶定義 樹 (tree) 是一個由一個或多個節點 (node) 所組成的集合，滿足以下兩點： 有一個特別的節點叫作「根」(root) 其餘的節點 (node) 可被分割為 n &gt;= 0 個互斥 (disjoint) 的集合 T1, …, Tn , 每個集合都是一棵樹，並且稱作根的「子樹」 (subtree) ¶名詞介紹 degree : 一個 node 子節點的個數，e.g. degree(A) = 3, degree© = 1, degree(F) = 0 leaf (terminal) : degree = 0 的 node，e.g. leaf = {K, L, F, G, M, I, J} parent &amp; children : A 是 B 的 parent, B 是 A 的 children siblings : 具有相同 parent 的 nodes, e.g. B, C, D 是 siblings degree of tree : 所有 node 的 degree 中最大值代表這棵數的 degree，degree 為 k 的樹又稱為 k-ary tree ancestors : 一個 node 走回 root 所要經過的 nodes, e.g. M 的 ancestors 有 A, D, H level : root 的 level 為 0 或 1，children 的 level 為 root 的 level + 1，以此類推 height (depth) : 所有 node 中 level 的最大值代表這棵數的 height (depth) ¶表示方法 List Representation Left Child-Right Sibling Representation Representation as Degree-Two Tree (Left Child-Right Child) ¶二元樹 Binary Tree 以上介紹的tree並沒有限制subtree的數量，但我們一般較常用的還是接下來要介紹的二元樹 (Binary Tree) 。 二元樹有以下幾點特性： 每個 node 的 degree 不超過 2 binary tree 可以不存在任何的節點 (empty) 需要區分左子樹與右子樹，也就是左右子樹互換位置的話就會形成另一個新的樹 ¶二元樹的表示方法 Array Representation 使用array來表示binary tree通常會具有以下的特性： parent index = ⌊ current index / 2 ⌋ leftChild index = current index * 2 rightChild index = current index * 2 + 1 但是使用array表示，常常會造成空間上的浪費，如： 能夠最有效利用array空間的樹是complete binary tree，像是heap就非常適合用array進行實作。 Linked Representation 另一種表示方法是使用linked list實作，雖然每個node都需要額外的空間來儲存link，但是對於空間上的利用卻能夠進行比較有效的管控。 ¶實作 以下使用python以Linked List實作Binary Tree。 首先定義樹的節點： 12345class TreeNode(): def __init__(self, data=None, left=None, right=None): self.data = data self.left = left self.right = right 接著是二元樹的類別以及基本方法： 12345678910111213141516class BinaryTree(): def __init__(self, root=None): self.root = root def isEmpty(self, node): return node is None def left_child(self, node): if self.isEmpty(node): return return node.left def right_child(self, node): if self.isEmpty(node): return return node.right ¶二元樹的遍歷 Traversal 若遍歷一棵樹具有以下三個步驟， L: moving left V: visiting the node R: moving right 則遍歷的方法就有，L,V,R 三種步驟的排列數 = 3! = 6 種: LVR, LRV, VLR, VRL, RLV, RVL 。 取其中三種當作遍歷的方法： preorder: VLR inorder: LVR postorder: LRV 記的方法很簡單，看 V 的位置就對了，若 V 的位置在前面就是preorder，在中間就是inorder，在後面就是postorder。 Inorder Traversal 12345def inorder(self, node): if node: self.inorder(node.left) print(node.data, end=" ") self.inorder(node.right) Preorder Traversal 12345def preorder(self, node): if node: print(node.data, end=" ") self.preorder(node.left) self.preorder(node.right) Postorder Traversal 12345def postorder(self, node): if node: self.postorder(node.left) self.postorder(node.right) print(node.data, end=" ") Level-Order Traversal 第四種遍歷的方法，依照node的level的順序依序拜訪各結點。 12345678910111213def levelorder(self, node): if not node: return from collections import deque queue = deque() queue.append(node) while queue: node = queue.popleft() print(node.data, end=" ") if node.left: queue.append(node.left) if node.right: queue.append(node.right) ¶其他操作 複製 12345# Return a Pointer to a same data from original nodedef copy(self, node): if node: return TreeNode(node.data, self.copy(node.left), self.copy(node.right)) return None 相等 1234@staticmethoddef equal(first, second): return (not first and not second) or (first and second and (first.data == second.data) \and BinaryTree.equal(first.left, second.left) and BinaryTree.equal(first.right, second.right)) (未完成，待補上…)]]></content>
      <categories>
        <category>學校課程</category>
        <category>資料結構</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Data Structure</tag>
        <tag>Tree</tag>
        <tag>Binary Tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[堆疊(Stack)與佇列(Queue)]]></title>
    <url>%2F2017%2F09%2F06%2F%E5%A0%86%E7%96%8A-Stack-%E8%88%87%E4%BD%87%E5%88%97-Queue%2F</url>
    <content type="text"><![CDATA[¶介紹 Stack 與 Queue 都是一種 抽象資料型別 (Abstract Data Type)，兩者的區別簡單來說就是Stack是 Last-In-First-Out (LIFO)，而 Queue 則是 First-In-First-Out (FIFO)。 ¶實作 以下用 Linked List 來實作 Stack 與 Queue ¶Stack 首先是 stack 的節點。 1234class StackNode(): def __init__(self, value=None, next=None): self.value = value self.next = None 接著定義函式，因為是後進先出，所以用 top 來紀錄最後進入的數，因此不管是 push 還是 pop 都是更動到 top 的值。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class Stack(): def __init__(self, top=None): self.top = top def print_nodes(self): current = self.top while current: end = " -&gt; " if current.next else "\n" print(current.value, end=end) current = current.next def push(self, value): old_top = self.top self.top = StackNode(value, old_top) self.top.next = old_top def pop(self): if self.isEmpty(): print("Pop nothing, the stack is empty!") return self.top = self.top.next def top_value(self): if self.isEmpty(): return "Nothing at the top, the stack is empty!" return self.top.value def search(self, value): count = 0 current = self.top while current: if current.value == value: return count count += 1 current = current.next def isEmpty(self): return self.top is None def size(self): count = 0 current = self.top while current: count += 1 current = current.next return count ¶Queue 接著是 queue 的節點。 1234class QueueNode(): def __init__(self, value=None, next=None): self.value = value self.next = next 因為 queue 為先進先出，因此需要有兩個變數來紀錄頭跟尾的位置，可以將 queue 想像成排隊的時候，add 就是將新的節點接在隊伍的後方，delete 則是將節點從隊伍的頭移除。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class Queue(): def __init__(self, front=None, rear=None): self.front = front self.rear = rear def print_queue(self): current = self.front while current: end = " -&gt; " if current.next else "\n" print(current.value, end=end) current = current.next def add(self, value): if self.isEmpty(): self.front = QueueNode(value) self.rear = self.front else: self.rear.next = QueueNode(value) self.rear = self.rear.next def delete(self): if self.isEmpty(): print("Delete nothing, the queue is empty.") else: self.front = self.front.next def search(self, value): count = 0 current = self.front while current: if current.value == value: return count count += 1 current = current.next def top_value(self): if self.isEmpty(): return "Nothing at the top, the stack is empty!" return self.front.value def end_value(self): if self.isEmpty(): return "Nothing at the end, the stack is empty!" return self.rear.value def isEmpty(self): return self.front is None 程式碼：Stack Queue]]></content>
      <categories>
        <category>學校課程</category>
        <category>資料結構</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Data Structure</tag>
        <tag>Stack</tag>
        <tag>Queue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode#167 Two Sum II - Input array is sorted feat. Two Pointers]]></title>
    <url>%2F2017%2F09%2F05%2FLeetCode-167-Two-Sum-II-Input-array-is-sorted-feat-Two-Pointers%2F</url>
    <content type="text"><![CDATA[¶問題 給定一個遞增的整數陣列nums(已排序)，其中有兩個數的和會等於目標target，回傳他們的位置，且同個位置不能重複選取。 ¶輸入 numbers = [2, 7, 11, 15] target = 9 ¶輸出 [1, 2] ¶方法 這題是LeetCode#1 Two Sums的改版，差別是這題輸入的陣列已經經過排序了，這題還是可以用hash table來解，不過這裡我們要用比較不一樣的方法，叫作two pointers 。 two pointers在這裡的使用方法，就是兩個箭頭分別指向陣列的頭跟尾，藉由將左邊的箭頭向右調整以及右邊的箭頭向左調整，逼近出所要求的值。 12345678910class Solution(object): def twoSum(self, numbers, target): left, right = 0, len(numbers) - 1 while left &lt; right: if numbers[left] + numbers[right] &lt; target: left += 1 elif numbers[left] + numbers[right] &gt; target: right -= 1 else: return [left + 1, right + 1] 以下的問題也常常用two pointers來解： 字串、陣列反轉 兩個跑者一快一慢在操場上奔馳，問兩人何時相遇 字串回文(Palindrome)]]></content>
      <categories>
        <category>程式解題</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>Python</tag>
        <tag>Two Pointers</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[挑戰LeetCode Weekly Contest 48]]></title>
    <url>%2F2017%2F09%2F04%2F%E6%8C%91%E6%88%B0LeetCode-Weekly-Contest-48%2F</url>
    <content type="text"><![CDATA[¶再度挑戰 這次的題目稍稍比上次簡單一點，前兩題解完大概還剩 20 分鐘，看看第三題好像也不難，但可惜還是沒寫完，最終成績 1170 / 2668，嗯…好像沒進步 Orz 。 ¶第1題 671. Second Minimum Node In a Binary Tree ¶問題 給一棵二元樹，每一個node的子節點不是 0 個就是 2 個，輸出所有節點當中第二小的值，若不存在則輸出-1。 ¶輸入 tree 的 root 範例： 2 / \ 2 5 / \ 5 7 ¶輸出 5 ¶方法 遍歷所有的節點，不斷刷新最小值(下界)與第二小的值(上界)，夾擠出真正第二小的數值。 1234567891011121314151617181920212223class Solution(object): def findSecondMinimumValue(self, root): if not root or not root.left: return -1 self.smallest = root.val self.second = max(root.left.val, root.right.val) self.find(root) if self.smallest == self.second: return -1 return self.second def find(self, root): if root.left and root.right: if root.left.val &lt; self.smallest: self.smallest = root.left.val if root.right.val &lt; self.smallest: self.smallest = root.right.val if root.left.val &gt; self.smallest and root.left.val &lt;= self.second: self.second = root.left.val if root.right.val &gt; self.smallest and root.right.val &lt;= self.second: self.second = root.right.val self.find(root.left) self.find(root.right) ¶第2題 669. Trim a Binary Search Tree ¶問題 給一個 binary search tree，以及正整數 L 與 R (L &lt;= R)，修剪這棵樹使得所有節點的值介於 [L, R] 。 ¶輸入 tree 的 root 以及 L 與 R 的值 範例： 3 / \ 0 4 \ 2 / 1 L = 1 R = 2 ¶輸出 新的 tree 的 root 3 / 2 / 1 ¶方法 利用二元搜尋樹的特性，左子樹的值一定小於等於 root 的值，右子樹的值一定大於等於 root 的值。因此遍歷每一個節點，若 root 的值小於 L ，捨棄左子樹；若 root 的值大於 R ，捨棄右子樹。 1234567891011class Solution(object): def trimBST(self, root, L, R): if not root: return None if L &gt; root.val: return self.trimBST(root.right, L, R) elif R &lt; root.val: return self.trimBST(root.left, L, R) root.left = self.trimBST(root.left, L, R) root.right = self.trimBST(root.right, L, R) return root ¶第3題 670. Maximum Swap ¶問題 給一個非負整數，問交換任兩個digit的位置所能得到的最大值為何？ 給的值範圍介於 [0, 108] 之間。 ¶輸入 2736 ¶輸出 7236 ¶方法 範圍介於 [0, 108] ，代表最多有 8 個digit，任選兩個的可能性只有 28 種，直接用暴力法窮舉所有的可能性。 12345678910class Solution(object): def maximumSwap(self, num): maximum = num n = list(str(num)) for i in range(len(n) - 1): for j in range(i + 1, len(n)): new_num = int("".join(n[0:i] + list(n[j]) + n[i+1:j] + list(n[i]) + n[j+1:])) if new_num &gt; maximum: maximum = new_num return maximum ¶第4題 672. Bulb Switcher II ¶問題 給兩個正整數n與m，分別代表燈泡的數量以及操作開關的次數，n 個燈泡依序標記為 1, 2, 3,…, n ，一開始所有的燈泡都是打開的。 有以下四種操作開關的方法： 按下所有開關 按下編號為偶數的開關 按下編號為奇數的開關 按下編號為(3k+1)的開關, k = 0, 1, 2, … 問 n 個燈泡在操作 m 次之後，這些開關的狀態有哪些的可能性。 ¶輸入 n = 3, m = 1. ¶輸出 4 ¶方法 3 個燈泡在操作 1 次後可能的狀態為： [off, on, off], [on, off, on], [off, off, off], [off, on, on]. 一開始想說用遞迴來解，枚舉所有的可能，但是居然 TLE ，太可惡了！ 1234567891011121314151617class Solution(object): def flipLights(self, n, m): self.lights = [True for i in range(n)] self.times = m self.status = [] self.flipping(self.lights, 0) return len(self.status) def flipping(self, light, time): if time == self.times: if light not in self.status: self.status.append(light) return self.flipping([not v for v in light], time + 1) self.flipping([not v if (i + 1) % 2 == 0 else v for i, v in enumerate(light)], time + 1) self.flipping([not v if (i + 1) % 2 == 1 else v for i, v in enumerate(light)], time + 1) self.flipping([not v if (i + 1) % 3 == 1 else v for i, v in enumerate(light)], time + 1) 果然 O(2n) 還是行不通嗎，但是請仔細觀察可能的狀態，就會發現最多的可能只有八種。 n = 1, m = 0 [on] n = 1, m &gt;= 1 [on], [off] -------------- n = 2, m = 0 [on, on] n = 2, m = 1 [off, off], [on, off], [off, on] n = 2, m &gt;= 2 [on, on], [off, off], [on, off], [off, on] ------------------------------------------- n = 3, m = 0 [on, on, on] n = 3, m = 1 [off, off, off], [on, off, on], [off, on, off], [off, on, on] n = 3, m = 2 [on, on, on], [off, on, off], [on, off, on], [on, off, off], [off, off, off], [off, off, on], [on, on, off] n = 3, m &gt;= 3 [on, on, on], [off, on, off], [on, off, on], [on, off, off], [off, off, off], [off, off, on], [on, on, off], [off, on, on] --------------------------------------------------------------------------------------------------------------------------- n &gt;= 4 的情況都跟 n = 3 相同 因此根據n與m的值可以歸納出以下的結果： 1234567891011class Solution(object): def flipLights(self, n, m): if n == 0 or m == 0: return 1 if n == 1: return 2 if n == 2: return 3 if m == 1 else 4 if m == 1: return 4 return 7 if m == 2 else 8]]></content>
      <categories>
        <category>程式解題</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>Python</tag>
        <tag>LeetCode Weekly Contest</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[連結串列(Linked List)]]></title>
    <url>%2F2017%2F09%2F02%2F%E9%80%A3%E7%B5%90%E4%B8%B2%E5%88%97-Linked-List%2F</url>
    <content type="text"><![CDATA[連結串列一般指的是單向連結串列(Single Linked List)，由node所組成，每個node都具有兩種屬性，分別是「資料」以及「指標」。資料是儲存目前這個節點的值，指標是指向下一個節點的連結，至於最後一個結點則會指向null。 連結串列比起一般的陣列，優點是能夠隨著需求動態配置記憶體，插入或移除元素時也相對方便；缺點是取出元素時無法直接指定位置，需要遍歷整個串列。 以下用python來實作Linked List。 首先我們要實作node，宣告一個class名稱ListNode，每個node都需要兩個元素，資料value與指向下一個node的指標next。 1234class ListNode(object): def __init__(self, value=None, next=None): self.val = value self.next = next 我們還要宣告一個名為LinkedList的類別，用來紀錄串列的起始位置。 123class LinkedList(object): def __init__(self, head=None): self.head = head 接著在LinkedList實作下面的方法： print_nodes() at(index) append(value) insert(index, value) removePos(index) remove(value, all=False) indexOf(value) clear() isEmpty() size() 首先是print_nodes方法，走訪所有的節點並印出每個結點的資料。 12345678def print_nodes(self): if not self.head: print(self.head) node = self.head while node: end = " -&gt; " if node.next else "\n" print(node.val, end=end) node = node.next 接著是at方法，回傳index位置的value。 12345678def at(self, index): count = 0 node = self.head while node: if count == index: return node.val count += 1 node = node.next 第三個是append，將值為value的節點接在陣列的最尾端。 12345678def append(self, value): if not self.head: self.head = ListNode(value) return node = self.head while node.next: node = node.next node.next = ListNode(value) 第四個是insert，將value插入index的位置。 123456789101112131415161718def insert(self, index, value): if index &gt;= self.size(): self.append(value) return count = 0 node = self.head previous = None while node: if count == index: if previous: new_node = ListNode(value, previous.next) previous.next = new_node else: self.head = ListNode(value, node) return count += 1 previous = node node = node.next 第五個與第六個是removePos以及remove，分別是移除位置為index的節點，以及移除值為value的節點。 12345678910111213141516def removePos(self, index): count = 0 node = self.head previous = None while node: if count == index: if previous: previous.next = node.next node = node.next else: self.head = node.next node = self.head return else: previous = node node = node.next 12345678910111213141516def remove(self, val, all=False): node = self.head previous = None while node: if node.val == val: if previous: previous.next = node.next node = node.next else: self.head = node.next node = self.head if not all: return else: previous = node node = node.next 第七個是indexOf，回傳第一個出現的value在串列中的的位置。 12345678def indexOf(self, value): node = self.head count = 0 while node: if node.val == value: return count count += 1 node = node.next 最後三個是clear, isEmpty, 以及size。 12def clear(self): self.head = None 12def isEmpty(self): return self.head is None 1234567def size(self): count = 0 node = self.head while node: count += 1 node = node.next return count 程式碼連結]]></content>
      <categories>
        <category>學校課程</category>
        <category>資料結構</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Linked List</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode#78 Subsets]]></title>
    <url>%2F2017%2F08%2F31%2FLeetCode-78-Subsets%2F</url>
    <content type="text"><![CDATA[¶問題 給一串元素相異的數列，求由此數列之元素所組成的所有子集合。 ¶輸入 [1,2,3] ¶輸出 [ [3], [1], [2], [1,2,3], [1,3], [2,3], [1,2], [] ] ¶方法 每個元素可以出現或是不出現，每一種組合方法都是一個子集合，所以時間複雜度是O(2n) 。 利用Backtracking遞迴枚舉所有可能。 1234567891011121314class Solution(object): def subsets(self, nums): self.length = len(nums) self.ans = [] self.backtrack(nums, []) return(self.ans) def backtrack(self, nums, n): if len(n) == self.length: a = [nums[i] for i, v in enumerate(n) if v] self.ans.append(a) else: self.backtrack(nums, n + [True]) self.backtrack(nums, n + [False]) 其他解法以及類似問題請看這裡。]]></content>
      <categories>
        <category>程式解題</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>Python</tag>
        <tag>Backtracking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode#442 Find All Duplicates in an Array]]></title>
    <url>%2F2017%2F08%2F30%2FLeetCode-442-Find-All-Duplicates-in-an-Array%2F</url>
    <content type="text"><![CDATA[¶問題 給一個長度為 n 的整數陣列，裡面的元素由 1~n 所組成，每個數字出現 0~2 次，找出所有出現 2 次的數字。(不使用額外空間且時間複雜度必須為O(n)) ¶輸入 [4, 3, 2, 7, 8, 2, 3, 1] ¶輸出 [2, 3] ¶方法 負號標記法 將陣列nums的值遍歷一次，將每個出現過的值當作index，將對應到的num[index]的值乘上-1，若對應到的值已經為負數的話，代表已經出現過 1 次，目前是第 2 次，這樣就能找出所有出現過 2 次的元素了。 123456789class Solution(object): def findDuplicates(self, nums): ans = [] for i in nums: if nums[abs(i) - 1] &lt; 0: ans.append(abs(i)) else: nums[abs(i) - 1] *= -1 return ans #448 Find All Numbers Disappeared in an Array，也可以用同樣的方法來解。]]></content>
      <categories>
        <category>程式解題</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>Python</tag>
        <tag>Array</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[挑戰LeetCode Weekly Contest 47]]></title>
    <url>%2F2017%2F08%2F28%2F%E6%8C%91%E6%88%B0LeetCode-Weekly-Contest-47%2F</url>
    <content type="text"><![CDATA[¶前言 這次心血來潮報名了 LeetCode 每周都會舉辦的比賽，規則是在1.5小時之內要解出4題，這對我來說是一大挑戰啊！因為以往參加的比賽，時間都是3小時左右，這麼短的時間內要解出4題真的不容易，不過既然都報名了就全力以赴吧！ ¶第1題 665. Non-decreasing Array ¶問題 給定一個長度為 n 的整數陣列，問是否能在最多更改 1 個數的情況下，將陣列變成 non-decreasing array，也就是遞增的陣列。 ¶輸入 [4, 2, 3] ¶輸出 True ¶方法 我的策略是分別從頭尾出發走一次，若遇到下一個數 pj 小於/大於目前這個數 pi 的話，就將下一個數的值改為目前這個數，使得目前為止的元素為遞增的狀態，此時修改的次數就會增加一次。若分別從頭尾走，修改的次數都大於一次的話，代表無法符合題目的條件，回傳False，反之回傳True。 那為什麼要分別從頭尾走一次呢？ 以範例的輸入當例子，若只從正方向走， (4, 2) 不符合遞增，將 2 改成 4，陣列變成 [4, 4, 3]，接著遇到 (4, 3) 又不符合遞增，因此只修改一個值並不能使原本的陣列變成遞增數列。 但是錯啦！其實只要把第一個數 4 改成 1 或 2 就可以了，想到從反方向走的話好像可以達到這個效果，所以必須要檢查兩個方向才能確保沒有漏網之魚。 1234567891011121314151617181920class Solution: def checkPossibility(self, nums): n = list(nums) edit_time_1 = 0 for i in range(len(nums) - 1): if nums[i] &gt; nums[i + 1]: nums[i + 1] = nums[i] edit_time_1 += 1 nums = n edit_time_2 = 0 for i in range(len(nums) - 1, 0, -1): if nums[i] &lt; nums[i - 1]: nums[i - 1] = nums[i] edit_time_2 += 1 if edit_time_1 &lt;= 1 or edit_time_2 &lt;= 1: return True else: return False ¶第2題 666. Path Sum IV ¶問題 輸入為一個 Depth &lt; 5 的 tree，每個 node 可用三位數來表示。 百位數D代表這個 node 所在的深度，1 &lt;= D &lt;= 4 。 十位數P代表這個 node 在這個深度的第幾個位置, 1 &lt;= P &lt;= 8 。 個位數V代表這個 node 的值, 0 &lt;= V &lt;= 9 。 找出所有從 root 到 leaf 的路徑和，也就是把路徑上的值相加。 ¶輸入 [113, 215, 221] ¶輸出 12 ¶方法 以下是範例輸入的解釋： The tree that the list represents is: 3 / \ 5 1 The path sum is (3 + 5) + (3 + 1) = 12. 先將輸入的點分別對應 1 ~ 15 的值(也就是用陣列表示 tree, 但這裡直接用dictionary比較方便)，接著找出所有的 leaf ，再將每個 leaf 到 root 的路徑上所經過的點全部加起來即可。 1234567891011class Solution: def pathSum(self, nums): order = ['00', '11', '21', '22', '31', '32', '33', '34', '41', '42', '43', '44', '45', '46', '47', '48'] tree = dict([(order.index(str(n)[:2]), n % 10) for n in nums]) summation = 0 leaves = [n for n in tree.keys() if not (2 * n in tree or 2 * n + 1 in tree)] for leaf in leaves: while leaf &gt;= 1: summation += tree[leaf] leaf = int(leaf / 2) return summation ¶第3題 667. Beautiful Arrangement II ¶問題 輸入整數n與k，找到一個由 1 ~ n 組成的陣列[a1, a2, …, an]，陣列元素不得重複， 使得 [|a1-a2|, |a2-a3|, …, |an-1-an|] 剛好由 k 種不同的數組成。 ¶輸入 n = 3, k = 1 ¶輸出 [1, 2, 3] ¶方法 這題推導了很久，想到了一個方法，[1, 2, …, n] 的差有 k 種值，1 &lt;= k &lt;= n - 1，只要建立一個規則產生所有的差，接著再想辦法湊出這個數列，舉例： output: 5 1 4 3 2 diff: 4 3 1 1 假設要使得[1, 2, 3, 4, 5]鄰近的差只有三種，那就先將兩種差設為 n-1 與 n-2，也就是 4 跟 3，剩下的差都填 1 代表第三種差。至於要產生的陣列，先填入最大的數 5，接著根據每個差依序湊出陣列接下來的值，最後就會得到滿足條件的陣列了。 1234567891011121314class Solution: def constructArray(self, n, k): diff = [] for i in range(k - 1): diff.append(n - i - 1) for i in range(n - k): diff.append(1) nums = [n] for i in range(n - 1): new_value = abs(nums[i] + diff[i]) if new_value &gt; n or new_value in nums: new_value = abs(nums[i] - diff[i]) nums.append(new_value) return nums ¶第4題 668. Kth Smallest Number in Multiplication Table ¶問題 找出在一個 m * n 的乘法表中，第 k 小的數字。 ¶輸入 m = 3, n = 3, k = 5 ¶輸出 3 ¶方法 以 3 * 3 的乘法表舉例： The Multiplication Table: 1 2 3 2 4 6 3 6 9 The 5-th smallest number is 3 (1, 2, 2, 3, 3). 這題我試了很多方法，不管怎麼樣總是不夠周延，最後看了別人的解法，使用 binary search ，實在太厲害了完全想不到。 看懂了之後寫了個 python 的版本，方法簡單來說就是利用二分搜尋法，找出中間值 mid ，接著計算每一列小於等於 mid 的個數的和，若總數小於 k ，代表值太小了還要再多一點；若總數大於 k ，代表值太大了要小一點，迴圈重複直到 low &lt; high 的時候代表找到了。 1234567891011121314151617class Solution: def findKthNumber(self, m, n, k): low, high = 1, m * n + 1 while low &lt; high: mid = (low + high) // 2 c = self.count_less_than_middle(mid, m, n) if c &gt;= k: high = mid else: low = mid + 1 return high def count_less_than_middle(self, middle, m, n): num = 0 for i in range(1, m + 1): num += min(middle // i, n) return num ¶結果 最後在時間內只有解出一題，排名只有 1157 / 2554，剩下的都是比賽結束後才想出來QQ，太慘烈了。看來在下做的題目還不夠多，看到題目沒辦法馬上有 sense 要用什麼方式解，只好先來閉關修煉一下了QQ 。]]></content>
      <categories>
        <category>程式解題</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>Python</tag>
        <tag>LeetCode Weekly Contest</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode#566 Reshape the Matrix]]></title>
    <url>%2F2017%2F08%2F27%2FLeetCode-566-Reshape-the-Matrix%2F</url>
    <content type="text"><![CDATA[¶問題 給定一個二維的矩陣以及正整數 r 與 c ，將此矩陣轉為 r * c 的矩陣。 若無法轉換，則輸出原本的矩陣。 ¶輸入 nums = [[1,2], [3,4]] r = 1, c = 4 ¶輸出 [[1,2,3,4]] ¶方法 檢查兩個矩陣的行數與列數相乘的結果是否相同，若成立代表這兩個矩陣的元素個數相同，也就是兩個矩陣能夠互相轉換。 接著將原本 2D 的矩陣轉成 1D ，這樣在轉換成另一個大小的矩陣時會比較方便，只要依序取出即可。 1234567891011121314class Solution(object): def matrixReshape(self, nums, r, c): row = len(nums) col = len(nums[0]) if row * col == r * c: one_d_array = [nums[i][j] for i in range(row) for j in range(col)] ans = [[0 for j in range(c)] for i in range(r)] count_element = 0 for i in range(r): for j in range(c): ans[i][j] = one_d_array[count_element] count_element += 1 return ans return nums]]></content>
      <categories>
        <category>程式解題</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>Python</tag>
        <tag>Matrix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode#561 Array Partition I]]></title>
    <url>%2F2017%2F08%2F26%2FLeetCode-561-Array-Partition-I%2F</url>
    <content type="text"><![CDATA[¶問題 給一個長度為 2n 的整數陣列，將陣列的值兩兩配對，使得所有 min(ai, bi) 的和愈大愈好 (0 &lt;= i &lt;= n) 。 ¶輸入 [1,4,3,2] ¶輸出 4 ¶方法 這題直覺就是先將陣列排序之後再取奇數項相加就好。 sort: [1,2,3,4] min(1, 2) + min(3, 4) = 4 1234567class Solution(object): def arrayPairSum(self, nums): summation = 0 for i, v in enumerate(sorted(nums)): if i % 2 == 0: summation += v return summation 在底下討論版找到了一行解決的 code ，完全體現 python 簡約的風格，太神啦! 123class Solution(object): def arrayPairSum(self, nums): return sum(sorted(nums)[::2])]]></content>
      <categories>
        <category>程式解題</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>Python</tag>
        <tag>Sorting</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode#461 Hamming Distance]]></title>
    <url>%2F2017%2F08%2F26%2FLeetCode-461-Hamming-Distance%2F</url>
    <content type="text"><![CDATA[¶問題 給兩個整數x與y，計算兩者的 Hamming Distance 。 ¶輸入 x = 1, y = 4 ¶輸出 2 ¶方法 所謂的 Hamming Distance 就是兩個string中有幾個相異的位元個數，舉個例子： 1 = (0 0 0 1) 4 = (0 1 0 0) ↑ ↑ 1 跟 4 有兩個相異的位元個數，因此 1 跟 4 的 Hamming Distance 為 2 。 我們可以很直覺的想到可以用 exclusive or 簡單的計算出來，也就是兩個 bit 相同時輸出 0 ，兩個 bit 相異時輸出 1 ，最後再計算總共有幾位數為 1 即可。 123class Solution(object): def hammingDistance(self, x, y): return bin(x^y).count("1")]]></content>
      <categories>
        <category>程式解題</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>Python</tag>
        <tag>Hamming Distance</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode#1 Two Sums]]></title>
    <url>%2F2017%2F08%2F26%2FLeetCode-1-Two-Sums%2F</url>
    <content type="text"><![CDATA[¶問題 給定一個整數的陣列nums，其中有兩個數的和會等於目標target，回傳他們的位置，且同個位置不能重複選取。 ¶輸入 nums = [2, 7, 11, 15] target = 9 ¶輸出 [0, 1] ¶方法 Brute Force Hash Table 第一種方法就是直接使用暴力法，把陣列裡面的元素都加加看，但是時間複雜度會到O(n2)。 123456class Solution(object): def twoSum(self, nums, target): for i in range(len(nums)): for j in range(i + 1, len(nums)): if nums[i] + nums[j] == target: return [i, j] 居然吃了一發TLE，沒想到第一題就玩真的，只好另尋他法了。 這時想到python中相當好用的dictionary，第二種方法就是利用雜湊表將讀到的 index 及 value 紀錄下來，即可在讀入一個新的數之後查看這個數的 complement 是否在 table 當中，這樣就可以完美找到一組解啦。 因為使用了dictionary，所以查表的時間只要O(1)，最後時間複雜度降到了O(n)，當然也就順利通過了！ 12345678class Solution(object): def twoSum(self, nums, target): dic = &#123;&#125; for i, n in enumerate(nums): if target - n in dic: return [dic[target - n], i] else: dic[n] = i]]></content>
      <categories>
        <category>程式解題</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>Hash</tag>
        <tag>Python</tag>
      </tags>
  </entry>
</search>
