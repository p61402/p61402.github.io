<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[CS231n assignment 2]]></title>
    <url>%2F2019%2F05%2F04%2FCS231n-assignment-2%2F</url>
    <content type="text"><![CDATA[簡介 這次的作業相較於第一次作業又更深入了一些，這次作業要依序實作 Fully Connected Network、Batch Normalization、Dropout、Convolutional Neural Network 等方法，並將裡頭的步驟模組化。 Fully Connected Network Modular network 首先要將 FCN 的 forward pass 以及 backward 模組化，包括 affine layer 與 ReLU activation。 在 forward pass 時使用cache將所需變數儲存，以便 backward pass 的時候使用。 12345def affine_forward(x, w, b): out = None out = x.reshape(x.shape[0], -1).dot(w) + b cache = (x, w, b) return out, cache 123456789def affine_backward(dout, cache): x, w, b = cache dx, dw, db = None, None, None dx = dout.dot(w.T).reshape(x.shape) dw = x.reshape(x.shape[0], -1).T.dot(dout) db = np.sum(dout, axis=0) return dx, dw, db 12345def relu_forward(x): out = None out = np.maximum(x, 0) cache = x return out, cache 1234def relu_backward(dout, cache): dx, x = None, cache dx = dout * (x &gt; 0) return dx 利用剛剛完成的模組來建構TwoLayerNet以及可以自訂 size 的FullyConnectedNet。 1234567891011121314151617181920212223242526272829303132333435class TwoLayerNet(object): def __init__(self, input_dim=3*32*32, hidden_dim=100, num_classes=10, weight_scale=1e-3, reg=0.0): self.params = &#123;&#125; self.reg = reg self.params['W1'] = np.random.normal(0, weight_scale, (input_dim, hidden_dim)) self.params['b1'] = np.zeros((hidden_dim, )) self.params['W2'] = np.random.normal(0, weight_scale, (hidden_dim, num_classes)) self.params['b2'] = np.zeros((num_classes, )) def loss(self, X, y=None): scores = None h1, cache_h1 = affine_relu_forward(X, self.params['W1'], self.params['b1']) scores, cache_scores = affine_forward(h1, self.params['W2'], self.params['b2']) # If y is None then we are in test mode so just return scores if y is None: return scores loss, grads = 0, &#123;&#125; loss, dS = softmax_loss(scores, y) dh1, grads['W2'], grads['b2'] = affine_backward(dS, cache_scores) dx, grads['W1'], grads['b1'] = affine_relu_backward(dh1, cache_h1) loss += 0.5 * self.reg * (np.sum(self.params['W2'] ** 2) + np.sum(self.params['W1'] ** 2)) grads['W1'] += self.reg * self.params['W1'] grads['W2'] += self.reg * self.params['W2'] return loss, grads 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596class FullyConnectedNet(object): def __init__(self, hidden_dims, input_dim=3*32*32, num_classes=10, dropout=1, normalization=None, reg=0.0, weight_scale=1e-2, dtype=np.float32, seed=None): self.normalization = normalization self.use_dropout = dropout != 1 self.reg = reg self.num_layers = 1 + len(hidden_dims) self.dtype = dtype self.params = &#123;&#125; self.params['W1'] = np.random.normal(0, weight_scale, (input_dim, hidden_dims[0])) self.params['b1'] = np.zeros((hidden_dims[0], )) for i in range(1, self.num_layers - 1): self.params['W'+str(i+1)] = np.random.normal(0, weight_scale, (hidden_dims[i-1], hidden_dims[i])) self.params['b'+str(i+1)] = np.zeros((hidden_dims[i], )) self.params['W'+str(self.num_layers)] = np.random.normal(0, weight_scale, (hidden_dims[-1], num_classes)) self.params['b'+str(self.num_layers)] = np.zeros((num_classes, )) # When using dropout we need to pass a dropout_param dictionary to each # dropout layer so that the layer knows the dropout probability and the mode # (train / test). You can pass the same dropout_param to each dropout layer. self.dropout_param = &#123;&#125; if self.use_dropout: self.dropout_param = &#123;'mode': 'train', 'p': dropout&#125; if seed is not None: self.dropout_param['seed'] = seed # With batch normalization we need to keep track of running means and # variances, so we need to pass a special bn_param object to each batch # normalization layer. You should pass self.bn_params[0] to the forward pass # of the first batch normalization layer, self.bn_params[1] to the forward # pass of the second batch normalization layer, etc. self.bn_params = [] if self.normalization=='batchnorm': self.bn_params = [&#123;'mode': 'train'&#125; for i in range(self.num_layers - 1)] if self.normalization=='layernorm': self.bn_params = [&#123;&#125; for i in range(self.num_layers - 1)] # Cast all parameters to the correct datatype for k, v in self.params.items(): self.params[k] = v.astype(dtype) def loss(self, X, y=None): X = X.astype(self.dtype) mode = 'test' if y is None else 'train' # Set train/test mode for batchnorm params and dropout param since they # behave differently during training and testing. if self.use_dropout: self.dropout_param['mode'] = mode if self.normalization=='batchnorm': for bn_param in self.bn_params: bn_param['mode'] = mode scores = None h, cache = [None] * (self.num_layers + 1), [None] * (self.num_layers + 1) h[0] = X for i in range(self.num_layers - 1): W, b = self.params['W'+str(i+1)], self.params['b'+str(i+1)] h[i+1], cache[i+1] = affine_relu_forward(h[i], W, b) else: i += 1 W, b = self.params['W'+str(i+1)], self.params['b'+str(i+1)] h[i+1], cache[i+1] = affine_forward(h[i], W, b) scores = h[-1] # If test mode return early if mode == 'test': return scores loss, grads = 0.0, &#123;&#125; loss, dS = softmax_loss(scores, y) dh = [None] * (self.num_layers + 1) dh[-1] = dS i = self.num_layers dh[i-1], grads['W'+str(i)], grads['b'+str(i)] = affine_backward(dh[i], cache[i]) loss += 0.5 * self.reg * np.sum(self.params['W'+str(i)] ** 2) grads['W'+str(i)] += 0.5 * self.reg * (self.params['W'+str(i)] ** 2) i -= 1 while i &gt; 0: dh[i-1], grads['W'+str(i)], grads['b'+str(i)] = affine_relu_backward(dh[i], cache[i]) loss += 0.5 * self.reg * np.sum(self.params['W'+str(i)] ** 2) grads['W'+str(i)] += self.reg * self.params['W'+str(i)] i -= 1 return loss, grads SGD + Momentum 原始的 Stochastic Gradient Descent： \(x_{t+1}=x_t-\alpha\nabla f(x_t)\) SGD + Momumtum： \(v_{t+1}=\rho v_t-\alpha\nabla f(x_t) \\ x_{t+1}=x_t+v_{t+1}\) \(v\) 代表目前的方向速度，初始值為 0，如果負梯度與目前方向相同，則速度會越來越快，參數的更新幅度就會變大；反之則越來越慢，參數的更新幅度會變小。 至於 \(\rho\) 則是一個 hyperparameter，通常設在 0.9 左右。 使用 SGD + Momentum 通常比 Vanilla SGD 能夠更快收斂。 1234567891011121314def sgd_momentum(w, dw, config=None): if config is None: config = &#123;&#125; config.setdefault('learning_rate', 1e-2) config.setdefault('momentum', 0.9) v = config.get('velocity', np.zeros_like(w)) next_w = None v = config['momentum'] * v - config['learning_rate'] * dw next_w = w + v config['velocity'] = v return next_w, config RMSProp \(v_t=\rho v_{t-1}+(1-\rho) \times (\nabla f(x_t))^2\) \(\Delta x_t=-\dfrac{\alpha}{\sqrt{v_t+\epsilon}} \times \nabla f(x_t)\) \(x_{t+1}=x_t+\Delta x_t\) \(\rho\) 為 decay rate，通常設為 0.9、0.99、0.999。 \(\epsilon\) 是一個很小的值，為了避免除以 0 的情況產生。 12345678910111213def rmsprop(w, dw, config=None): if config is None: config = &#123;&#125; config.setdefault('learning_rate', 1e-2) config.setdefault('decay_rate', 0.99) config.setdefault('epsilon', 1e-8) config.setdefault('cache', np.zeros_like(w)) next_w = None config['cache'] = config['decay_rate'] * config['cache'] + (1 - config['decay_rate']) * (dw ** 2) next_w = w + -config['learning_rate'] * dw / np.sqrt(config['cache'] + config['epsilon']) return next_w, config Adam 1234567891011121314151617181920def adam(w, dw, config=None): if config is None: config = &#123;&#125; config.setdefault('learning_rate', 1e-3) config.setdefault('beta1', 0.9) config.setdefault('beta2', 0.999) config.setdefault('epsilon', 1e-8) config.setdefault('m', np.zeros_like(w)) config.setdefault('v', np.zeros_like(w)) config.setdefault('t', 0) next_w = None config['t'] += 1 config['m'] = config['beta1'] * config['m'] + (1 - config['beta1']) * dw mt = config['m'] / (1 - config['beta1'] ** config['t']) config['v'] = config['beta2'] * config['v'] + (1 - config['beta2']) * (dw ** 2) vt = config['v'] / (1 - config['beta2'] ** config['t']) next_w = w + -config['learning_rate'] * mt / (np.sqrt(vt) + config['epsilon']) return next_w, config 比較不同 optimizer 的表現： Optimizer Comparison Batch Normalization 實作過程參考以下論文： Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift batch normalization 的目的是為了讓每一層的輸出都保持高斯分佈，主要的目的是為了避免 gradient vanishing。做法是將 fordward pass 時用來訓練的批次資料計算 mean 以及 variance，利用 mini-batch 的 mean 及 vairance 來更新整體的 mean 及 variance。 Forward Pass 論文中具體的實作方法如下： batch_normalization 12345678910111213141516171819202122232425262728293031323334def batchnorm_forward(x, gamma, beta, bn_param): mode = bn_param['mode'] eps = bn_param.get('eps', 1e-5) momentum = bn_param.get('momentum', 0.9) N, D = x.shape running_mean = bn_param.get('running_mean', np.zeros(D, dtype=x.dtype)) running_var = bn_param.get('running_var', np.zeros(D, dtype=x.dtype)) out, cache = None, None if mode == 'train': sample_mean = x.mean(axis=0) sample_var = x.var(axis=0) sqrtvar = np.sqrt(sample_var + eps) xmu = x - sample_mean ivar = 1./sqrtvar x_hat = xmu * ivar out = gamma * x_hat + beta cache = (xmu, sample_var, ivar, sqrtvar, x_hat, gamma, eps) running_mean = momentum * running_mean + (1 - momentum) * sample_mean running_var = momentum * running_var + (1 - momentum) * sample_var elif mode == 'test': x_hat = (x - running_mean) / np.sqrt(running_var + eps) out = gamma * x_hat + beta else: raise ValueError('Invalid forward batchnorm mode "%s"' % mode) # Store the updated running means back into bn_param bn_param['running_mean'] = running_mean bn_param['running_var'] = running_var return out, cache Backward Pass 論文中對於計算 BN 的反向傳播也有一些描述： BN backward 真的是滿複雜的，最好還是自己畫過一次計算圖之後再試著去計算 backward pass，這部分的話這篇文章寫得滿不錯的，可以參考一下。 自己畫的計算圖 123456789101112131415161718192021222324252627def batchnorm_backward(dout, cache): dx, dgamma, dbeta = None, None, None N, D = dout.shape xmu, var, ivar, sqrtvar, x_hat, gamma, eps = cache dbeta = np.sum(dout, axis=0) dgamma_x = dout dgamma = np.sum(dgamma_x * x_hat, axis=0) dx_hat = dgamma_x * gamma divar = np.sum(dx_hat * xmu, axis=0) dx_mu1 = dx_hat * ivar dsqrtvar = -divar / (sqrtvar ** 2) dvar = 0.5 * dsqrtvar / np.sqrt(var + eps) dsq = dvar * np.ones((N, D)) / N dx_mu2 = 2 * xmu * dsq dx1 = dx_mu1 + dx_mu2 dmu = -np.sum(dx_mu1 + dx_mu2, axis=0) dx2 = dmu * np.ones((N, D)) / N dx = dx1 + dx2 return dx, dgamma, dbeta 簡化版： 12345678910def batchnorm_backward_alt(dout, cache): dx, dgamma, dbeta = None, None, None xmu, var, ivar, sqrtvar, x_hat, gamma, eps = cache N, D = dout.shape dbeta = np.sum(dout, axis=0) dgamma = np.sum(x_hat * dout, axis=0) dx = (gamma * ivar / N) * (N * dout - x_hat * dgamma - dbeta) return dx, dgamma, dbeta Layer Normalization batch normalization 使得類神經網路的訓練更有效率，但是對於複雜的網路結構來說，在 batch size 不夠大的時候效果可能不會太好。因此另一個方法是對 feature 進行 normalize，參考論文：Layer Normalization。 12345678910111213def layernorm_forward(x, gamma, beta, ln_param): out, cache = None, None eps = ln_param.get('eps', 1e-5) x_T = x.T sample_mean = np.mean(x_T, axis=0) sample_var = np.var(x_T, axis=0) x_norm_T = (x_T - sample_mean) / np.sqrt(sample_var + eps) x_norm = x_norm_T.T out = x_norm * gamma + beta cache = (x, x_norm, gamma, sample_mean, sample_var, eps) return out, cache 1234567891011121314151617def layernorm_backward(dout, cache): dx, dgamma, dbeta = None, None, None x, x_norm, gamma, sample_mean, sample_var, eps = cache x_T = x.T dout_T = dout.T N = x_T.shape[0] dbeta = np.sum(dout, axis=0) dgamma = np.sum(x_norm * dout, axis=0) dx_norm = dout_T * gamma[:,np.newaxis] dv = ((x_T - sample_mean) * -0.5 * (sample_var + eps)**-1.5 * dx_norm).sum(axis=0) dm = (dx_norm * -1 * (sample_var + eps)**-0.5).sum(axis=0) + (dv * (x_T - sample_mean) * -2 / N).sum(axis=0) dx_T = dx_norm / (sample_var + eps)**0.5 + dv * 2 * (x_T - sample_mean) / N + dm / N dx = dx_T.T return dx, dgamma, dbeta Dropout Dropout: A Simple Way to Prevent Neural Networks from Overfitting drouput 是一種正規化的方法，在 forward pass 時隨機將某些 neuron 的值丟掉，跟 L1, L2 regularization 一樣，目的都是為了避免 overfitting。 dropout 實作方法是在 training 時根據一個機率 p 來隨機產生一個 mask (值為 True or False)，將 x 乘上 mask 就可以將部分 neuron 的值設為 0， predicting 的時候就直接將 x 乘上 p。 但與其在 predicting 時乘上 p，其實我們可以在 training 的時候就除以 p，這樣就可以減少 predicting 的計算量，因為我們通常比較在意 predicting 時的效率，這個技巧稱為 inverted dropout。 123456789101112131415161718def dropout_forward(x, dropout_param): p, mode = dropout_param['p'], dropout_param['mode'] if 'seed' in dropout_param: np.random.seed(dropout_param['seed']) mask = None out = None if mode == 'train': mask = (np.random.rand(*x.shape) &lt; p) / p out = x * mask elif mode == 'test': out = x cache = (dropout_param, mask) out = out.astype(x.dtype, copy=False) return out, cache 12345678910def dropout_backward(dout, cache): dropout_param, mask = cache mode = dropout_param['mode'] dx = None if mode == 'train': dx = dout * mask elif mode == 'test': dx = dout return dx Convolutional Neural Network Convolution Layer Forward Pass 實作 CNN 的 forward pass，輸入 \(x\) 的大小為 \((N,C,H,W)\)，以及 \(F\) 個 filter，合起來成為一個 \((F,C,HH,WW)\) 的矩陣，經過 convolution 的計算後，輸出一個 \((N,F,H^\prime,W^\prime)\) 的矩陣。 12345678910111213141516171819202122def conv_forward_naive(x, w, b, conv_param): out = None N, C, H, W = x.shape F, C, HH, WW = w.shape pad, stride = conv_param['pad'], conv_param['stride'] x_pad = np.pad(x, ((0,), (0,), (pad,), (pad,)), 'constant', constant_values=0) H_prime = 1 + (H + 2 * pad - HH) // stride W_prime = 1 + (W + 2 * pad - WW) // stride out = np.empty((N, F, H_prime, W_prime)) for f in range(F): for i in range(H_prime): for j in range(W_prime): out[:, f, i, j] = np.sum(x_pad[:, :, i*stride:i*stride+HH, j*stride:j*stride+WW] * w[f], axis=(1,2,3)) out += b.reshape(F, 1, 1) cache = (x, w, b, conv_param) return out, cache Convolution Layer Backward Pass 計算 convolution layer 的 backpropagation 可以參考這篇文章。因為 forward 時的計算也算是 \(x\) 乘上 \(w\)，因此 backward 時計算 \(dx\) 就是用 \(dout\) 與 \(w\) 做計算；計算 \(dw\) 時則是用 \(dout\) 與 \(x\) 做計算，雖然概念上不難理解，但是要透過numpy實作的話對維度要有一定的掌握才行。 12345678910111213141516171819202122232425262728def conv_backward_naive(dout, cache): dx, dw, db = None, None, None x, w, b, conv_param = cache N, C, H, W = x.shape F, C, HH, WW = w.shape pad, stride = conv_param['pad'], conv_param['stride'] x_pad = np.pad(x, ((0,), (0,), (pad,), (pad,)), mode='constant', constant_values=0) H_prime = 1 + (H + 2 * pad - HH) // stride W_prime = 1 + (W + 2 * pad - WW) // stride dx = np.zeros_like(x) dx_pad = np.zeros_like(x_pad) dw = np.zeros_like(w) db = np.sum(dout, axis=(0,2,3)) for i in range(H_prime): for j in range(W_prime): for f in range(F): dw[f] += np.sum(dout[:, f, i, j].reshape(-1,1,1,1) * x_pad[:, :, i*stride:i*stride+HH, j*stride:j*stride+WW], axis=0) for n in range(N): dx_pad[n, :, i*stride:i*stride+HH, j*stride:j*stride+WW] += np.sum(w * dout[n, :, i, j].reshape(-1,1,1,1), axis=0) dx = dx_pad[:, :, pad:-pad, pad:-pad] return dx, dw, db Max Pooling Forward Pass 12345678910111213141516def max_pool_forward_naive(x, pool_param): out = None N, C, H, W = x.shape pool_height, pool_width, stride = pool_param['pool_height'], pool_param['pool_width'], pool_param['stride'] H_prime = 1 + (H - pool_height) // stride W_prime = 1 + (W - pool_width) // stride out = np.empty((N, C, H_prime, W_prime)) for i in range(H_prime): for j in range(W_prime): out[:, :, i, j] = np.max(x[:, :, i*stride:i*stride+pool_height, j*stride:j*stride+pool_width], axis=(2,3)) cache = (x, pool_param) return out, cache Max Pooling Backward Pass 123456789101112131415161718def max_pool_backward_naive(dout, cache): dx = None x, pool_param = cache N, C, H, W = x.shape pool_height, pool_width, stride = pool_param['pool_height'], pool_param['pool_width'], pool_param['stride'] H_prime = 1 + (H - pool_height) // stride W_prime = 1 + (W - pool_width) // stride dx = np.zeros_like(x) for i in range(H_prime): for j in range(W_prime): arg = np.max(x[:, :, i*stride:i*stride+pool_height, j*stride:j*stride+pool_width], axis=(2,3), keepdims=True) == \ x[:, :, i*stride:i*stride+pool_height, j*stride:j*stride+pool_width] dx[:, :, i*stride:i*stride+pool_height, j*stride:j*stride+pool_width] += arg * dout[:, :, i, j][:,:,np.newaxis,np.newaxis] return dx 最後還有實作 Spatial Batch Normalization 以及 Group Normalization，但這部分不是很熟所以略過。]]></content>
      <categories>
        <category>學校課程</category>
        <category>圖像辨識</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Computer Vision</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n assignment 1]]></title>
    <url>%2F2019%2F04%2F20%2FCS231n-assignment-1%2F</url>
    <content type="text"><![CDATA[簡介 這次作業主要實作以下演算法： k-Nearest Neighbor (kNN) Support Vector Machine (SVM) Softmax classifier Two-Layer Neural Network Higher Level Representations: Image Features k-Nearest Neighbor (kNN) 在knn.ipynb中已經將資料載入完成，使用 CIFAR-10 圖片集中的 5000 筆當作訓練，500 筆當作測試。每張圖片的大小都是 (32, 32, 3)，3 代表 RGB 三個通道。 cifar-10 資料集的10種類別 我們要實作三個版本的 kNN，分別是使用雙迴圈、單迴圈、無迴圈的版本，實作的程式碼在cs231n/classifiers/k_nearest_neighbor.py。 compute_distances_two_loops 首先是雙迴圈的版本，X是輸入的 test data，大小為 (num_test, D)，輸出dists為一個大小為 (num_test, num_train) 的 numpy array，元素dists[i,j]代表第 i 個 test data point 與第 j 個 train data point 的歐幾里得距離。 123456789def compute_distances_two_loops(self, X): num_test = X.shape[0] num_train = self.X_train.shape[0] dists = np.zeros((num_test, num_train)) for i in range(num_test): for j in range(num_train): dists[i][j] = np.sqrt(np.sum((X[i] - self.X_train[j]) ** 2)) return dists predict_labels 接著實作函式predict_labels，先找出前 k 個與測試資料最接近的點，再透過 majority vote 的方式選出最有可能的類別。 12345678910111213141516171819def predict_labels(self, dists, k=1): num_test = dists.shape[0] y_pred = np.zeros(num_test) for i in range(num_test): # A list of length k storing the labels of the k nearest neighbors to # the ith test point. closest_y = [] indices = np.argsort(dists[i])[:k] closest_y = self.y_train[indices] y_count = &#123;&#125; for y in closest_y: y_count[y] = y_count.get(y, 0) + 1 max_value = max(y_count.values()) candidates = [y for y, v in y_count.items() if v == max_value] y_pred[i] = min(candidates) return y_pred 下半部其實可以用一行程式碼解決： 1y_pred[i] = np.bincount(closest_y).argmax() 利用剛剛得到的dists，可以計算出 test data 的預測結果，再將預測結果與正確答案比較就可以算出準確率。 k = 1 時的準確率： 1Got 137 / 500 correct =&gt; accuracy: 0.274000 k = 5 時的準確率： 1Got 139 / 500 correct =&gt; accuracy: 0.278000 compute_distances_one_loop 接著實作單迴圈版本的 kNN： 12345678def compute_distances_one_loop(self, X): num_test = X.shape[0] num_train = self.X_train.shape[0] dists = np.zeros((num_test, num_train)) for i in range(num_test): dists[i] = np.sqrt(np.sum((X[i] - self.X_train) ** 2, axis=1)) return dists compute_distances_no_loops 無迴圈的概念就是將兩點間的距離以平方差公式展開：\((x-y)^2=x^2+y^2-2xy\)。 12345678def compute_distances_no_loops(self, X): num_test = X.shape[0] num_train = self.X_train.shape[0] dists = np.zeros((num_test, num_train)) dists += np.sqrt(np.sum(self.X_train ** 2, axis=1) + np.sum(X ** 2, axis=1)[:, np.newaxis] \ - 2 * np.dot(X, self.X_train.T)) return dists 執行time_function可以觀察到無迴圈的版本執行速度，遠遠將其他兩個版本甩在後頭。 123Two loop version took 29.963571 secondsOne loop version took 23.200013 secondsNo loop version took 0.140244 seconds Cross Validation (交叉驗證) 實作交叉驗證來進行 hyperparameter 的搜索，要找的超參數為 k 值。 1234567891011121314151617181920212223num_folds = 5k_choices = [1, 3, 5, 8, 10, 12, 15, 20, 50, 100]X_train_folds = []y_train_folds = []X_train_folds = np.array_split(X_train, num_folds)y_train_folds = np.array_split(y_train, num_folds)k_to_accuracies = &#123;&#125;for k in k_choices: k_to_accuracies[k] = [] for n in range(num_folds): classifier.train(np.concatenate(X_train_folds[:n] + X_train_folds[n+1:]), np.concatenate(y_train_folds[:n] + y_train_folds[n+1:])) pred = classifier.predict(X_train_folds[n], k=k) num_correct = np.sum(pred == y_train_folds[n]) k_to_accuracies[k].append(float(num_correct) / len(pred))# Print out the computed accuraciesfor k in sorted(k_to_accuracies): for accuracy in k_to_accuracies[k]: print('k = %d, accuracy = %f' % (k, accuracy)) 執行此段程式碼會輸出每個 k 值在每個 fold 的表現。 1234567891011121314k = 1, accuracy = 0.263000k = 1, accuracy = 0.257000k = 1, accuracy = 0.264000k = 1, accuracy = 0.278000k = 1, accuracy = 0.266000k = 3, accuracy = 0.239000k = 3, accuracy = 0.249000k = 3, accuracy = 0.240000...k = 100, accuracy = 0.256000k = 100, accuracy = 0.270000k = 100, accuracy = 0.263000k = 100, accuracy = 0.256000k = 100, accuracy = 0.263000 視覺化 將結果視覺化，可以觀察到在 k = 10 的時候會在訓練集得到最好的準確率。 cross validation result 將 k 值設為 10 之後，在測試集的準確率確實有提升一些。 1Got 141 / 500 correct =&gt; accuracy: 0.282000 Support Vector Machine (SVM) svm_loss_naive 首先實作迴圈版本的 svm loss function。 輸入的資料維度是 D，總共有 C 種類別，每個 minibatch 有 N 筆資料。 參數W是大小為 (D,C) 的權重，X是大小為 (N,D) 的 minibatch data，y大小為 (N,1) 代表 training labels。 SVM 的 loss function 如下： \(L_i=\sum_{j\neq y_i}max(0,s_j-s_{y_i}+\Delta)\) Loss function 的 gradient 如下： \(\nabla_{w_{y_i}}L_i=-(\sum_{j\neq y_i} 1(w_j^Tx_i-w_{y_i}^Tx_i+\Delta&gt;0))x_i\) \(\nabla_{w_j}L_i=1(w_j^Tx_i-w_{y_i}^Tx_i+\Delta&gt;0)x_i\) 1234567891011121314151617181920212223242526272829def svm_loss_naive(W, X, y, reg): dW = np.zeros(W.shape) # initialize the gradient as zero # compute the loss and the gradient num_classes = W.shape[1] num_train = X.shape[0] loss = 0.0 for i in range(num_train): scores = X[i].dot(W) correct_class_score = scores[y[i]] for j in range(num_classes): if j == y[i]: continue margin = scores[j] - correct_class_score + 1 # note delta = 1 if margin &gt; 0: loss += margin dW[:, j] += X[i] dW[:, y[i]] -= X[i] # Right now the loss is a sum over all training examples, but we want it # to be an average instead so we divide by num_train. loss /= num_train dW /= num_train # Add regularization to the loss. loss += reg * np.sum(W * W) dW += reg * W return loss, dW svm_loss_vectorized 利用numpy vectorized 的計算方式提升運算速度。 12345678910111213141516171819202122def svm_loss_vectorized(W, X, y, reg): """ Structured SVM loss function, vectorized implementation. Inputs and outputs are the same as svm_loss_naive. """ loss = 0.0 dW = np.zeros(W.shape) # initialize the gradient as zero num_train = X.shape[0] scores = X.dot(W) correct_class_score = scores[np.arange(num_train), y].reshape(-1, 1) margin = scores - correct_class_score + 1 margin[np.arange(num_train), y] = 0 loss += margin[margin &gt; 0].sum() / num_train loss += reg * np.sum(W * W) counts = (margin &gt; 0).astype(int) counts[range(num_train), y] = - np.sum(counts, axis = 1) dW += np.dot(X.T, counts) / num_train + reg * W return loss, dW Stochastic Gradient Descent (SGD) cs331n/linear_classifier.py 隨機梯度下降，每次更新W時只利用一部分的資料來計算 loss 及 gradient，能夠減少運算量。 123456789101112131415161718192021222324252627def train(self, X, y, learning_rate=1e-3, reg=1e-5, num_iters=100, batch_size=200, verbose=False): num_train, dim = X.shape num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes if self.W is None: # lazily initialize W self.W = 0.001 * np.random.randn(dim, num_classes) # Run stochastic gradient descent to optimize W loss_history = [] for it in range(num_iters): X_batch = None y_batch = None idx = np.random.choice(np.arange(num_train), batch_size) X_batch, y_batch = X[idx], y[idx] # evaluate loss and gradient loss, grad = self.loss(X_batch, y_batch, reg) loss_history.append(loss) self.W -= learning_rate * grad if verbose and it % 100 == 0: print('iteration %d / %d: loss %f' % (it, num_iters, loss)) return loss_history 12345def predict(self, X): y_pred = np.zeros(X.shape[0]) y_pred = X.dot(self.W).argmax(axis=1) return y_pred Parameter Tuning 使用 Grid Search 的方法尋找超參數。 123456789101112131415161718192021222324252627282930313233343536learning_rates = [1e-7, 5e-5]regularization_strengths = [2.5e4, 5e4]results = &#123;&#125;best_val = -1 # The highest validation accuracy that we have seen so far.best_svm = None # The LinearSVM object that achieved the highest validation rate.for lr in learning_rates: for reg in regularization_strengths: print("hyperparameter tuning: lr=&#123;&#125;, reg=&#123;&#125;".format(lr, reg)) svm = LinearSVM() tic = time.time() loss_hist = svm.train(X_train, y_train, learning_rate=lr, reg=reg, num_iters=1500, verbose=True) y_train_pred = svm.predict(X_train) train_acc = np.mean(y_train == y_train_pred) y_val_pred = svm.predict(X_val) val_acc = np.mean(y_val == y_val_pred) results[(lr, reg)] = (train_acc, val_acc) if val_acc &gt; best_val: best_val = val_acc best_svm = svm print('-'*40)# Print out results.for lr, reg in sorted(results): train_accuracy, val_accuracy = results[(lr, reg)] print('lr %e reg %e train accuracy: %f val accuracy: %f' % ( lr, reg, train_accuracy, val_accuracy)) print('best validation accuracy achieved during cross-validation: %f' % best_val) 將學習到的權重視覺化 learned_weight Softmax classifier 與 svm 相同都是要實作兩種方法： softmax_loss_naive 模型的W,X,y都與 SVM 相同，唯一不同的點是 softmax classifier 使用的 loss function 不是 hinge loss，而是 cross-entropy loss： \(L_i=-log(\dfrac{e^{f_{y_i}}}{\sum_j e^{f_j}})\) 也可以寫成 \(L_i＝-f_{y_i}+log\sum_j e^{f_j}\) 1234567891011121314151617181920212223def softmax_loss_naive(W, X, y, reg): # Initialize the loss and gradient to zero. loss = 0.0 dW = np.zeros_like(W) num_classes = W.shape[1] num_train = X.shape[0] for i in range(num_train): scores = X[i].dot(W) scores -= np.max(scores) correct_class_score = scores[y[i]] exp_sum = np.sum(np.exp(scores)) loss += -np.log(np.exp(correct_class_score) / exp_sum) for j in range(num_classes): dW[:, j] += (np.exp(scores[j]) / exp_sum - (y[i] == j)) * X[i] loss /= num_train dW /= num_train loss += reg * np.sum(W * W) dW += reg * W return loss, dW softmax_loss_vectorized 12345678910111213141516171819def softmax_loss_vectorized(W, X, y, reg): # Initialize the loss and gradient to zero. loss = 0.0 dW = np.zeros_like(W) num_train = X.shape[0] scores = X.dot(W) scores -= np.max(scores, axis=1, keepdims=True) prob = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True) loss += np.sum(-np.log(prob[np.arange(num_train), y])) ind = np.zeros_like(prob) ind[np.arange(num_train), y] = 1 dW = X.T.dot(prob - ind) loss = loss / num_train + reg * np.sum(W * W) dW = dW / num_train + reg * W return loss, dW 接下來也是實作 Grid Search 搜尋超參數，與 SVM 的部份相同，就不再贅述。 Two-Layer Neural Network Fordward Pass 實作一個 two-layer 的 NN，使用 ReLU nonlinearity。 計算方法如下： \(h_1=ReLU(X\cdot W_1+b_1)\) \(S=h_1\cdot W_2+b_2\) 12345678910111213141516def loss(self, X, y=None, reg=0.0): # Unpack variables from the params dictionary W1, b1 = self.params['W1'], self.params['b1'] W2, b2 = self.params['W2'], self.params['b2'] N, D = X.shape # Compute the forward pass scores = None relu = lambda x: np.maximum(0, x) h1 = relu(X.dot(W1) + b1) scores = h1.dot(W2) + b2 # If the targets are not given then jump out, we're done if y is None: return scores Compute Loss 使用 softmax classifier loss：\(L_i=-log(\dfrac{e^{f_{y_i}}}{\sum_j e^{f_j}})\) 1234567# Compute the lossloss = Nonescores_ = scores - np.max(scores, axis=1, keepdims=True)prob = np.exp(scores_) / np.sum(np.exp(scores_), axis=1, keepdims=True)loss = np.sum(-np.log(prob[np.arange(N), y]))loss = loss / N + 0.5 * reg * (np.sum(W1 * W1) + np.sum(W2 * W2)) Backpropagation 計算 Loss 對每個參數的偏微分：\(\dfrac{\partial L}{\partial W_2},\dfrac{\partial L}{\partial b_2},\dfrac{\partial L}{\partial W_1},\dfrac{\partial L}{\partial b_1}\) 以下進行四個偏微分的推導： \(\mathbf{\dfrac{\partial L}{\partial W_2}}=\dfrac{\partial L}{\partial S}\dfrac{\partial S}{\partial W_2}=dS\cdot h_1^\top\) 而 \(dS\) 為 softmax function 對 score 的偏微分： \(dS=\begin{cases} \dfrac{e^{s_i}}{\sum_j e^{s_j}} - 1, &amp; j=y_i \\ \dfrac{e^{s_i}}{\sum_j e^{s_j}}, &amp; j\neq y_i \end{cases}\) \(\mathbf{\dfrac{\partial L}{\partial b_2}}=\dfrac{\partial L}{\partial S}\dfrac{\partial S}{\partial b_2}=dS\) \(\mathbf{\dfrac{\partial L}{\partial W_1}}=\dfrac{\partial L}{\partial S}\dfrac{\partial S}{\partial h_1}\dfrac{\partial h_1}{\partial W_1}=dS\cdot W_2^\top\cdot \dfrac{\partial h_1}{\partial W_1}\) 而 \(\dfrac{\partial h_1}{\partial W_1}=\begin{cases} 0, &amp; h_1=0 \\ x^\top, &amp; h_1 &gt; 0 \end{cases}\) 因為 \(ReLU(x)=max(0,x),\dfrac{\partial ReLU}{\partial x}=\begin{cases} 0, &amp; x&lt;0 \\ 1, &amp; x&gt;0 \end{cases}\) \(\mathbf{\dfrac{\partial L}{\partial b_1}}=\dfrac{\partial L}{\partial S}\dfrac{\partial S}{\partial h_1}\dfrac{\partial h_1}{\partial b_1}=dS\cdot W_2^\top\cdot\dfrac{\partial h_1}{\partial b_1}\) 123456789101112131415# Backward pass: compute gradientsgrads = &#123;&#125;dS = probdS[np.arange(N), y] -= 1dS /= Ngrads['W2'] = h1.T.dot(dS) + reg * W2grads['b2'] = np.sum(dS, axis=0)dh1 = dS.dot(W2.T) * (h1 &gt; 0)grads['W1'] = X.T.dot(dh1) + reg * W1grads['b1'] = np.sum(dh1, axis=0)return loss, grads Higher Level Representations: Image Features 計算圖片的 Histogram of Oriented Gradients (HOG) 當作 feature，丟進模型做訓練。這部分的程式碼都已經先寫好了，我們只要 Tuning 模型的 Hyperparameter 使準確率到預期的值即可。]]></content>
      <categories>
        <category>學校課程</category>
        <category>圖像辨識</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Computer Vision</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS224n assignment 2]]></title>
    <url>%2F2018%2F12%2F31%2FCS224n-assignment-2%2F</url>
    <content type="text"><![CDATA[CS224n assignment 2 這次的作業主要目的是讓我們實作 Dependency Parsing 以及熟悉 Tensorflow 的運作原理。 1. Tensorflow Softmax In this question, we will implement a linear classifier with loss function \[ J(\mathbf{W}) = CE(\mathbf{y},softmax(\mathbf{xW} + \mathbf{b})) \] Where \(\mathbf{x}\) is a vector of features, \(\mathbf{W}\) is the model’s weight matrix, and \(\mathbf{b}\) is a bias term. We will use TensorFlow’s automatic differentiation capability to fit this model to provided data. (a) 使用 Tensorflow 實作 Softmax Implement the softmax function using TensorFlow in q1_softmax.py. Remember that \[ softmax(x)_i = \dfrac{e^{x_i}}{\sum_j{e^{x_j}}} \] Note that you may not use tf.nn.softmax or related built-in functions. You can run basic (nonexhaustive tests) by running python q1_softmax.py. 1.(a) solution 123def softmax(x): out = tf.exp(x) / tf.reduce_sum(tf.exp(x), axis=1, keepdims=True) return out (b) 使用 Tensorflow 實作 Cross Entropy Loss Implement the cross-entropy loss using TensorFlow in q1_softmax.py. Remember that \[ CE(\mathbf{y},\mathbf{\hat{y}})=-\sum\limits_{i=1}^{N_c} y_i log(\hat{y_i})\] where \(\mathbf{y} \in \mathbb{R}^{N_c}\) is a one-hot label vector and \(Nc\) is the number of classes. This loss is summed over all examples in a minibatch. Note that you may not use TensorFlow’s built-in cross-entropy functions for this question. You can run basic (non-exhaustive tests) by running python q1_softmax.py. 1.(b) solution 123def cross_entropy_loss(y, yhat): out = -tf.reduce_sum(tf.multiply(tf.to_float(y), tf.log(yhat))) return out (c) Placeholder Variable 與 Feed Dictionary Carefully study the Model class in model.py. Briefly explain the purpose of placeholder variables and feed dictionaries in TensorFlow computations. Fill in the implementations for add_placeholders and create_feed_dict in q1_classifier.py. 1.(c) solution Hint: Note that configuration variables are stored in the Config class. You will need to use these configuration variables in the code. 123def add_placeholders(self): self.input_placeholder = tf.placeholder(dtype=tf.float32, shape=(self.config.batch_size, self.config.n_features)) self.labels_placeholder = tf.placeholder(dtype=tf.float32, shape=(self.config.batch_size, self.config.n_classes)) 12345def create_feed_dict(self, inputs_batch, labels_batch=None): feed_dict = &#123;self.input_placeholder: inputs_batch, self.labels_placeholder: labels_batch&#125; return feed_dict (d) 使用 Tensorflow 建立網路架構 Implement the transformation for a softmax classifier in the function add_prediction_op in q1_classifier.py. Add cross-entropy loss in the function add_loss_op in the same file. Use the implementations from the earlier parts of the problem (already imported for you), not TensorFlow built-ins. 1.(d) solution 1234567def add_prediction_op(self): with tf.variable_scope('transformation'): b = tf.Variable(tf.zeros(shape=[self.config.n_classes])) W = tf.Variable(tf.zeros(shape=[self.config.n_features, self.config.n_classes])) z = tf.matmul(self.input_placeholder, W) + b pred = softmax(z) return pred 123def add_loss_op(self, pred): loss = cross_entropy_loss(self.labels_placeholder, pred) return loss (e) 使用 Tensorflow 加入 Optimizer Fill in the implementation for add_training_op in q1_classifier.py. Explain in a few sentences what happens when the model’s train_op is called (what gets computed during forward propagation, what gets computed during backpropagation, and what will have changed after the op has been run?). Verify that your model is able to fit to synthetic data by running python q1_classifier.py and making sure that the tests pass. Hint: Make sure to use the learning rate specified in Config. 1.(e) solution 123def add_training_op(self, loss): train_op = tf.train.GradientDescentOptimizer(learning_rate=self.config.lr).minimize(loss) return train_op 2. Neural Transition-Based Dependency Parsing In this section, you’ll be implementing a neural-network based dependency parser. A dependency parser analyzes the grammatical structure of a sentence, establishing relationships between “head” words and words which modify those heads. Your implementation will be a transition-based parser, which incrementally builds up a parse one step at a time. At every step it maintains a partial parse, which is represented as follows: A stack of words that are currently being processed. A buffer of words yet to be processed. A list of dependencies predicted by the parser. Initially, the stack only contains ROOT, the dependencies lists is empty, and the buffer contains all words of the sentence in order. At each step, the parse applies a transition to the partial parse until its buffer is empty and the stack is size 1. The following transitions can be applied: SHIFT: removes the first word from the buffer and pushes it onto the stack. LEFT-ARC: marks the second (second most recently added) item on the stack as a dependent of the first item and removes the second item from the stack. RIGHT-ARC: marks the first (most recently added) item on the stack as a dependent of the second item and removes the first item from the stack. Your parser will decide among transitions at each state using a neural network classifier. First, you will implement the partial parse representation and transition functions. (a) 試試看 Go through the sequence of transitions needed for parsing the sentence “I parsed this sentence correctly”. The dependency tree for the sentence is shown below. At each step, give the configuration of the stack and buffer, as well as what transition was applied this step and what new dependency was added (if any). The first three steps are provided below as an example. 2.(a) solution stack buffer new dependency transition [ROOT] [I, parsed, this, setence, correctly] Initial Configuration [ROOT, I] [parsed, this, setence, correctly] SHIFT [ROOT, I, parsed] [this, setence, correctly] SHIFT [ROOT, parsed] [this, setence, correctly] parsed -&gt; I LEFT-ARC [ROOT, parsed, this] [setence, correctly] SHIFT [ROOT, parsed, this] [setence, correctly] SHIFT [ROOT, parsed, this, sentence] [correctly] SHIFT [ROOT, parsed, setence] [correctly] sentence -&gt; this LEFT-ARC [ROOT, parsed] [correctly] parse -&gt; sentence RIGHT-ARC [ROOT, parsed, correctly] [] SHFIT [ROOT, parsed] [] parse -&gt; correctly RIGHT-ARC [ROOT] [] ROOT -&gt; parsed RIGHT-ARC (b) 時間複雜度 A sentence containing n words will be parsed in how many steps (in terms of n)? Briefly explain why. 2.(b) solution 每個詞都會進行一次 SHIFT 及 LEFT/RIGHT-ARC，因此共 2n 次。 (c) 實作 Dependency Parsing Implement the __init__ and parse_step functions in the PartialParse class in q2_parser_transitions.py. This implements the transition mechanics your parser will use. You can run basic (not-exhaustive) tests by running python q2_parser_transitions.py. 2.(c) solution 12345678910111213141516def __init__(self, sentence): self.sentence = sentence self.stack = ['ROOT'] self.buffer = sentence[:] self.dependencies = []def parse_step(self, transition): if transition == "S": self.stack.append(self.buffer[0]) self.buffer.pop(0) elif transition == "LA": self.dependencies.append((self.stack[-1], self.stack[-2])) self.stack.pop(-2) else: self.dependencies.append((self.stack[-2], self.stack[-1])) self.stack.pop(-1) (d) Minibatch Dependency Parsing Our network will predict which transition should be applied next to a partial parse. We could use it to parse a single sentence by applying predicted transitions until the parse is complete. However, neural networks run much more efficiently when making predictions about batches of data at a time (i.e., predicting the next transition for a many different partial parses simultaneously). We can parse sentences in minibatches with the following algorithm. Implement this algorithm in the minibatch_parse function in q2_ parser_transitions.py. You can run basic (not-exhaustive) tests by running python q2_parser_transitions.py. Note: You will need minibatch_parse to be correctly implemented to evaluate the model you will build in part (h). However, you do not need it to train the model, so you should be able to complete most of part (h) even if minibatch parse is not implemented yet. 2.(d) solution 123456789101112131415def minibatch_parse(sentences, model, batch_size): partial_parse = [PartialParse(sentence) for sentence in sentences] dependencies = [] while len(partial_parse) &gt; 0: mini_batch = partial_parse[:batch_size] while len(mini_batch) &gt; 0: transitions = model.predict(mini_batch) for i, action in enumerate(transitions): mini_batch[i].parse_step(action) mini_batch = [parse for parse in mini_batch if len(parse.stack) &gt; 1 or len(parse.buffer) &gt; 0] dependencies.extend(p.dependencies for p in partial_parse[:batch_size]) partial_parse = partial_parse[batch_size:] return dependencies (e) Xavier initialization In order to avoid neurons becoming too correlated and ending up in poor local minimina, it is often helpful to randomly initialize parameters. One of the most frequent initializations used is called Xavier initialization. Given a matrix \(A\) of dimension \(m × n\), Xavier initialization selects values \(A_{ij}\) uniformly from \([-\epsilon,\epsilon]\), where \[\epsilon=\dfrac{\sqrt{6}}{\sqrt{m+n}}\] Implement the initialization in xavier weight init in q2 _initialization.py. You can run basic (nonexhaustive tests) by running python q2_initialization.py. This function will be used to initialize \(W\) and \(U\). 2.(e) solution 12345678def xavier_weight_init(): def _xavier_initializer(shape, **kwargs): epsilon = tf.sqrt(6 / tf.cast(np.sum(shape), tf.float32)) out = tf.random_uniform(shape=shape, minval=-epsilon, maxval=epsilon) return out # Returns defined initializer function. return _xavier_initializer (f) Dropout We will regularize our network by applying Dropout. During training this randomly sets units in the hidden layer \(h\) to zero with probability \(p_{drop}\) and then multiplies \(h\) by a constant \(\gamma\) (dropping different units each minibatch). We can write this as \[h_{drop}=\gamma d \circ h\] where \(d ∈ \{0, 1\}^{D_h}\) (\(D_h\) is the size of \(h\)) is a mask vector where each entry is 0 with probability \(p_{drop}\) and 1 with probability (1 − \(p_{drop}\)). \(\gamma\) is chosen such that the value of hdrop in expectation equals \(h\): \[E_{p_{drop}}[h_{drop}]_i=h_i\] for all \(0 &lt; i &lt; D_h\). What must \(\gamma\) equal in terms of \(p_{drop}\)? Briefly justify your answer. 2.(f) solution \(E_p{h_p}_i=E_p[\gamma d_i h_i] = p \times 0 + (1-p) \times \gamma h_i = (1-p)\gamma h_i = h_i\) \(\Rightarrow r = \dfrac{1}{1-p}\) (g) Adam Optmizer We will train our model using the Adam optimizer. Recall that standard SGD uses the update rule \[\theta \leftarrow \theta - \alpha \nabla_\theta J_{minibatch}(\theta)\] where \(\theta\) is a vector containing all of the model parameters, \(J\) is the loss function, \(\nabla_\theta J_{minibatch}(\theta)\) is the gradient of the loss function with respect to the parameters on a minibatch of data, and \(\alpha\) is the learning rate. Adam uses a more sophisticated update rule with two additional steps. First, Adam uses a trick called momentum by keeping track of m, a rolling average of the gradients: \[m \leftarrow \beta_1 m + (1-\beta_1) \nabla_\theta J_{minibatch}(\theta)\] \[\theta \leftarrow \theta - \alpha m\] where \(\beta_1\) is a hyperparameter between 0 and 1 (often set to 0.9). Briefly explain (you don’t need to prove mathematically, just give an intuition) how using \(m\) stops the updates from varying as much. Why might this help with learning? Adam also uses adaptive learning rates by keeping track of v, a rolling average of the magnitudes of the gradients: \[m \leftarrow \beta_1 m + (1-\beta_1) \nabla_\theta J_{minibatch}(\theta)\] \[m \leftarrow \beta_2 v + (1-\beta_2) \nabla_\theta J_{minibatch}(\theta) \circ \nabla_\theta J_{minibatch}(\theta)\] \[\theta \leftarrow \theta - \alpha \circ m / \sqrt{v}\] where \(\circ\) and \(/\) denote elementwise multiplication and division (so \(z \circ z\) is elementwise squaring) and \(\beta_2\) is a hyperparameter between 0 and 1 (often set to 0.99). Since Adam divides the update by \(\sqrt{v}\), which of the model parameters will get larger updates? Why might this help with learning? (h) Parser 模型實作 In q2_parser_model.py implement the neural network classifier governing the dependency parser by filling in the appropriate sections. We will train and evaluate our model on the Penn Treebank (annotated with Universal Dependencies).Run python q2_parser_model.py to train your model and compute predictions on the test data (make sure to turn off debug settings when doing final evaluation). Hints: When debugging, pass the keyword argument debug=True to the main method (it is set to true by default). This will cause the code to run over a small subset of the data, so the training the model won’t take as long. This code should run within 1 hour on a CPU. When running with debug=True, you should be able to get a loss smaller than 0.2 and a UAS larger than 65 on the dev set (although in rare cases your results may be lower, there is some randomness when training). When running with debug=False, you should be able to get a loss smaller than 0.08 on the train set and an Unlabeled Attachment Score larger than 87 on the dev set. For comparison, the model in the original neural dependency parsing paper gets 92.5 UAS. If you want, you can tweak the hyperparameters for your model (hidden layer size, hyperparameters for Adam, number of epochs, etc.) to improve the performance (but you are not required to do so). 2.(h) solution 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144import cPickleimport osimport timeimport tensorflow as tffrom model import Modelfrom q2_initialization import xavier_weight_initfrom utils.parser_utils import minibatches, load_and_preprocess_dataclass Config(object): n_features = 36 n_classes = 3 dropout = 0.5 # (p_drop in the handout) embed_size = 50 hidden_size = 200 batch_size = 1024 n_epochs = 10 lr = 0.0005class ParserModel(Model): def add_placeholders(self): self.input_placeholder = tf.placeholder(dtype=tf.int32, shape=(None, self.config.n_features)) self.labels_placeholder = tf.placeholder(dtype=tf.float32, shape=(None, self.config.n_classes)) self.dropout_placeholder = tf.placeholder(dtype=tf.float32) def create_feed_dict(self, inputs_batch, labels_batch=None, dropout=0): feed_dict = &#123;self.input_placeholder: inputs_batch, self.dropout_placeholder: dropout&#125; if labels_batch is not None: feed_dict[self.labels_placeholder] = labels_batch return feed_dict def add_embedding(self): embedding = tf.Variable(self.pretrained_embeddings) embeddings = tf.reshape(tf.nn.embedding_lookup(embedding, self.input_placeholder), [-1, self.config.n_features * self.config.embed_size]) return embeddings def add_prediction_op(self): x = self.add_embedding() xavier_initializer = xavier_weight_init() W = tf.Variable(xavier_initializer((self.config.n_features * self.config.embed_size, self.config.hidden_size))) b1 = tf.Variable(tf.zeros(self.config.hidden_size)) U = tf.Variable(xavier_initializer((self.config.hidden_size, self.config.n_classes))) b2 = tf.Variable(tf.zeros(self.config.n_classes)) h = tf.nn.relu(tf.matmul(x, W) + b1) h_drop = tf.nn.dropout(h, 1 - self.dropout_placeholder) pred = tf.matmul(h_drop, U) + b2 return pred def add_loss_op(self, pred): loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=self.labels_placeholder)) return loss def add_training_op(self, loss): train_op = tf.train.AdamOptimizer(learning_rate=self.config.lr).minimize(loss) return train_op def train_on_batch(self, sess, inputs_batch, labels_batch): feed = self.create_feed_dict(inputs_batch, labels_batch=labels_batch, dropout=self.config.dropout) _, loss = sess.run([self.train_op, self.loss], feed_dict=feed) return loss def run_epoch(self, sess, parser, train_examples, dev_set): n_minibatches = 1 + len(train_examples) / self.config.batch_size prog = tf.keras.utils.Progbar(target=n_minibatches) for i, (train_x, train_y) in enumerate(minibatches(train_examples, self.config.batch_size)): loss = self.train_on_batch(sess, train_x, train_y) prog.update(i + 1, [("train loss", loss)]) print "Evaluating on dev set", dev_UAS, _ = parser.parse(dev_set) print "- dev UAS: &#123;:.2f&#125;".format(dev_UAS * 100.0) return dev_UAS def fit(self, sess, saver, parser, train_examples, dev_set): best_dev_UAS = 0 for epoch in range(self.config.n_epochs): print "Epoch &#123;:&#125; out of &#123;:&#125;".format(epoch + 1, self.config.n_epochs) dev_UAS = self.run_epoch(sess, parser, train_examples, dev_set) if dev_UAS &gt; best_dev_UAS: best_dev_UAS = dev_UAS if saver: print "New best dev UAS! Saving model in ./data/weights/parser.weights" saver.save(sess, './data/weights/parser.weights') print def __init__(self, config, pretrained_embeddings): self.pretrained_embeddings = pretrained_embeddings self.config = config self.build()def main(debug=True): print 80 * "=" print "INITIALIZING" print 80 * "=" config = Config() parser, embeddings, train_examples, dev_set, test_set = load_and_preprocess_data(debug) if not os.path.exists('./data/weights/'): os.makedirs('./data/weights/') with tf.Graph().as_default() as graph: print "Building model...", start = time.time() model = ParserModel(config, embeddings) parser.model = model init_op = tf.global_variables_initializer() saver = None if debug else tf.train.Saver() print "took &#123;:.2f&#125; seconds\n".format(time.time() - start) graph.finalize() with tf.Session(graph=graph) as session: parser.session = session session.run(init_op) print 80 * "=" print "TRAINING" print 80 * "=" model.fit(session, saver, parser, train_examples, dev_set) if not debug: print 80 * "=" print "TESTING" print 80 * "=" print "Restoring the best model weights found on the dev set" saver.restore(session, './data/weights/parser.weights') print "Final evaluation on test set", UAS, dependencies = parser.parse(test_set) print "- test UAS: &#123;:.2f&#125;".format(UAS * 100.0) print "Writing predictions" with open('q2_test.predicted.pkl', 'w') as f: cPickle.dump(dependencies, f, -1) print "Done!"if __name__ == '__main__': main(False) (i) 加分題 3. Recurrent Neural Networks: Language Modeling]]></content>
      <categories>
        <category>學校課程</category>
        <category>自然語言處理</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS224n assignment 1]]></title>
    <url>%2F2018%2F12%2F05%2FCS224n-assignment-1%2F</url>
    <content type="text"><![CDATA[CS224n assignment 1 這是 Stanford University 的一門開放式課程，叫做 Natural Language Processing with Deep Learning，所指派的第一個作業。這個作業透過以數學推導類神經網路的運算過程，以及實作wor2vec來進行情感分析，為初學者學習自然語言處理打下基礎。 這個作業主要分為 4 個部分： Softmax Neural Network Bascis word2vec Sentiment Analysis 1. Softmax (a) softmax 的性質 Prove that softmax is invariant to constant offsets in the input, that is, for any input vector \(x\) and any constant \(c\), \[softmax(x) = softmax(x + c)\] where \(x + c\) means adding the constant \(c\) to every dimension of \(x\). Remember that \[softmax(x)_i=\dfrac{e^{x_i}}{\sum_j{e^{x_j}}}\] Note: In practice, we make use of this property and choose \(c = − max_i x_i\) when computing softmax probabilities for numerical stability (i.e., subtracting its maximum element from all elements of \(x\)). solution 證明若將 softmax 的輸入 x 的每一項都加上一個常數後，結果會與原本相同。 \(softmax(x+c)_i=\dfrac{e^{x_i+c}}{\sum_j{e^{x_j+c}}}=\dfrac{e^{x_i}e^c}{\sum_j{e^{x_j}e^c}}=\dfrac{e^ce^{x_i}}{e^c\sum_j{e^{x_j}}}=\dfrac{e^{x_i}}{\sum_j{e^{x_j}}}=softmax(x)\) 這個特性在實際計算 softmax 時常被使用，將輸入 x 的每一項都減去 x 中的最大值，可以減少計算量。 (b) 實作 softmax function Given an input matrix of N rows and D columns, compute the softmax prediction for each row using the optimization in part (a). Write your implementation in q1_softmax.py. You may test by executing python q1_softmax.py. Note: The provided tests are not exhaustive. Later parts of the assignment will reference this code so it is important to have a correct implementation. Your implementation should also be efficient and vectorized whenever possible (i.e., use numpy matrix operations rather than for loops). A non-vectorized implementation will not receive full credit! solution 輸入：x -- 一個維度為 N x D 的 numpy 矩陣 輸出：x -- softmax(x)，可以 in-place 修改 x 的值 注意在x的維度為 1 x D 與 N x D (N ≥ 2)時的處理方式不同。 12345678910111213def softmax(x): orig_shape = x.shape if len(x.shape) &gt; 1: # Matrix x = np.apply_along_axis(softmax, 1, x) else: # Vector x -= np.amax(x) x = np.exp(x) / np.sum(np.exp(x)) assert x.shape == orig_shape return x 2. Neural Network Basics (a) 推導 sigmoid function 的 gradient Derive the gradients of the sigmoid function and show that it can be rewritten as a function of the function value (i.e., in some expression where only \(\sigma(x)\), but not \(x\), is present). Assume that the input \(x\) is a scalar for this question. Recall, the sigmoid function is \[\sigma(x)=\dfrac{1}{1+e^{-x}}\] solution \(\dfrac{\partial\sigma(x)}{\partial x}=\dfrac{-1}{(1+e^{-x})^2}\times e^{-x}\times (-1)=\dfrac{e^{-x}}{(1+e^{-x})^2}=\dfrac{1}{1+e^{-x}}\times{\dfrac{1+e^{-x}-1}{1+e^{-x}}}=\sigma(x)(1-\sigma(x))\) (b) 推導 cross entropy loss 的 gradient Derive the gradient with regard to the inputs of a softmax function when cross entropy loss is used for evaluation, i.e., find the gradients with respect to the softmax input vector \(\theta\), when the prediction is made by \(\hat y = softmax(\theta)\). Remember the cross entropy function is \[CE(y,\hat{y})=-\sum_i y_i log(\hat{y_i})\] where \(y\) is the one-hot label vector, and \(\hat{y}\) is the predicted probability vector for all classes. (Hint: you might want to consider the fact many elements of \(y\) are zeros, and assume that only the k-th dimension of \(y\) is one.) solution 計算 \(\dfrac{\partial CE(y,\hat{y})}{\partial \theta}\) 分成兩種情況： 1. m = n \(\dfrac{\partial CE(y_m,\hat{y_m})}{\partial \theta_n}=\dfrac{\partial -y_nlog\hat{y_n}}{\partial \theta_n}=-y_n \times \dfrac{1}{\hat{y_n}}\times \dfrac{\partial \dfrac{e^{\theta_n}}{\sum_j{e^{\theta j}}}}{\partial \theta_n}=-y_n \times \dfrac{1}{\hat{y_n}}\times\dfrac{e^{\theta n}\sum_j{e^{\theta_j}-e^{\theta n}e^{\theta n}}}{(\sum_j{e^{\theta_j}})^2}\) \(=-y_n \times \dfrac{1}{\hat{y_n}} \times \dfrac{e^{\theta n}}{\sum_j{e^{\theta_j}}}\times\dfrac{\sum_j{e^{\theta j}-e^{\theta n}}}{\sum_j{e^{\theta_j}}}=-y_n \times \dfrac{1}{\hat{y_n}} \times \hat{y_n} \times (1-\hat{y_n}) =-y_n(1-\hat{y_n})\) 2. m ≠ n \(\dfrac{\partial CE(y_m,\hat{y_m})}{\partial\theta\_n}=\dfrac{- \sum\_{m \neq n}{y_m log\hat{y_m}}}{\partial\theta\_n}=-\sum_{m \neq n} y_m \times \dfrac{1}{\hat{y_m}} \times \dfrac{\partial \dfrac{e^{\theta_m}}{\sum_j{e^{\theta_j}}}}{\partial \theta\_n}=-\sum_{m \neq n} y_m \times \dfrac{1}{\hat{y_m}} \times \dfrac{0-e^{\theta_n}e^{\theta_m}}{(\sum_j{e^{\theta\_j}})^2}\) \(=-\sum_{m \neq n} y_m \times \dfrac{1}{\hat{y_m}} \times \dfrac{-e^{\theta_n}}{\sum_j{e^{\theta_j}}} \times \dfrac{e^{\theta_m}}{\sum_j{e^{\theta\_j}}}=-\sum_{m \neq n} y_m \times \dfrac{1}{\hat{y_m}} \times (-\hat{y_n}) \times \hat{y\_m}=\sum_{m \neq n}y_m \hat{y_n}\) 因此 \(\dfrac{\partial CE}{\partial \theta_i}=-y_i(1-\hat{y\_i}) + \sum_{k \neq i}y_k \hat{y_i}=-y_i+\hat{y\_i}^2 + \sum_{k \neq i}y_k \hat{y_i}=\sum_k{y_k \hat{y_i}}-y_i=\hat{y_i}\sum_k{y_k}-y_i=\hat{y_i}-y_i\) (c) Backpropagation Derive the gradients with respect to the inputs \(x\) to an one-hidden-layer neural network (that is, find \(\frac{\partial J}{\partial x}\) where \(J = CE(y, \hat{y})\) is the cost function for the neural network). The neural network employs sigmoid activation function for the hidden layer, and softmax for the output layer. Assume the one-hot label vector is \(y\), and cross entropy cost is used. (Feel free to use \(\sigma&#39;(x)\) as the shorthand for sigmoid gradient, and feel free to define any variables whenever you see fit.) Recall that the forward propagation is as follows \[h = sigmoid(xW_1 + b_1), \hat{y} = softmax(hW_2 + b_2)\] Note that here we’re assuming that the input vector (thus the hidden variables and output probabilities) is a row vector to be consistent with the programming assignment. When we apply the sigmoid function to a vector, we are applying it to each of the elements of that vector. \(W_i\) and \(b_i (i = 1, 2)\) are the weights and biases, respectively, of the two layers. solution 令 \(z_1=xW_1+b_1,z_2=hW_2+b_2\) 則 \(\dfrac{\partial J}{\partial x}=\dfrac{\partial J}{\partial z2}\dfrac{\partial z2}{\partial h}\dfrac{\partial h}{\partial z1}\dfrac{\partial z1}{\partial x}=(\hat{y}-y)\times W_2^\top\times h(1-h)\times W_1^\top\) (d) 類神經網路的參數數量 How many parameters are there in this neural network, assuming the input is \(Dx\)-dimensional, the output is \(Dy\)-dimensional, and there are \(H\) hidden units? solution 根據上一題的圖，此神經網路有三個 layer：\(x,h,\hat{y}\)，其中的參數有 \(W_1,b_1,W_2,b_2\) 共四個： \(W_1: D_x \times H\) \(b_1: H\) \(W_2: H \times D_y\) \(b_2: D_y\) 總共有 \((D_x \times H)+H+(H \times D_y)+D_y=(D_x+1)H+(H+1)D_y\) 個參數。 (e) 實作 sigmoid function Fill in the implementation for the sigmoid activation function and its gradient in q2_sigmoid.py. Test your implementation using python q2_sigmoid.py. Again, thoroughly test your code as the provided tests may not be exhaustive. solution 實作出sigmoid以及其梯度sigmoid_grad兩個函式，直接利用(a)小題的結果可以輕鬆實現。 123def sigmoid(x): s = 1 / (1 + np.exp(-x)) return s 123def sigmoid_grad(s): ds = s * (1 - s) return ds (f) Gradient Check To make debugging easier, we will now implement a gradient checker. Fill in the implementation for gradcheck naive in q2_gradcheck.py. Test your code using python q2_gradcheck.py. solution 註解中指定使用 centered difference 方法來實作 gradient check，因為他比 forward / backward difference 方法誤差更小。 Forward difference approximation: \[f&#39;(x)\approx \dfrac{f(x+h)−f(x)}{h}\] Central difference approximations \[f&#39;(x)\approx \dfrac{f(x+h)-f(x-h)}{2h}\] Backward difference approximations: \[f&#39;(x)\approx \dfrac{f(x)−f(x−h)}{h}\] 1234567891011121314151617181920212223242526272829303132333435363738def gradcheck_naive(f, x): rndstate = random.getstate() random.setstate(rndstate) fx, grad = f(x) # Evaluate function value at original point h = 1e-4 # Do not change this! # Iterate over all indexes ix in x to check the gradient. it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite']) while not it.finished: ix = it.multi_index ### YOUR CODE HERE: x_upper, x_lower = x.copy(), x.copy() x_upper[ix] += h random.setstate(rndstate) f_upper = f(x_upper)[0] x_lower[ix] -= h random.setstate(rndstate) f_lower = f(x_lower)[0] numgrad = (f_upper - f_lower) / (2 * h) ### END YOUR CODE # Compare gradients reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix])) if reldiff &gt; 1e-5: print "Gradient check failed." print "First gradient error found at index %s" % str(ix) print "Your gradient: %f \t Numerical gradient: %f" % ( grad[ix], numgrad) return it.iternext() # Step to next dimension print "Gradient check passed!" (g) 實作類神經網路的 Forward 及 Backward Pass Now, implement the forward and backward passes for a neural network with one sigmoid hidden layer. Fill in your implementation in q2_neural.py. Sanity check your implementation with python q2_neural.py. solution 1234567891011121314151617181920212223242526272829303132333435def forward_backward_prop(X, labels, params, dimensions): ### Unpack network parameters (do not modify) ofs = 0 Dx, H, Dy = (dimensions[0], dimensions[1], dimensions[2]) W1 = np.reshape(params[ofs:ofs+ Dx * H], (Dx, H)) ofs += Dx * H b1 = np.reshape(params[ofs:ofs + H], (1, H)) ofs += H W2 = np.reshape(params[ofs:ofs + H * Dy], (H, Dy)) ofs += H * Dy b2 = np.reshape(params[ofs:ofs + Dy], (1, Dy)) # Note: compute cost based on `sum` not `mean`. ### YOUR CODE HERE: forward propagation h = sigmoid(np.dot(X, W1) + b1) y_hat = softmax(np.dot(h, W2) + b2) cost = cost = -np.sum(labels * np.log(y_hat)) ### END YOUR CODE ### YOUR CODE HERE: backward propagation d3 = y_hat - labels gradW2 = np.dot(h.T, d3) gradb2 = np.sum(d3, axis=0) dh = np.dot(d3, W2.T) grad_h = sigmoid_grad(h) * dh gradW1 = np.dot(X.T, grad_h) gradb1 = np.sum(grad_h, axis=0) ### END YOUR CODE ### Stack gradients (do not modify) grad = np.concatenate((gradW1.flatten(), gradb1.flatten(), gradW2.flatten(), gradb2.flatten())) return cost, grad 3. word2vec (a) 計算 Loss Function 對 \(v_c\) 的 gradient Assume you are given a “predicted” word vector \(v_c\) corresponding to the center word \(c\) for Skip-Gram, and word prediction is made with the softmax function found in word2vec models \[\hat{y_o}=p(o|c)=\dfrac{exp(u_o^\top v\_c)}{\sum\limits_{w=1}^V{exp(u_w^\top v_c)}}\] where \(u_w (w = 1, . . . , V )\) are the “output” word vectors for all words in the vocabulary. Assuming cross entropy cost is applied to this prediction and word o is the expected word (the o-th element of the one-hot label vector is one), derive the gradients with respect to \(v_c\). Hint: It will be helpful to use notation from question 2. For instance, letting \(\hat{y}\) be the vector of softmax predictions for every word, \(y\) as the expected word vector, and the loss function \[J_{softmax−CE}(o, v_c, U) = CE(y,\hat{y})\] where \(U = [u_1,u_2, · · · ,u_V ]\) is the matrix of all the output vectors. Make sure you state the orientation of your vectors and matrices. solution 求 \(\dfrac{\partial J}{\partial v_c}\) \(J=-\sum_i y_i log\hat{y_i}=-log \dfrac{exp(u_o^\top v\_c)}{\sum\limits_{w=1}^V exp(u_w^\top v_c)}=-log(exp(u_o^\top v\_c))+log(\sum\limits_{w=1}^V exp(u_w^\top v_c))\) \(\dfrac{\partial J}{\partial v_c}=\dfrac{\partial -log(exp(u_o^\top v_c))}{\partial v\_c}+\dfrac{\partial log(\sum_{w=1}^V exp(u_w^\top v_c))}{\partial v_c}\) \(\dfrac{-1}{exp(u_o^\top v_c)} \times exp(u_o^\top v_c) \times u_o=-u_o\) \(\dfrac{1}{\sum\limits_{w=1}^V exp(u_w^\top v\_c)} \times \sum\limits_{x=1}^V exp(u_x^\top v_c) \times u\_x=\sum\limits\_{x=1}^V \dfrac{exp(u_x^\top v\_c)}{\sum\limits_{w=1}^V exp(u_w^\top v_c)}\times u\_x=\sum\limits\_{x=1}^V \hat{y_x} u_x\) \(\dfrac{\partial J}{\partial v_c}=-u\_o+\sum\limits_{x=1}^V \hat{y_x} u_x=U(\hat{y}-y)\) (b) 計算 Loss Function 對 \(u_k\) 的 Gradient As in the previous part, derive gradients for the “output” word vectors \(u_k\)’s (including \(u_o\)). solution \(\dfrac{\partial J}{\partial u_w}=\dfrac{\partial -log(exp(u_o^\top v_c))}{\partial u\_w}+\dfrac{\partial log(\sum_{w=1}^V exp(u_w^\top v_c))}{\partial u_w}\) \(1. w=o\) \(=-v\_c+\dfrac{1}{\sum_{x=1}^V exp(u_x^\top v\_c)} \times \sum\limits_{w=1}^{V}exp(u_w^\top v_c) \times v_c=v\_c \times \sum\limits_{w=1}^V \dfrac{exp(v_w^\top v\_c)}{\sum\limits_{x=1}^V exp(u_x^\top v\_c)}-v\_c=\sum\limits\_{w=1}^V \hat{y_w}v_c-v_c=v_c(\hat{y}-y)^\top\) \(2. w\neq o\) \(=0+\dfrac{1}{\sum_{x=1}^V exp(u_x^\top v\_c)} \times \sum\limits_{w=1}^{V}exp(u_w^\top v_c) \times v_c=v\_c \times \sum\limits_{w=1}^V \dfrac{exp(v_w^\top v\_c)}{\sum\limits_{x=1}^V exp(u_x^\top v\_c)}=\sum\limits\_{w=1}^V \hat{y_w}v_c\) (c) Negative Sampling Repeat part (a) and (b) assuming we are using the negative sampling loss for the predicted vector \(v_c\), and the expected output word index is \(o\). Assume that \(K\) negative samples (words) are drawn, and they are \(1, 2, ..., K\) respectively for simplicity of notation \((o \notin {1, . . . , K})\). Again, for a given word, \(o\), denote its output vector as \(u_o\). The negative sampling loss function in this case is \[J_{neg-sample}(o,V_c,U)=-log(\sigma(u_o^\top v\_c))-\sum\limits_{k=1}^K log(\sigma(-u_k^\top v_c))\] where \(σ(·)\) is the sigmoid function. After you’ve done this, describe with one sentence why this cost function is much more efficient to compute than the softmax-CE loss (you could provide a speed-up ratio, i.e., the runtime of the softmaxCE loss divided by the runtime of the negative sampling loss). Note: the cost function here is the negative of what Mikolov et al had in their original paper, because we are doing a minimization instead of maximization in our code. solution \(1. \mathbf{\dfrac{\partial J}{\partial v_c}}=-dfrac{1}{\sigma(u_o^\top v_c)}\times \sigma(u_o^\top v_c) \times (1-\sigma(u_o^\top v_c)) \times v\_o - \sum\limits\_{k=1}^K \dfrac{1}{\sigma(-u_k^\top v_c)} \times \sigma(-u_k^\top v_c) \times (1-\sigma(-u_k^\top v_c)) \times (-u_k)\) \(=-u_o(1-\sigma(u_o^\top v\_c))+\sum\limits\_{k=1}^K u_k(1-\sigma(-u_k^\top v_c))\) \(2. \mathbf{\dfrac{\partial J}{\partial u_o}}=-\dfrac{1}{\sigma(u_o^\top v_c)}\times \sigma(u_o^\top v_c) \times (1-\sigma(u_o^\top v_c)) \times v_c\) \(=-v_c(1-\sigma(u_o^\top v_c))=(\sigma(u_o^\top v_c)-1)v_c\) \(3. \mathbf{\dfrac{\partial J}{\partial u_k}}=-\dfrac{1}{\sigma(-u_k^\top v_c)}\times \sigma(-u_k^\top v_c) \times (1-\sigma(-u_k^\top v_c)) \times (-v_c)\) \(=(1-\sigma(-u_k^\top v_c))v_c,k=1,2,...,K\) (d) 計算 Skip-gram 及 CBOW 的 Gradient Suppose the center word is \(c = w\_t\) and the context words are \([w\_{t−m}, . . ., w_{t−1}, w\_t, w\_{t+1},. . ., w_{t+m}]\), where \(m\) is the context size. Derive gradients for all of the word vectors for Skip-Gram and CBOW given the previous parts. Hint: feel free to use \(F(o, v\_c)\) (where o is the expected word) as a placeholder for the \(J_{softmax−CE}(o, v\_c, ...)\) or \(J_{neg−sample}(o, v_c, ...)\) cost functions in this part — you’ll see that this is a useful abstraction for the coding part. That is, your solution may contain terms of the form \(\frac{\partial F(o,v_c)} {\partial ...}\) . Recall that for skip-gram, the cost for a context centered around \(c\) is \[J\_{skip-gram}(w\_{t−m...t+m}) = \sum\_{−m≤j≤m,j\neq 0}F(w_{t+j} , v_c)\] where \(w_{t+j}\) refers to the word at the j-th index from the center. CBOW is slightly different. Instead of using \(v_c\) as the predicted vector, we use \(\hat{v}\) defined below. For (a simpler variant of) CBOW, we sum up the input word vectors in the context \[\hat{v} = \sum\_{−m≤j≤m,j\neq =0}v_{w_t+j}\] then the CBOW cost is \[J\_{CBOW}(w_{c−m...c+m}) = F(w_t, \hat{v})\] Note: To be consistent with the \(\hat{v}\) notation such as for the code portion, for skip-gram \(\hat{v} = v_c\). solution Skip-gram \(\dfrac{\partial J\_{skip-gram}(w\_{t-m...t+m})}{\partial U}=\sum\_{-m≤j≤m,j\neq 0} \dfrac{\partial F(w\_{t+j},v_c)}{\partial U}\) \(\dfrac{\partial J\_{skip-gram}(w\_{t-m...t+m})}{\partial v\_c}=\sum\_{-m≤j≤m,j\neq 0} \dfrac{\partial F(w\_{t+j},v_c)}{\partial v_c}\) \(\dfrac{\partial J\_{skip-gram}(w\_{t-m...t+m})}{\partial v_{t+j}}=0,\forall j \neq c\) CBOW \(\dfrac{\partial J\_{CBOW}(w\_{t-m...t+m})}{\partial U}=\dfrac{\partial F(w_t,\hat{v})}{\partial U}\) \(\dfrac{\partial J\_{CBOW}(w\_{t-m...t+m})}{\partial v\_{w\_{t+j}}}=\dfrac{\partial F(w_t,\hat{v})}{\partial \hat{v}},\forall j \in \{-m,...,-1,1,...,m\}\) \(\dfrac{\partial J\_{CBOW}(w\_{t-m...t+m})}{\partial U}=0,\forall j \notin \{-m,...,-1,1,...,m\}\) (e) 實作 Skip-gram In this part you will implement the word2vec models and train your own word vectors with stochastic gradient descent (SGD). First, write a helper function to normalize rows of a matrix in q3_word2vec.py. In the same file, fill in the implementation for the softmax and negative sampling cost and gradient functions. Then, fill in the implementation of the cost and gradient functions for the skipgram model. When you are done, test your implementation by running python q3_word2vec.py. Note: If you choose not to implement CBOW (part h), simply remove the NotImplementedError so that your tests will complete. solution 實作q3_word2vec.py中的以下函式： normalizeRows softmaxCostAndGradient negSamplingCostAndGradient skipgram normalizeRows 對每個列向量進行正規化，公式：\(\hat{v}=\dfrac{v}{|v|}\)。 1234def normalizeRows(x): length = np.sqrt(np.sum(np.power(x, 2), axis=1)) x /= length[:,None] return x softmaxCostAndGradient predicted: \(v_c\)、target: \(o\)、outputVector: \(U^\top\)、gradz2: \(\hat{y}-y\)、gradPred: \(\dfrac{\partial J}{\partial v_c}\)、grad: \(\dfrac{\partial J}{\partial u_k}\) 利用 (a)(b) 小題的結果： \(\dfrac{\partial J}{\partial v_c}=U(\hat{y}-y)\) \(\dfrac{\partial J}{\partial u_k}=v_c(\hat{y}-y)^\top\) 123456789101112def softmaxCostAndGradient(predicted, target, outputVectors, dataset): ### YOUR CODE HERE y_hat = softmax(np.dot(outputVectors, predicted)) cost = -np.log(y_hat[target]) gradz2 = y_hat.copy() gradz2[target] -= 1.0 gradPred = np.dot(outputVectors.T, gradz2) grad = np.outer(gradz2, predicted) ### END YOUR CODE return cost, gradPred, grad negSamplingCostAndGradient 利用 (c) 小題的結果： gradPred: \(\mathbf{\dfrac{\partial J}{\partial v_c}}=-u_o(1-\sigma(u_o^\top v\_c))+\sum\limits\_{k=1}^K u_k(1-\sigma(-u_k^\top v_c))\) grad: \(\mathbf{\dfrac{\partial J}{\partial u_o}}=(\sigma(u_o^\top v_c)-1)v_c\) grad: \(\mathbf{\dfrac{\partial J}{\partial u_k}}=(1-\sigma(-u_k^\top v_c))v_c,k=1,2,...,K\) cost function 為 \(CE(y,\hat{y})\) 1234567891011121314151617181920212223def negSamplingCostAndGradient(predicted, target, outputVectors, dataset, K=10): # Sampling of indices is done for you. Do not modify this if you # wish to match the autograder and receive points! indices = [target] indices.extend(getNegativeSamples(target, dataset, K)) ### YOUR CODE HERE grad = np.zeros(outputVectors.shape) gradPred = np.zeros(predicted.shape) z = sigmoid(np.dot(outputVectors[target].T, predicted)) cost = -np.log(z) grad[target] += (z - 1.0) * predicted gradPred += (z - 1.0) * outputVectors[target] for k in xrange(1, K + 1): index = indices[k] z = sigmoid(np.dot(-outputVectors[index].T, predicted)) cost -= np.log(z) grad[index] -= (z - 1.0) * predicted gradPred -= (z - 1.0) * outputVectors[index] ### END YOUR CODE return cost, gradPred, grad skipgram 利用 (d) 小題的結果： Skip-gram gradOut:\(\dfrac{\partial J\_{skip-gram}(w\_{t-m...t+m})}{\partial U}=\sum\_{-m≤j≤m,j\neq 0} \dfrac{\partial F(w\_{t+j},v_c)}{\partial U}\) gradIn: \(\dfrac{\partial J\_{skip-gram}(w\_{t-m...t+m})}{\partial v\_c}=\sum\_{-m≤j≤m,j\neq 0} \dfrac{\partial F(w\_{t+j},v_c)}{\partial v_c}\) gradIn: \(\dfrac{\partial J\_{skip-gram}(w\_{t-m...t+m})}{\partial v_{t+j}}=0,\forall j \neq c\) 1234567891011121314151617181920def skipgram(currentWord, C, contextWords, tokens, inputVectors, outputVectors, dataset, word2vecCostAndGradient=softmaxCostAndGradient): cost = 0.0 gradIn = np.zeros(inputVectors.shape) gradOut = np.zeros(outputVectors.shape) ### YOUR CODE HERE current_word_index = tokens[currentWord] vc = inputVectors[current_word_index] for j in contextWords: target_index = tokens[j] c_cost, c_grad_in, c_grad_out = word2vecCostAndGradient(vc, target_index, outputVectors, dataset) cost += c_cost gradIn[current_word_index] += c_grad_in gradOut += c_grad_out ### END YOUR CODE return cost, gradIn, gradOut 執行python q3_word2vec.py，產生以下結果代表成功。 1234567Testing normalizeRows...[[ 0.6 0.8 ] [ 0.4472136 0.89442719]]==== Gradient check for skip-gram ====Gradient check passed!Gradient check passed! (f) Stochastic Gradient Descent Complete the implementation for your SGD optimizer in q3_sgd.py. Test your implementation by running python q3_sgd.py. solution step就是 learning rate，原始程式碼已經給定，直接將其乘上 gradient 即可。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849def sgd(f, x0, step, iterations, postprocessing=None, useSaved=False, PRINT_EVERY=10): # Anneal learning rate every several iterations ANNEAL_EVERY = 20000 if useSaved: start_iter, oldx, state = load_saved_params() if start_iter &gt; 0: x0 = oldx step *= 0.5 ** (start_iter / ANNEAL_EVERY) if state: random.setstate(state) else: start_iter = 0 x = x0 if not postprocessing: postprocessing = lambda x: x expcost = None for iter in xrange(start_iter + 1, iterations + 1): # Don't forget to apply the postprocessing after every iteration! # You might want to print the progress every few iterations. cost = None ### YOUR CODE HERE cost, grad = f(x) x -= step * grad postprocessing(x) ### END YOUR CODE if iter % PRINT_EVERY == 0: if not expcost: expcost = cost else: expcost = .95 * expcost + .05 * cost print "iter %d: %f" % (iter, expcost) if iter % SAVE_PARAMS_EVERY == 0 and useSaved: save_params(iter, x) if iter % ANNEAL_EVERY == 0: step *= 0.5 return x (g) 訓練詞向量 Show time! Now we are going to load some real data and train word vectors with everything you just implemented! We are going to use the Stanford Sentiment Treebank (SST) dataset to train word vectors, and later apply them to a simple sentiment analysis task. You will need to fetch the datasets first. To do this, run sh get datasets.sh. There is no additional code to write for this part; just run python q3_run.py. Note: The training process may take a long time depending on the efficiency of your implementation (an efficient implementation takes approximately an hour). Plan accordingly! When the script finishes, a visualization for your word vectors will appear. It will also be saved as q3_word vectors.png in your project directory. Include the plot in your homework write up. Briefly explain in at most three sentences what you see in the plot. solution 執行sh get datasets.sh下載 Stanford Sentiment Treebank (SST) 資料集。 接著再執行python q3_run.py開始訓練，預設iteration=40000，每 10 次 iteration 會印出一次當前的 cost，沒意外的話 cost 應該要隨著訓練的過程遞減。 12345678iter 39950: 9.435311iter 39960: 9.492463iter 39970: 9.520291iter 39980: 9.524589iter 39990: 9.550077iter 40000: 9.577164sanity check: cost at convergence should be around or below 10training took 4038 seconds 訓練時間約為 67 分鐘，訓練使用的處理器為 i7-8750H，可以參考一下。 訓練完成後會產生一張圖q3_word_vectors.png： q3_word_vectors.png 在q3_run.py中可以發現在原始程式碼中，挑選了一些詞來進行視覺化，透過 SVD 降維後使得能夠在二維平面上觀察這些詞的相對距離。 (h) 實作 CBOW Implement the CBOW model in q3_word2vec.py. Note: This part is optional but the gradient derivations for CBOW in part (d) are not!. solution 同樣利用 (d) 小題的結果： gradOut: \(\dfrac{\partial J\_{CBOW}(w\_{t-m...t+m})}{\partial U}=\dfrac{\partial F(w_t,\hat{v})}{\partial U}\) gradIn: \(\dfrac{\partial J\_{CBOW}(w\_{t-m...t+m})}{\partial v\_{w\_{t+j}}}=\dfrac{\partial F(w_t,\hat{v})}{\partial \hat{v}},\forall j \in \{-m,...,-1,1,...,m\}\) gradIn: \(\dfrac{\partial J\_{CBOW}(w\_{t-m...t+m})}{\partial U}=0,\forall j \notin \{-m,...,-1,1,...,m\}\) 1234567891011121314151617def cbow(currentWord, C, contextWords, tokens, inputVectors, outputVectors, dataset, word2vecCostAndGradient=softmaxCostAndGradient): cost = 0.0 gradIn = np.zeros(inputVectors.shape) gradOut = np.zeros(outputVectors.shape) ### YOUR CODE HERE indices = [tokens[contextWord] for contextWord in contextWords] v_hat = np.sum(inputVectors[indices], axis=0) target_index = tokens[currentWord] cost, grad_in, gradOut = word2vecCostAndGradient(v_hat, target_index, outputVectors, dataset) for j in indices: gradIn[j] += grad_in ### END YOUR CODE return cost, gradIn, gradOut 執行python q3_word2vec.py，在完成 (e)(h) 小題後的結果應如下： 1234567891011Testing normalizeRows...[[ 0.6 0.8 ] [ 0.4472136 0.89442719]]==== Gradient check for skip-gram ====Gradient check passed!Gradient check passed!==== Gradient check for CBOW ====Gradient check passed!Gradient check passed! 4. Sentiment Analysis Now, with the word vectors you trained, we are going to perform a simple sentiment analysis. For each sentence in the Stanford Sentiment Treebank dataset, we are going to use the average of all the word vectors in that sentence as its feature, and try to predict the sentiment level of the said sentence. The sentiment level of the phrases are represented as real values in the original dataset, here we’ll just use five classes: “very negative (−−)”, “negative (−)”, “neutral”, “positive (+)”, “very positive (++)” which are represented by 0 to 4 in the code, respectively. For this part, you will learn to train a softmax classifier, and perform train/dev validation to improve generalization. (a) 以特徵向量來表示一個句子 Implement a sentence featurizer. A simple way of representing a sentence is taking the average of the vectors of the words in the sentence. Fill in the implementation in q4_sentiment.py. solution 將句子中所有詞的詞向量進行平均，將得到的向量用來表示整個句子。 Inputs: tokens: 一個 dictionary，key 為詞，value 為該詞的索引 wordVectors: 詞向量 sentence: 一個句子 Output: sentVector: 句子的特徵向量 12345678910111213def getSentenceFeatures(tokens, wordVectors, sentence): sentVector = np.zeros((wordVectors.shape[1],)) ### YOUR CODE HERE for word in sentence: index = tokens[word] sentVector += wordVectors[index] sentVector /= float(len(sentence)) ### END YOUR CODE assert sentVector.shape == (wordVectors.shape[1],) return sentVector (b) 正規化 (Regularization) Explain in at most two sentences why we want to introduce regularization when doing classification (in fact, most machine learning tasks). solution 正規化可以避免 overfitting，使得模型更加 generalization。 (c) 尋找超參數 (hyperparameter) Fill in the hyperparameter selection code in q4_sentiment.py to search for the “optimal” regularization parameter. You need to implement both getRegularizationValues and chooseBestModel. Attach your code for chooseBestModel to your written write-up. You should be able to attain at least 36.5% accuracy on the dev and test sets using the pretrained vectors in part (d). solution 參考其他人的做法，使用 log function 產生遞增的值。 123456def getRegularizationValues(): values = None # Assign a list of floats in the block below ### YOUR CODE HERE values = np.logspace(-5, 2, num=100, base=10) ### END YOUR CODE return sorted(values) 12345678def chooseBestModel(results): bestResult = None ### YOUR CODE HERE bestResult = max(results, key=lambda x: x["dev"]) ### END YOUR CODE return bestResult (d) 模型比較 Run python q4_sentiment.py --yourvectors to train a model using your word vectors from q3. Now, run python q4_sentiment.py --pretrained to train a model using pretrained GloVe vectors (on Wikipedia data). Compare and report the best train, dev, and test accuracies. Why do you think the pretrained vectors did better? Be specific and justify with 3 distinct reasons. yourvectors Reg Train Dev Test 1.00E-05 31.004 32.516 30.452 1.15E-04 31.063 32.516 30.362 1.12E-03 31.133 32.516 30.362 1.10E-02 30.922 32.334 29.955 1.07E-01 30.290 31.789 29.864 1.05E+00 28.816 29.609 27.059 12Best regularization value: 2.26E-05Test accuracy (%): 30.316742 pretrained GloVe vectors (on Wikipedia data) Reg Train Dev Test 1.00E-05 39.923 36.421 37.059 1.15E-04 39.958 36.512 37.014 1.12E-03 39.899 36.331 37.014 1.10E-02 39.923 36.331 37.195 1.07E-01 39.782 36.240 37.149 1.05E+00 39.478 36.512 37.330 12Best regularization value: 1.20E+01Test accuracy (%): 37.556561 觀察兩者的表現可以發現使用 pretrained 的模型效果明顯比較好，可能原因如下： 更高維度的詞向量可能包含更多的資訊 GloVe 使用更大的語料庫進行訓練 word2vec 與 GloVe 的差異 (參考資料) (e) Regularization Term 的變化在 train set 及 dev set 的表現差異 Plot the classification accuracy on the train and dev set with respect to the regularization value for the pretrained GloVe vectors, using a logarithmic scale on the x-axis. This should have been done automatically. Include q4_reg acc.png in your homework write up. Briefly explain in at most three sentences what you see in the plot. solution (f) Confusion Matrix We will now analyze errors that the model makes (with pretrained GloVe vectors). When you ran python q4_sentiment.py --pretrained, two files should have been generated. Take a look at q4_dev_conf.png and include it in your homework writeup. Interpret the confusion matrix in at most three sentences. solution 由預測結果與標準答案的數量比較所產生的 confusion matrix，由左上到右下的對角線所經過的格子為 True Positive 的數量。 (g) 為何分類錯誤？ Next, take a look at q4_dev_pred.txt. Choose 3 examples where your classifier made errors and briefly explain the error and what features the classifier would need to classify the example correctly (1 sentence per example). Try to pick examples with different reasons. solution 將每筆分類結果與標準答案的差距取絕對值，統計如下： 差距 0 1 2 3 4 數量 409 444 188 59  1 可以看到其實大部分的誤差都在 1 分以內，若將標準降低為差距 1 分或以下就算分類正確，準確率會從原來的 37% 大幅提高到 77%，因此整體的預測其實還算是準確的。 Answer Predicted Sentence Possible Issue 0 4 a lackluster , unessential sequel to the classic disney adaptation of j.m. barrie 's peter pan . 唯一一筆預測與實際分數差距4分的資料，看起來 lackluster, unessential 都是負面的詞彙，classic 則是偏向正面的詞彙，而 j.m. barrie 's peter pan 則比較算是雜訊，其餘則是稍微中性的詞語，若將專有名詞去掉或許能提升預測的分數 4 1 the draw -lrb- for `` big bad love '' -rrb- is a solid performance by arliss howard . 太多無意義的符號干擾 4 1 it is amusing , and that 's all it needs to be . amusing 應該比較偏向正面的詞彙，但是其餘的詞感覺大部分都是冗詞。]]></content>
      <categories>
        <category>學校課程</category>
        <category>自然語言處理</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode#204 Count Primes]]></title>
    <url>%2F2018%2F09%2F10%2FLeetCode-204-Count-Primes%2F</url>
    <content type="text"><![CDATA[問題 經典問題，給一個非負整數 n ，問小於 n 的質數有幾個。 輸入與輸出 Input: 10 Output: 4 Explanation: There are 4 prime numbers less than 10, they are 2, 3, 5, 7. 方法 建質數表。 方法一： 逐一檢查 1 ~ n - 1 的整數中有多少個質數，但效率不好，因此採用第二種方法。 方法二： 埃拉托斯特尼篩法，從 2 開始將已知質數的倍數標記成合數，可以減少很多不必要的計算。 123456789101112131415161718int countPrimes(int n)&#123; int i, j, prime_count = 0; bool* primes = malloc(n * sizeof(bool)); memset(primes, true, n * sizeof(bool)); for (i = 2; i &lt; n; i++) &#123; if (primes[i]) &#123; prime_count++; for (j = 2 * i; j &lt; n; j += i) primes[j] = false; &#125; &#125; return prime_count;&#125;]]></content>
      <categories>
        <category>程式解題</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>C</tag>
        <tag>Prime</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode#198 House Robber]]></title>
    <url>%2F2018%2F09%2F10%2FLeetCode-198-House-Robber%2F</url>
    <content type="text"><![CDATA[問題 有一個職業盜賊想要在某條街上行竊，這條街上有著一整排的房屋，每棟房屋內都有特定數量的金錢，他希望能夠偷到越多錢越好。但是想在這條街行竊有個限制，每兩棟相鄰的房屋之間都有連結保全系統，若他行竊兩棟相鄰的房屋，保全系統就會自動聯繫警察。 給定一個非負整數的list，表示每間房屋內的金錢數量，請問在不驚動保全系統的前提下，竊賊能夠偷到最多的金錢數量是多少？ 輸入與輸出 Input: [1,2,3,1] Output: 4 Explanation: Rob house 1 (money = 1) and then rob house 3 (money = 3). Total amount you can rob = 1 + 3 = 4. Input: [2,7,9,3,1] Output: 12 Explanation: Rob house 1 (money = 2), rob house 3 (money = 9) and rob house 5 (money = 1). Total amount you can rob = 2 + 9 + 1 = 12. 方法 使用動態規劃，假設 an 為行竊第 n 棟房屋能夠獲得的金錢數量，可得 an = max(an-2, an-3)。 1234567891011121314151617181920212223242526int max(int a, int b)&#123; if (a &gt; b) return a; else return b;&#125;int rob(int* nums, int numsSize) &#123; if (numsSize == 0) return 0; int i; if (numsSize &gt;= 3) nums[2] += nums[0]; for (i = 3; i &lt; numsSize; i++) nums[i] += max(nums[i - 2], nums[i - 3]); if (numsSize == 1) return nums[0]; else if (numsSize == 2) return max(nums[0], nums[1]); else return max(nums[i - 1], nums[i - 2]);&#125;]]></content>
      <categories>
        <category>程式解題</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>C</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[挑戰LeetCode Weekly Contest 50]]></title>
    <url>%2F2017%2F09%2F23%2F%E6%8C%91%E6%88%B0LeetCode-Weekly-Contest-50%2F</url>
    <content type="text"><![CDATA[解題心得 這次的題目有兩題都可以用遞迴來寫，但對我來說腦袋真的轉不過來啊！ 果然如俗話所說：「遞迴只應天上有，凡人應當用迴圈」，描述的真是貼切，但遞迴之美，總是讓我們這些凡人流連忘返，遞迴啊遞迴，究竟何時才能讓願意人們真正看清您的真面目呢？ 第1題 680. Valid Palindrome II 問題 給一個非空的字串s，在最多只能刪除一個字元的條件下，s是否能成為一個回文字串。 字串只包含小寫字母a-z，字串長度不超過50000。 輸入與輸出 Input: &quot;aba&quot; Output: True Input: &quot;abca&quot; Output: True Explanation: You could delete the character &#39;c&#39;. 方法 使用two pointer的技巧，low與high分別從前面與後面開始走，若遇到不一樣的字元，則檢查在刪除其中任一個字元後是否能滿足回文的條件。 1234567891011121314151617181920212223class Solution: def validPalindrome(self, s): low = 0 high = len(s) - 1 while low &lt; high: if s[low] == s[high]: low += 1 high -= 1 else: if self.isPalindrome(s, low+1, high): return True if self.isPalindrome(s, low, high-1): return True return False return True def isPalindrome(self, s, low, high): while low &lt; high: if s[low] != s[high]: return False low += 1 high -= 1 return True 第2題 677. Map Sum Pairs 問題 實作一個字典，能夠插入一對(key, value)，其中key為字串，value為整數。 輸入一個字串prefix，輸出字典中所有以prefix為開頭的所有key所對應的value的和。 輸入與輸出 Input: &quot;aba&quot; Output: True Input: &quot;abca&quot; Output: True Explanation: You could delete the character &#39;c&#39;. 方法 直接使用python的內建dict。 1234567891011121314151617class MapSum: def __init__(self): self.d = dict() def insert(self, key, val): self.d[key] = val def sum(self, prefix): summation = 0 length = len(prefix) for k, v in self.d.items(): if k[:length] == prefix: summation += v return summation 第3題 678. Valid Parenthesis String 問題 輸入一個字串，字串內的元素只包含(,),*三種，判斷此字串是否滿足以下條件： 1. 任何一個左方的(都必須對應到一個右方的) 2. 任何一個右方的)都必須對應到一個左方的( 3. 左方的(必須出現在右方的)之前 4. *可以是(或)或空字串其中一種 5. 空字串是合法的 輸入與輸出 Input: &quot;()&quot; Output: True Input: &quot;(*)&quot; Output: True Input: &quot;(*))&quot; Output: True 方法 使用一個變數p紀錄括號的消長，出現一組(,)便可抵消。 至於出現*的話，因為有三種可能，因此用backtracking分支成三種可能，只要其中一種能夠滿足條件就代表合法。 123456789101112131415161718class Solution: def checkValidString(self, s): return self.backtracking(list(s), 0, 0) def backtracking(self, c, p, index): if p &lt; 0: return False for i in range(index, len(c)): x = c[i] if x == '(': p += 1 elif x == ')': if p &lt;= 0: return False p -= 1 else: return self.backtracking(c, p + 1, i + 1) or self.backtracking(c, p - 1, i + 1) or self.backtracking(c, p, i + 1) return p == 0 以上是使用python的解法，可惜跳出 TLE，因此將同樣的方法改寫成C++的版本，就可以通過了。 123456789101112131415161718192021222324class Solution &#123;public: bool checkValidString(string s) &#123; return backtracking(s, 0, 0); &#125; bool backtracking(string c, int p, int index) &#123; if (p &lt; 0) return false; for (int i = index; i &lt; c.length(); i++) &#123; char x = c[i]; if (x == '(') p++; else if (x == ')') &#123; if (p &lt;= 0) return false; p--; &#125; else return backtracking(c, p+1, i+1) || backtracking(c, p-1, i+1) || backtracking(c, p, i+1); &#125; return p == 0; &#125;&#125;; 第4題 679. 24 Game 問題 給四個範圍1~9的整數，問是否能經過+,-,*,/,(,)的運算後的得到結果為24。 輸入與輸出 Input: [4, 1, 8, 7] Output: True Explanation: (8-4) * (7-1) = 24 Input: [1, 2, 1, 2] Output: False 方法 總共有 4 個數字以及 4 個運算子，括號的部份代表運算的先後順序，因此計算的步驟如下： 從 4 個數字中取 2 個(順序有分)，再從 4 個運算子當中選 1 個，所以有 4 x 5 x 3 種選法 第 1 步驟運算完後數字剩下 3 個，再從 3 個數字中取 2 個(順序有分)，同樣從 4 個運算子當中選 1 個，所以有 3 x 2 x 4 種選法 第 2 步驟運算完後數字剩下 2 個，再從 2 個數字當中取 2 個(順序有分)，從 4 個運算子中選一個，所以有 2 x 4 種選法 因此總共有 4 x 3 x 3 x 3 x 2 x 4 x 2 x 4 = 9216 種組合方法 所以用遞迴枚舉出所有的方法就可以將所有答案計算出來。要注意的是除數不能為 0 ，還有由於浮點數運算的誤差，因此計算結果未必剛好等於 24 ，可能會有非常微小的誤差值，所以要給一個容忍值才行。 1234567891011121314class Solution(object): def judgePoint24(self, A): if len(A) == 1: if abs(A[0] - 24) &lt; 1e-6: return True for i in range(len(A)): for j in range(len(A)): if i != j: B = [A[x] for x in range(len(A)) if x != i and x != j] if self.judgePoint24([A[i] + A[j]] + B) or self.judgePoint24([A[i] - A[j]] + B) or self.judgePoint24([A[i] * A[j]] + B): return True if A[j] and self.judgePoint24([A[i] / A[j]] + B): return True return False]]></content>
      <categories>
        <category>程式解題</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>Python</tag>
        <tag>LeetCode Weekly Contest</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[挑戰LeetCode Weekly Contest 49]]></title>
    <url>%2F2017%2F09%2F10%2F%E6%8C%91%E6%88%B0LeetCode-Weekly-Contest-49%2F</url>
    <content type="text"><![CDATA[三度挑戰 這次的題目，前兩題真的簡單，至於後兩題，真心難QQ ，最終成績 753 / 2362。 第1題 674. Longest Continuous Increasing Subsequence 問題 給一個未排序的整數陣列，找出最長的連續遞增子序列的長度。 輸入範例 1 [1,3,5,4,7] 輸出範例 1 3 輸入範例 2 [2,2,2,2,2] 輸出範例 2 1 方法 遞增序列的特性就是較晚出現的元素一定大於較先出現的元素，遍歷整個陣列，因為要求為「連續」，所以只要遇到不符合遞增條件的元素，就要重新計算。 1234567891011121314class Solution(object): def findLengthOfLCIS(self, nums): if not nums: return 0 maximum = 1 current = 1 for i in range(1, len(nums)): if nums[i - 1] &lt; nums[i]: current += 1 else: maximum = max(maximum, current) current = 1 maximum = max(maximum, current) return maximum 第2題 676. Implement Magic Dictionary 問題 定義一個陣列，由許多字串組成，輸入一個字串，若陣列中任一字串更改一個字母後，與輸入的字串相同，回傳True，反之則回傳False。 輸入與輸出 Input: buildDict([&quot;hello&quot;, &quot;leetcode&quot;]), Output: Null Input: search(&quot;hello&quot;), Output: False Input: search(&quot;hhllo&quot;), Output: True Input: search(&quot;hell&quot;), Output: False Input: search(&quot;leetcoded&quot;), Output: False 方法 這題沒有什麼特殊的技巧，只要將輸入字串與陣列內的字串一一比對即可。 1234567891011121314151617181920class MagicDictionary(object): def __init__(self): self.magic = [] def buildDict(self, dic): self.magic = dic def search(self, word): for m in self.magic: modify = 0 if len(m) == len(word): for i in range(len(m)): if m[i] != word[i]: modify += 1 if modify == 1: return True return False ## 第3題 675. Cut Off Trees for Golf Event ## ### 問題 ### 輸入一個二維陣列，0代表無法通過的障礙物，1代表可以通行的草皮，大於1的數代表一棵樹，同樣可以通行，數字大小代表樹的高度。 今天我們要根據樹的高度由低到高進行修剪，經過修剪後的樹會變成草皮，我們要從(0, 0)出發，依序走訪所有的樹，輸出修剪所有的樹所需經過最短的距離，若有任何一棵樹無法抵達，則輸出-1。 ### 輸入與輸出 ### Input: [ [1,2,3], [0,0,4], [7,6,5]] Output: 6 Input: [ [1,2,3], [0,0,0], [7,6,5]] Output: -1 Input: [ [2,3,4], [0,0,5], [8,7,6]] Output: 6 Explanation: You started from the point (0,0) and you can cut off the tree in (0,0) directly without walking. ### 方法 ### 我先將所有非0或1的數加入一個陣列中，將其排序後由小至大取出，利用BFS計算從目前位置到各點的最短距離，取出目前位置到欲修剪的距離。 很不幸的我的方法吃了一發 TLE ，應該是做的 BFS 有太多多餘的範圍了，應該只要找出目前位置到目的地的最短距離就好，之後若有時間再想辦法補上新的方法吧。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354class Solution(object): def cutOffTree(self, forest): import math self.forest = forest self.r, self.c = len(self.forest), len(self.forest[0]) f = [col for row in forest for col in row if col != 0 and col != 1] f.sort(reverse=True) total_step = 0 cur_pos = (0, 0) while f: self.num_step = [[math.inf for j in range(self.c)] for i in range(self.r)] self.num_step[cur_pos[0]][cur_pos[1]] = 0 self.min_step(cur_pos) next_cut = f.pop() next_pos = self.find_pos(next_cut) if not self.num_step[next_pos[0]][next_pos[1]] &lt; math.inf: return -1 total_step += self.num_step[next_pos[0]][next_pos[1]] cur_pos = next_pos return total_step def min_step(self, pos): i, j = pos[0], pos[1] # print(i, j, self.num_step[i][j]) if not (0 &lt;= i &lt; self.r and 0 &lt;= j &lt; self.c): return if 0 &lt;= i - 1 &lt; self.r and 0 &lt;= j &lt; self.c and self.forest[i - 1][j]: if self.num_step[i - 1][j] &gt;= self.num_step[i][j] + 1: self.num_step[i - 1][j] = self.num_step[i][j] + 1 self.min_step((i - 1, j)) if 0 &lt;= i + 1 &lt; self.r and 0 &lt;= j &lt; self.c and self.forest[i + 1][j]: if self.num_step[i + 1][j] &gt;= self.num_step[i][j] + 1: self.num_step[i + 1][j] = self.num_step[i][j] + 1 self.min_step((i + 1, j)) if 0 &lt;= i &lt; self.r and 0 &lt;= j - 1 &lt; self.c and self.forest[i][j - 1]: if self.num_step[i][j - 1] &gt;= self.num_step[i][j] + 1: self.num_step[i][j - 1] = self.num_step[i][j] + 1 self.min_step((i, j - 1)) if 0 &lt;= i &lt; self.r and 0 &lt;= j + 1 &lt; self.c and self.forest[i][j + 1]: if self.num_step[i][j + 1] &gt;= self.num_step[i][j] + 1: self.num_step[i][j + 1] = self.num_step[i][j] + 1 self.min_step((i, j + 1)) def find_pos(self, num): for i in range(len(self.forest)): if num in self.forest[i]: return (i, self.forest[i].index(num)) return -1 第4題 673. Number of Longest Increasing Subsequence 問題 給一個未排序的整數陣列，找出最長的遞增子序列的數量。 輸入與輸出 Input: [1,3,5,4,7] Output: 2 Explanation: The two longest increasing subsequence are [1, 3, 4, 7] and [1, 3, 5, 7]. Input: [2,2,2,2,2] Output: 5 Explanation: The length of longest continuous increasing subsequence is 1, and there are 5 subsequences&#39; length is 1, so output 5. 方法 計算LIS的長度有兩種思維方式，第一種是找出哪些數字能接在nums[i]後面，第二種是找出nums[i]能接在哪些數字後面，這裡採用第二種方法。 這裡需要用到兩個陣列： 1. LIS: LIS[i]代表以nums[i]結束的最長遞增子序列的長度 2. cnt: cnt[i]代表以nums[i]結束的最長遞增子序列的數量 假設nums[i]能接在nums[j]後面，代表nums[i] &gt; nums[j]，這裡有三種狀況可以討論： 1. LIS[i] &gt; LIS[j] + 1 : nums[i]接在 nums[j]後面比原本還要短，不接 2. LIS[i] = LIS[j] + 1 : nums[i]接在 nums[j]後面比跟原本一樣長，只要把數量相加就好 3. LIS[i] &lt; LIS[j] + 1 : nums[i]接在 nums[j]後面比原本還要長，長度加1，繼承前面的數量 最後將所有LIS[i]為最大值所對應的cnt[i]相加即為答案。 123456789101112131415161718class Solution: def findNumberOfLIS(self, nums): if nums == []: return 0 # 初始化陣列，每個數字本身就是長度為1的LIS LIS, cnt = [1] * len(nums), [1] * len(nums) # nums[i]能接在哪些數字後面 for i in range(1, len(nums)): for j in range(0, i): if nums[i] &gt; nums[j]: if LIS[i] == LIS[j] + 1: cnt[i] += cnt[j] elif LIS[i] &lt; LIS[j] + 1: cnt[i] = cnt[j] LIS[i] = LIS[j] + 1 return sum((y for x, y in zip(LIS, cnt) if x == max(LIS)))]]></content>
      <categories>
        <category>程式解題</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>Python</tag>
        <tag>LeetCode Weekly Contest</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二元樹(Binary Tree)]]></title>
    <url>%2F2017%2F09%2F08%2F%E4%BA%8C%E5%85%83%E6%A8%B9-Binary-Tree%2F</url>
    <content type="text"><![CDATA[樹 Tree 生活中常見各式各樣的數狀圖，像是族譜、賽程表等等，至於在計算機科學中，tree則是一種抽象資料型別(Abstract Data Type)，是一種具有階層式 (Hierarchical) 的數狀資料集合。 定義 樹 (tree) 是一個由一個或多個節點 (node) 所組成的集合，滿足以下兩點： 1. 有一個特別的節點叫作「根」(root) 2. 其餘的節點 (node) 可被分割為 n &gt;= 0 個互斥 (disjoint) 的集合 T1, ..., Tn , 每個集合都是一棵樹，並且稱作根的「子樹」 (subtree) 名詞介紹 degree : 一個 node 子節點的個數，e.g. degree(A) = 3, degree(C) = 1, degree(F) = 0 leaf (terminal) : degree = 0 的 node，e.g. leaf = {K, L, F, G, M, I, J} parent &amp; children : A 是 B 的 parent, B 是 A 的 children siblings : 具有相同 parent 的 nodes, e.g. B, C, D 是 siblings degree of tree : 所有 node 的 degree 中最大值代表這棵數的 degree，degree 為 k 的樹又稱為 k-ary tree ancestors : 一個 node 走回 root 所要經過的 nodes, e.g. M 的 ancestors 有 A, D, H level : root 的 level 為 0 或 1，children 的 level 為 root 的 level + 1，以此類推 height (depth) : 所有 node 中 level 的最大值代表這棵數的 height (depth) 表示方法 List Representation Left Child-Right Sibling Representation Representation as Degree-Two Tree (Left Child-Right Child) 二元樹 Binary Tree 以上介紹的tree並沒有限制subtree的數量，但我們一般較常用的還是接下來要介紹的二元樹 (Binary Tree) 。 二元樹有以下幾點特性： 1. 每個 node 的 degree 不超過 2 2. binary tree 可以不存在任何的節點 (empty) 3. 需要區分左子樹與右子樹，也就是左右子樹互換位置的話就會形成另一個新的樹 二元樹的表示方法 Array Representation 使用array來表示binary tree通常會具有以下的特性： * parent index = ⌊ current index / 2 ⌋ * leftChild index = current index * 2 * rightChild index = current index * 2 + 1 但是使用array表示，常常會造成空間上的浪費，如： 能夠最有效利用array空間的樹是complete binary tree，像是heap就非常適合用array進行實作。 Linked Representation 另一種表示方法是使用linked list實作，雖然每個node都需要額外的空間來儲存link，但是對於空間上的利用卻能夠進行比較有效的管控。 實作 以下使用python以Linked List實作Binary Tree。 首先定義樹的節點： 12345class TreeNode(): def __init__(self, data=None, left=None, right=None): self.data = data self.left = left self.right = right 接著是二元樹的類別以及基本方法： 12345678910111213141516class BinaryTree(): def __init__(self, root=None): self.root = root def isEmpty(self, node): return node is None def left_child(self, node): if self.isEmpty(node): return return node.left def right_child(self, node): if self.isEmpty(node): return return node.right 二元樹的遍歷 Traversal 若遍歷一棵樹具有以下三個步驟， * L: moving left * V: visiting the node * R: moving right 則遍歷的方法就有，L,V,R 三種步驟的排列數 = 3! = 6 種: LVR, LRV, VLR, VRL, RLV, RVL 。 取其中三種當作遍歷的方法： * preorder: VLR * inorder: LVR * postorder: LRV 記的方法很簡單，看 V 的位置就對了，若 V 的位置在前面就是preorder，在中間就是inorder，在後面就是postorder。 Inorder Traversal 12345def inorder(self, node): if node: self.inorder(node.left) print(node.data, end=" ") self.inorder(node.right) Preorder Traversal 12345def preorder(self, node): if node: print(node.data, end=" ") self.preorder(node.left) self.preorder(node.right) Postorder Traversal 12345def postorder(self, node): if node: self.postorder(node.left) self.postorder(node.right) print(node.data, end=" ") Level-Order Traversal 第四種遍歷的方法，依照node的level的順序依序拜訪各結點。 12345678910111213def levelorder(self, node): if not node: return from collections import deque queue = deque() queue.append(node) while queue: node = queue.popleft() print(node.data, end=" ") if node.left: queue.append(node.left) if node.right: queue.append(node.right) 其他操作 複製 12345# Return a Pointer to a same data from original nodedef copy(self, node): if node: return TreeNode(node.data, self.copy(node.left), self.copy(node.right)) return None 相等 1234@staticmethoddef equal(first, second): return (not first and not second) or (first and second and (first.data == second.data) \and BinaryTree.equal(first.left, second.left) and BinaryTree.equal(first.right, second.right)) (未完成，待補上...)]]></content>
      <categories>
        <category>學校課程</category>
        <category>資料結構</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Data Structure</tag>
        <tag>Tree</tag>
        <tag>Binary Tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[堆疊(Stack)與佇列(Queue)]]></title>
    <url>%2F2017%2F09%2F06%2F%E5%A0%86%E7%96%8A-Stack-%E8%88%87%E4%BD%87%E5%88%97-Queue%2F</url>
    <content type="text"><![CDATA[介紹 Stack 與 Queue 都是一種 抽象資料型別 (Abstract Data Type)，兩者的區別簡單來說就是Stack是 Last-In-First-Out (LIFO)，而 Queue 則是 First-In-First-Out (FIFO)。 實作 以下用 Linked List 來實作 Stack 與 Queue Stack 首先是 stack 的節點。 1234class StackNode(): def __init__(self, value=None, next=None): self.value = value self.next = None 接著定義函式，因為是後進先出，所以用 top 來紀錄最後進入的數，因此不管是 push 還是 pop 都是更動到 top 的值。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class Stack(): def __init__(self, top=None): self.top = top def print_nodes(self): current = self.top while current: end = " -&gt; " if current.next else "\n" print(current.value, end=end) current = current.next def push(self, value): old_top = self.top self.top = StackNode(value, old_top) self.top.next = old_top def pop(self): if self.isEmpty(): print("Pop nothing, the stack is empty!") return self.top = self.top.next def top_value(self): if self.isEmpty(): return "Nothing at the top, the stack is empty!" return self.top.value def search(self, value): count = 0 current = self.top while current: if current.value == value: return count count += 1 current = current.next def isEmpty(self): return self.top is None def size(self): count = 0 current = self.top while current: count += 1 current = current.next return count Queue 接著是 queue 的節點。 1234class QueueNode(): def __init__(self, value=None, next=None): self.value = value self.next = next 因為 queue 為先進先出，因此需要有兩個變數來紀錄頭跟尾的位置，可以將 queue 想像成排隊的時候，add 就是將新的節點接在隊伍的後方，delete 則是將節點從隊伍的頭移除。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class Queue(): def __init__(self, front=None, rear=None): self.front = front self.rear = rear def print_queue(self): current = self.front while current: end = " -&gt; " if current.next else "\n" print(current.value, end=end) current = current.next def add(self, value): if self.isEmpty(): self.front = QueueNode(value) self.rear = self.front else: self.rear.next = QueueNode(value) self.rear = self.rear.next def delete(self): if self.isEmpty(): print("Delete nothing, the queue is empty.") else: self.front = self.front.next def search(self, value): count = 0 current = self.front while current: if current.value == value: return count count += 1 current = current.next def top_value(self): if self.isEmpty(): return "Nothing at the top, the stack is empty!" return self.front.value def end_value(self): if self.isEmpty(): return "Nothing at the end, the stack is empty!" return self.rear.value def isEmpty(self): return self.front is None 程式碼：Stack Queue]]></content>
      <categories>
        <category>學校課程</category>
        <category>資料結構</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Data Structure</tag>
        <tag>Stack</tag>
        <tag>Queue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode#167 Two Sum II - Input array is sorted feat. Two Pointers]]></title>
    <url>%2F2017%2F09%2F05%2FLeetCode-167-Two-Sum-II-Input-array-is-sorted-feat-Two-Pointers%2F</url>
    <content type="text"><![CDATA[問題 給定一個遞增的整數陣列nums(已排序)，其中有兩個數的和會等於目標target，回傳他們的位置，且同個位置不能重複選取。 輸入 numbers = [2, 7, 11, 15] target = 9 輸出 [1, 2] 方法 這題是LeetCode#1 Two Sums的改版，差別是這題輸入的陣列已經經過排序了，這題還是可以用hash table來解，不過這裡我們要用比較不一樣的方法，叫作two pointers 。 two pointers在這裡的使用方法，就是兩個箭頭分別指向陣列的頭跟尾，藉由將左邊的箭頭向右調整以及右邊的箭頭向左調整，逼近出所要求的值。 12345678910class Solution(object): def twoSum(self, numbers, target): left, right = 0, len(numbers) - 1 while left &lt; right: if numbers[left] + numbers[right] &lt; target: left += 1 elif numbers[left] + numbers[right] &gt; target: right -= 1 else: return [left + 1, right + 1] 以下的問題也常常用two pointers來解： 1. 字串、陣列反轉 2. 兩個跑者一快一慢在操場上奔馳，問兩人何時相遇 3. 字串回文(Palindrome)]]></content>
      <categories>
        <category>程式解題</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>Python</tag>
        <tag>Two Pointers</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[挑戰LeetCode Weekly Contest 48]]></title>
    <url>%2F2017%2F09%2F04%2F%E6%8C%91%E6%88%B0LeetCode-Weekly-Contest-48%2F</url>
    <content type="text"><![CDATA[再度挑戰 這次的題目稍稍比上次簡單一點，前兩題解完大概還剩 20 分鐘，看看第三題好像也不難，但可惜還是沒寫完，最終成績 1170 / 2668，嗯...好像沒進步 Orz 。 第1題 671. Second Minimum Node In a Binary Tree 問題 給一棵二元樹，每一個node的子節點不是 0 個就是 2 個，輸出所有節點當中第二小的值，若不存在則輸出-1。 輸入 tree 的 root 範例： 2 / \ 2 5 / \ 5 7 輸出 5 方法 遍歷所有的節點，不斷刷新最小值(下界)與第二小的值(上界)，夾擠出真正第二小的數值。 {% codeblock lang:python %} class Solution(object): def findSecondMinimumValue(self, root): if not root or not root.left: return -1 self.smallest = root.val self.second = max(root.left.val, root.right.val) self.find(root) if self.smallest == self.second: return -1 return self.second def find(self, root): if root.left and root.right: if root.left.val < self.smallest: self.smallest = root.left.val if root.right.val < self.smallest: self.smallest = root.right.val if root.left.val > self.smallest and root.left.val self.smallest and root.right.val]]></content>
      <categories>
        <category>程式解題</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>Python</tag>
        <tag>LeetCode Weekly Contest</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[連結串列(Linked List)]]></title>
    <url>%2F2017%2F09%2F02%2F%E9%80%A3%E7%B5%90%E4%B8%B2%E5%88%97-Linked-List%2F</url>
    <content type="text"><![CDATA[連結串列一般指的是單向連結串列(Single Linked List)，由node所組成，每個node都具有兩種屬性，分別是「資料」以及「指標」。資料是儲存目前這個節點的值，指標是指向下一個節點的連結，至於最後一個結點則會指向null。 連結串列比起一般的陣列，優點是能夠隨著需求動態配置記憶體，插入或移除元素時也相對方便；缺點是取出元素時無法直接指定位置，需要遍歷整個串列。 以下用python來實作Linked List。 首先我們要實作node，宣告一個class名稱ListNode，每個node都需要兩個元素，資料value與指向下一個node的指標next。 1234class ListNode(object): def __init__(self, value=None, next=None): self.val = value self.next = next 我們還要宣告一個名為LinkedList的類別，用來紀錄串列的起始位置。 123class LinkedList(object): def __init__(self, head=None): self.head = head 接著在LinkedList實作下面的方法： print_nodes() at(index) append(value) insert(index, value) removePos(index) remove(value, all=False) indexOf(value) clear() isEmpty() size() 首先是print_nodes方法，走訪所有的節點並印出每個結點的資料。 12345678def print_nodes(self): if not self.head: print(self.head) node = self.head while node: end = " -&gt; " if node.next else "\n" print(node.val, end=end) node = node.next 接著是at方法，回傳index位置的value。 12345678def at(self, index): count = 0 node = self.head while node: if count == index: return node.val count += 1 node = node.next 第三個是append，將值為value的節點接在陣列的最尾端。 12345678def append(self, value): if not self.head: self.head = ListNode(value) return node = self.head while node.next: node = node.next node.next = ListNode(value) 第四個是insert，將value插入index的位置。 123456789101112131415161718def insert(self, index, value): if index &gt;= self.size(): self.append(value) return count = 0 node = self.head previous = None while node: if count == index: if previous: new_node = ListNode(value, previous.next) previous.next = new_node else: self.head = ListNode(value, node) return count += 1 previous = node node = node.next 第五個與第六個是removePos以及remove，分別是移除位置為index的節點，以及移除值為value的節點。 12345678910111213141516def removePos(self, index): count = 0 node = self.head previous = None while node: if count == index: if previous: previous.next = node.next node = node.next else: self.head = node.next node = self.head return else: previous = node node = node.next 12345678910111213141516def remove(self, val, all=False): node = self.head previous = None while node: if node.val == val: if previous: previous.next = node.next node = node.next else: self.head = node.next node = self.head if not all: return else: previous = node node = node.next 第七個是indexOf，回傳第一個出現的value在串列中的的位置。 12345678def indexOf(self, value): node = self.head count = 0 while node: if node.val == value: return count count += 1 node = node.next 最後三個是clear, isEmpty, 以及size。 12def clear(self): self.head = None 12def isEmpty(self): return self.head is None 1234567def size(self): count = 0 node = self.head while node: count += 1 node = node.next return count 程式碼連結]]></content>
      <categories>
        <category>學校課程</category>
        <category>資料結構</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Linked List</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode#78 Subsets]]></title>
    <url>%2F2017%2F08%2F31%2FLeetCode-78-Subsets%2F</url>
    <content type="text"><![CDATA[問題 給一串元素相異的數列，求由此數列之元素所組成的所有子集合。 輸入 [1,2,3] 輸出 [ [3], [1], [2], [1,2,3], [1,3], [2,3], [1,2], [] ] 方法 每個元素可以出現或是不出現，每一種組合方法都是一個子集合，所以時間複雜度是O(2n) 。 利用Backtracking遞迴枚舉所有可能。 1234567891011121314class Solution(object): def subsets(self, nums): self.length = len(nums) self.ans = [] self.backtrack(nums, []) return(self.ans) def backtrack(self, nums, n): if len(n) == self.length: a = [nums[i] for i, v in enumerate(n) if v] self.ans.append(a) else: self.backtrack(nums, n + [True]) self.backtrack(nums, n + [False]) 其他解法以及類似問題請看這裡。]]></content>
      <categories>
        <category>程式解題</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>Python</tag>
        <tag>Backtracking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode#442 Find All Duplicates in an Array]]></title>
    <url>%2F2017%2F08%2F30%2FLeetCode-442-Find-All-Duplicates-in-an-Array%2F</url>
    <content type="text"><![CDATA[問題 給一個長度為 n 的整數陣列，裡面的元素由 1~n 所組成，每個數字出現 0~2 次，找出所有出現 2 次的數字。(不使用額外空間且時間複雜度必須為O(n)) 輸入 [4, 3, 2, 7, 8, 2, 3, 1] 輸出 [2, 3] 方法 負號標記法 將陣列nums的值遍歷一次，將每個出現過的值當作index，將對應到的num[index]的值乘上-1，若對應到的值已經為負數的話，代表已經出現過 1 次，目前是第 2 次，這樣就能找出所有出現過 2 次的元素了。 123456789class Solution(object): def findDuplicates(self, nums): ans = [] for i in nums: if nums[abs(i) - 1] &lt; 0: ans.append(abs(i)) else: nums[abs(i) - 1] *= -1 return ans #448 Find All Numbers Disappeared in an Array，也可以用同樣的方法來解。]]></content>
      <categories>
        <category>程式解題</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>Python</tag>
        <tag>Array</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[挑戰LeetCode Weekly Contest 47]]></title>
    <url>%2F2017%2F08%2F28%2F%E6%8C%91%E6%88%B0LeetCode-Weekly-Contest-47%2F</url>
    <content type="text"><![CDATA[前言 這次心血來潮報名了 LeetCode 每周都會舉辦的比賽，規則是在1.5小時之內要解出4題，這對我來說是一大挑戰啊！因為以往參加的比賽，時間都是3小時左右，這麼短的時間內要解出4題真的不容易，不過既然都報名了就全力以赴吧！ 第1題 665. Non-decreasing Array 問題 給定一個長度為 n 的整數陣列，問是否能在最多更改 1 個數的情況下，將陣列變成 non-decreasing array，也就是遞增的陣列。 輸入 [4, 2, 3] 輸出 True 方法 我的策略是分別從頭尾出發走一次，若遇到下一個數 pj 小於/大於目前這個數 pi 的話，就將下一個數的值改為目前這個數，使得目前為止的元素為遞增的狀態，此時修改的次數就會增加一次。若分別從頭尾走，修改的次數都大於一次的話，代表無法符合題目的條件，回傳False，反之回傳True。 那為什麼要分別從頭尾走一次呢？ 以範例的輸入當例子，若只從正方向走， (4, 2) 不符合遞增，將 2 改成 4，陣列變成 [4, 4, 3]，接著遇到 (4, 3) 又不符合遞增，因此只修改一個值並不能使原本的陣列變成遞增數列。 但是錯啦！其實只要把第一個數 4 改成 1 或 2 就可以了，想到從反方向走的話好像可以達到這個效果，所以必須要檢查兩個方向才能確保沒有漏網之魚。 1234567891011121314151617181920class Solution: def checkPossibility(self, nums): n = list(nums) edit_time_1 = 0 for i in range(len(nums) - 1): if nums[i] &gt; nums[i + 1]: nums[i + 1] = nums[i] edit_time_1 += 1 nums = n edit_time_2 = 0 for i in range(len(nums) - 1, 0, -1): if nums[i] &lt; nums[i - 1]: nums[i - 1] = nums[i] edit_time_2 += 1 if edit_time_1 &lt;= 1 or edit_time_2 &lt;= 1: return True else: return False 第2題 666. Path Sum IV 問題 輸入為一個 Depth &lt; 5 的 tree，每個 node 可用三位數來表示。 百位數D代表這個 node 所在的深度，1 &lt;= D &lt;= 4 。 十位數P代表這個 node 在這個深度的第幾個位置, 1 &lt;= P &lt;= 8 。 個位數V代表這個 node 的值, 0 &lt;= V &lt;= 9 。 找出所有從 root 到 leaf 的路徑和，也就是把路徑上的值相加。 輸入 [113, 215, 221] 輸出 12 方法 以下是範例輸入的解釋： The tree that the list represents is: 3 / \ 5 1 The path sum is (3 + 5) + (3 + 1) = 12. 先將輸入的點分別對應 1 ~ 15 的值(也就是用陣列表示 tree, 但這裡直接用dictionary比較方便)，接著找出所有的 leaf ，再將每個 leaf 到 root 的路徑上所經過的點全部加起來即可。 1234567891011class Solution: def pathSum(self, nums): order = ['00', '11', '21', '22', '31', '32', '33', '34', '41', '42', '43', '44', '45', '46', '47', '48'] tree = dict([(order.index(str(n)[:2]), n % 10) for n in nums]) summation = 0 leaves = [n for n in tree.keys() if not (2 * n in tree or 2 * n + 1 in tree)] for leaf in leaves: while leaf &gt;= 1: summation += tree[leaf] leaf = int(leaf / 2) return summation 第3題 667. Beautiful Arrangement II 問題 輸入整數n與k，找到一個由 1 ~ n 組成的陣列[a1, a2, ..., an]，陣列元素不得重複， 使得 [|a1-a2|, |a2-a3|, ..., |an-1-an|] 剛好由 k 種不同的數組成。 輸入 n = 3, k = 1 輸出 [1, 2, 3] 方法 這題推導了很久，想到了一個方法，[1, 2, ..., n] 的差有 k 種值，1 &lt;= k &lt;= n - 1，只要建立一個規則產生所有的差，接著再想辦法湊出這個數列，舉例： output: 5 1 4 3 2 diff: 4 3 1 1 假設要使得[1, 2, 3, 4, 5]鄰近的差只有三種，那就先將兩種差設為 n-1 與 n-2，也就是 4 跟 3，剩下的差都填 1 代表第三種差。至於要產生的陣列，先填入最大的數 5，接著根據每個差依序湊出陣列接下來的值，最後就會得到滿足條件的陣列了。 1234567891011121314class Solution: def constructArray(self, n, k): diff = [] for i in range(k - 1): diff.append(n - i - 1) for i in range(n - k): diff.append(1) nums = [n] for i in range(n - 1): new_value = abs(nums[i] + diff[i]) if new_value &gt; n or new_value in nums: new_value = abs(nums[i] - diff[i]) nums.append(new_value) return nums 第4題 668. Kth Smallest Number in Multiplication Table 問題 找出在一個 m * n 的乘法表中，第 k 小的數字。 輸入 m = 3, n = 3, k = 5 輸出 3 方法 以 3 * 3 的乘法表舉例： The Multiplication Table: 1 2 3 2 4 6 3 6 9 The 5-th smallest number is 3 (1, 2, 2, 3, 3). 這題我試了很多方法，不管怎麼樣總是不夠周延，最後看了別人的解法，使用 binary search ，實在太厲害了完全想不到。 看懂了之後寫了個 python 的版本，方法簡單來說就是利用二分搜尋法，找出中間值 mid ，接著計算每一列小於等於 mid 的個數的和，若總數小於 k ，代表值太小了還要再多一點；若總數大於 k ，代表值太大了要小一點，迴圈重複直到 low &lt; high 的時候代表找到了。 1234567891011121314151617class Solution: def findKthNumber(self, m, n, k): low, high = 1, m * n + 1 while low &lt; high: mid = (low + high) // 2 c = self.count_less_than_middle(mid, m, n) if c &gt;= k: high = mid else: low = mid + 1 return high def count_less_than_middle(self, middle, m, n): num = 0 for i in range(1, m + 1): num += min(middle // i, n) return num 結果 最後在時間內只有解出一題，排名只有 1157 / 2554，剩下的都是比賽結束後才想出來QQ，太慘烈了。看來在下做的題目還不夠多，看到題目沒辦法馬上有 sense 要用什麼方式解，只好先來閉關修煉一下了QQ 。]]></content>
      <categories>
        <category>程式解題</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>Python</tag>
        <tag>LeetCode Weekly Contest</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode#566 Reshape the Matrix]]></title>
    <url>%2F2017%2F08%2F27%2FLeetCode-566-Reshape-the-Matrix%2F</url>
    <content type="text"><![CDATA[問題 給定一個二維的矩陣以及正整數 r 與 c ，將此矩陣轉為 r * c 的矩陣。 若無法轉換，則輸出原本的矩陣。 ## 輸入 ## nums = [[1,2], [3,4]] r = 1, c = 4 ## 輸出 ## [[1,2,3,4]] ## 方法 ## 檢查兩個矩陣的行數與列數相乘的結果是否相同，若成立代表這兩個矩陣的元素個數相同，也就是兩個矩陣能夠互相轉換。 接著將原本 2D 的矩陣轉成 1D ，這樣在轉換成另一個大小的矩陣時會比較方便，只要依序取出即可。 1234567891011121314class Solution(object): def matrixReshape(self, nums, r, c): row = len(nums) col = len(nums[0]) if row * col == r * c: one_d_array = [nums[i][j] for i in range(row) for j in range(col)] ans = [[0 for j in range(c)] for i in range(r)] count_element = 0 for i in range(r): for j in range(c): ans[i][j] = one_d_array[count_element] count_element += 1 return ans return nums]]></content>
      <categories>
        <category>程式解題</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>Python</tag>
        <tag>Matrix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode#561 Array Partition I]]></title>
    <url>%2F2017%2F08%2F26%2FLeetCode-561-Array-Partition-I%2F</url>
    <content type="text"><![CDATA[問題 給一個長度為 2n 的整數陣列，將陣列的值兩兩配對，使得所有 min(ai, bi) 的和愈大愈好 (0 &lt;= i &lt;= n) 。 輸入 [1,4,3,2] 輸出 4 方法 這題直覺就是先將陣列排序之後再取奇數項相加就好。 sort: [1,2,3,4] min(1, 2) + min(3, 4) = 4 1234567class Solution(object): def arrayPairSum(self, nums): summation = 0 for i, v in enumerate(sorted(nums)): if i % 2 == 0: summation += v return summation 在底下討論版找到了一行解決的 code ，完全體現 python 簡約的風格，太神啦! 123class Solution(object): def arrayPairSum(self, nums): return sum(sorted(nums)[::2])]]></content>
      <categories>
        <category>程式解題</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>Python</tag>
        <tag>Sorting</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode#461 Hamming Distance]]></title>
    <url>%2F2017%2F08%2F26%2FLeetCode-461-Hamming-Distance%2F</url>
    <content type="text"><![CDATA[問題 給兩個整數x與y，計算兩者的 Hamming Distance 。 ## 輸入 ## x = 1, y = 4 ## 輸出 ## 2 ## 方法 ## 所謂的 Hamming Distance 就是兩個string中有幾個相異的位元個數，舉個例子： 1 = (0 0 0 1) 4 = (0 1 0 0) ↑ ↑ 1 跟 4 有兩個相異的位元個數，因此 1 跟 4 的 Hamming Distance 為 2 。 我們可以很直覺的想到可以用 exclusive or 簡單的計算出來，也就是兩個 bit 相同時輸出 0 ，兩個 bit 相異時輸出 1 ，最後再計算總共有幾位數為 1 即可。 123class Solution(object): def hammingDistance(self, x, y): return bin(x^y).count("1")]]></content>
      <categories>
        <category>程式解題</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>Python</tag>
        <tag>Hamming Distance</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode#1 Two Sums]]></title>
    <url>%2F2017%2F08%2F26%2FLeetCode-1-Two-Sums%2F</url>
    <content type="text"><![CDATA[問題 給定一個整數的陣列nums，其中有兩個數的和會等於目標target，回傳他們的位置，且同個位置不能重複選取。 輸入 nums = [2, 7, 11, 15] target = 9 輸出 [0, 1] 方法 Brute Force Hash Table 第一種方法就是直接使用暴力法，把陣列裡面的元素都加加看，但是時間複雜度會到O(n2)。 123456class Solution(object): def twoSum(self, nums, target): for i in range(len(nums)): for j in range(i + 1, len(nums)): if nums[i] + nums[j] == target: return [i, j] 居然吃了一發TLE，沒想到第一題就玩真的，只好另尋他法了。 這時想到python中相當好用的dictionary，第二種方法就是利用雜湊表將讀到的 index 及 value 紀錄下來，即可在讀入一個新的數之後查看這個數的 complement 是否在 table 當中，這樣就可以完美找到一組解啦。 因為使用了dictionary，所以查表的時間只要O(1)，最後時間複雜度降到了O(n)，當然也就順利通過了！ 12345678class Solution(object): def twoSum(self, nums, target): dic = &#123;&#125; for i, n in enumerate(nums): if target - n in dic: return [dic[target - n], i] else: dic[n] = i]]></content>
      <categories>
        <category>程式解題</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>Hash</tag>
        <tag>Python</tag>
      </tags>
  </entry>
</search>
