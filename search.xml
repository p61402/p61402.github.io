<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[CS231n assignment 2]]></title>
    <url>%2F2019%2F05%2F04%2FCS231n-assignment-2%2F</url>
    <content type="text"><![CDATA[ç°¡ä»‹ é€™æ¬¡çš„ä½œæ¥­ç›¸è¼ƒæ–¼ç¬¬ä¸€æ¬¡ä½œæ¥­åˆæ›´æ·±å…¥äº†ä¸€äº›ï¼Œé€™æ¬¡ä½œæ¥­è¦ä¾åºå¯¦ä½œ Fully Connected Networkã€Batch Normalizationã€Dropoutã€Convolutional Neural Network ç­‰æ–¹æ³•ï¼Œä¸¦å°‡è£¡é ­çš„æ­¥é©Ÿæ¨¡çµ„åŒ–ã€‚ Fully Connected Network Modular network é¦–å…ˆè¦å°‡ FCN çš„ forward pass ä»¥åŠ backward æ¨¡çµ„åŒ–ï¼ŒåŒ…æ‹¬ affine layer èˆ‡ ReLU activationã€‚ åœ¨ forward pass æ™‚ä½¿ç”¨cacheå°‡æ‰€éœ€è®Šæ•¸å„²å­˜ï¼Œä»¥ä¾¿ backward pass çš„æ™‚å€™ä½¿ç”¨ã€‚ 12345def affine_forward(x, w, b): out = None out = x.reshape(x.shape[0], -1).dot(w) + b cache = (x, w, b) return out, cache 123456789def affine_backward(dout, cache): x, w, b = cache dx, dw, db = None, None, None dx = dout.dot(w.T).reshape(x.shape) dw = x.reshape(x.shape[0], -1).T.dot(dout) db = np.sum(dout, axis=0) return dx, dw, db 12345def relu_forward(x): out = None out = np.maximum(x, 0) cache = x return out, cache 1234def relu_backward(dout, cache): dx, x = None, cache dx = dout * (x &gt; 0) return dx åˆ©ç”¨å‰›å‰›å®Œæˆçš„æ¨¡çµ„ä¾†å»ºæ§‹TwoLayerNetä»¥åŠå¯ä»¥è‡ªè¨‚ size çš„FullyConnectedNetã€‚ 1234567891011121314151617181920212223242526272829303132333435class TwoLayerNet(object): def __init__(self, input_dim=3*32*32, hidden_dim=100, num_classes=10, weight_scale=1e-3, reg=0.0): self.params = &#123;&#125; self.reg = reg self.params['W1'] = np.random.normal(0, weight_scale, (input_dim, hidden_dim)) self.params['b1'] = np.zeros((hidden_dim, )) self.params['W2'] = np.random.normal(0, weight_scale, (hidden_dim, num_classes)) self.params['b2'] = np.zeros((num_classes, )) def loss(self, X, y=None): scores = None h1, cache_h1 = affine_relu_forward(X, self.params['W1'], self.params['b1']) scores, cache_scores = affine_forward(h1, self.params['W2'], self.params['b2']) # If y is None then we are in test mode so just return scores if y is None: return scores loss, grads = 0, &#123;&#125; loss, dS = softmax_loss(scores, y) dh1, grads['W2'], grads['b2'] = affine_backward(dS, cache_scores) dx, grads['W1'], grads['b1'] = affine_relu_backward(dh1, cache_h1) loss += 0.5 * self.reg * (np.sum(self.params['W2'] ** 2) + np.sum(self.params['W1'] ** 2)) grads['W1'] += self.reg * self.params['W1'] grads['W2'] += self.reg * self.params['W2'] return loss, grads 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596class FullyConnectedNet(object): def __init__(self, hidden_dims, input_dim=3*32*32, num_classes=10, dropout=1, normalization=None, reg=0.0, weight_scale=1e-2, dtype=np.float32, seed=None): self.normalization = normalization self.use_dropout = dropout != 1 self.reg = reg self.num_layers = 1 + len(hidden_dims) self.dtype = dtype self.params = &#123;&#125; self.params['W1'] = np.random.normal(0, weight_scale, (input_dim, hidden_dims[0])) self.params['b1'] = np.zeros((hidden_dims[0], )) for i in range(1, self.num_layers - 1): self.params['W'+str(i+1)] = np.random.normal(0, weight_scale, (hidden_dims[i-1], hidden_dims[i])) self.params['b'+str(i+1)] = np.zeros((hidden_dims[i], )) self.params['W'+str(self.num_layers)] = np.random.normal(0, weight_scale, (hidden_dims[-1], num_classes)) self.params['b'+str(self.num_layers)] = np.zeros((num_classes, )) # When using dropout we need to pass a dropout_param dictionary to each # dropout layer so that the layer knows the dropout probability and the mode # (train / test). You can pass the same dropout_param to each dropout layer. self.dropout_param = &#123;&#125; if self.use_dropout: self.dropout_param = &#123;'mode': 'train', 'p': dropout&#125; if seed is not None: self.dropout_param['seed'] = seed # With batch normalization we need to keep track of running means and # variances, so we need to pass a special bn_param object to each batch # normalization layer. You should pass self.bn_params[0] to the forward pass # of the first batch normalization layer, self.bn_params[1] to the forward # pass of the second batch normalization layer, etc. self.bn_params = [] if self.normalization=='batchnorm': self.bn_params = [&#123;'mode': 'train'&#125; for i in range(self.num_layers - 1)] if self.normalization=='layernorm': self.bn_params = [&#123;&#125; for i in range(self.num_layers - 1)] # Cast all parameters to the correct datatype for k, v in self.params.items(): self.params[k] = v.astype(dtype) def loss(self, X, y=None): X = X.astype(self.dtype) mode = 'test' if y is None else 'train' # Set train/test mode for batchnorm params and dropout param since they # behave differently during training and testing. if self.use_dropout: self.dropout_param['mode'] = mode if self.normalization=='batchnorm': for bn_param in self.bn_params: bn_param['mode'] = mode scores = None h, cache = [None] * (self.num_layers + 1), [None] * (self.num_layers + 1) h[0] = X for i in range(self.num_layers - 1): W, b = self.params['W'+str(i+1)], self.params['b'+str(i+1)] h[i+1], cache[i+1] = affine_relu_forward(h[i], W, b) else: i += 1 W, b = self.params['W'+str(i+1)], self.params['b'+str(i+1)] h[i+1], cache[i+1] = affine_forward(h[i], W, b) scores = h[-1] # If test mode return early if mode == 'test': return scores loss, grads = 0.0, &#123;&#125; loss, dS = softmax_loss(scores, y) dh = [None] * (self.num_layers + 1) dh[-1] = dS i = self.num_layers dh[i-1], grads['W'+str(i)], grads['b'+str(i)] = affine_backward(dh[i], cache[i]) loss += 0.5 * self.reg * np.sum(self.params['W'+str(i)] ** 2) grads['W'+str(i)] += 0.5 * self.reg * (self.params['W'+str(i)] ** 2) i -= 1 while i &gt; 0: dh[i-1], grads['W'+str(i)], grads['b'+str(i)] = affine_relu_backward(dh[i], cache[i]) loss += 0.5 * self.reg * np.sum(self.params['W'+str(i)] ** 2) grads['W'+str(i)] += self.reg * self.params['W'+str(i)] i -= 1 return loss, grads SGD + Momentum åŸå§‹çš„ Stochastic Gradient Descentï¼š \(x_{t+1}=x_t-\alpha\nabla f(x_t)\) SGD + Momumtumï¼š \(v_{t+1}=\rho v_t-\alpha\nabla f(x_t) \\ x_{t+1}=x_t+v_{t+1}\) \(v\) ä»£è¡¨ç›®å‰çš„æ–¹å‘é€Ÿåº¦ï¼Œåˆå§‹å€¼ç‚º 0ï¼Œå¦‚æœè² æ¢¯åº¦èˆ‡ç›®å‰æ–¹å‘ç›¸åŒï¼Œå‰‡é€Ÿåº¦æœƒè¶Šä¾†è¶Šå¿«ï¼Œåƒæ•¸çš„æ›´æ–°å¹…åº¦å°±æœƒè®Šå¤§ï¼›åä¹‹å‰‡è¶Šä¾†è¶Šæ…¢ï¼Œåƒæ•¸çš„æ›´æ–°å¹…åº¦æœƒè®Šå°ã€‚ è‡³æ–¼ \(\rho\) å‰‡æ˜¯ä¸€å€‹ hyperparameterï¼Œé€šå¸¸è¨­åœ¨ 0.9 å·¦å³ã€‚ ä½¿ç”¨ SGD + Momentum é€šå¸¸æ¯” Vanilla SGD èƒ½å¤ æ›´å¿«æ”¶æ–‚ã€‚ 1234567891011121314def sgd_momentum(w, dw, config=None): if config is None: config = &#123;&#125; config.setdefault('learning_rate', 1e-2) config.setdefault('momentum', 0.9) v = config.get('velocity', np.zeros_like(w)) next_w = None v = config['momentum'] * v - config['learning_rate'] * dw next_w = w + v config['velocity'] = v return next_w, config RMSProp \(v_t=\rho v_{t-1}+(1-\rho) \times (\nabla f(x_t))^2\) \(\Delta x_t=-\dfrac{\alpha}{\sqrt{v_t+\epsilon}} \times \nabla f(x_t)\) \(x_{t+1}=x_t+\Delta x_t\) \(\rho\) ç‚º decay rateï¼Œé€šå¸¸è¨­ç‚º 0.9ã€0.99ã€0.999ã€‚ \(\epsilon\) æ˜¯ä¸€å€‹å¾ˆå°çš„å€¼ï¼Œç‚ºäº†é¿å…é™¤ä»¥ 0 çš„æƒ…æ³ç”¢ç”Ÿã€‚ 12345678910111213def rmsprop(w, dw, config=None): if config is None: config = &#123;&#125; config.setdefault('learning_rate', 1e-2) config.setdefault('decay_rate', 0.99) config.setdefault('epsilon', 1e-8) config.setdefault('cache', np.zeros_like(w)) next_w = None config['cache'] = config['decay_rate'] * config['cache'] + (1 - config['decay_rate']) * (dw ** 2) next_w = w + -config['learning_rate'] * dw / np.sqrt(config['cache'] + config['epsilon']) return next_w, config Adam 1234567891011121314151617181920def adam(w, dw, config=None): if config is None: config = &#123;&#125; config.setdefault('learning_rate', 1e-3) config.setdefault('beta1', 0.9) config.setdefault('beta2', 0.999) config.setdefault('epsilon', 1e-8) config.setdefault('m', np.zeros_like(w)) config.setdefault('v', np.zeros_like(w)) config.setdefault('t', 0) next_w = None config['t'] += 1 config['m'] = config['beta1'] * config['m'] + (1 - config['beta1']) * dw mt = config['m'] / (1 - config['beta1'] ** config['t']) config['v'] = config['beta2'] * config['v'] + (1 - config['beta2']) * (dw ** 2) vt = config['v'] / (1 - config['beta2'] ** config['t']) next_w = w + -config['learning_rate'] * mt / (np.sqrt(vt) + config['epsilon']) return next_w, config æ¯”è¼ƒä¸åŒ optimizer çš„è¡¨ç¾ï¼š Optimizer Comparison Batch Normalization å¯¦ä½œéç¨‹åƒè€ƒä»¥ä¸‹è«–æ–‡ï¼š Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift batch normalization çš„ç›®çš„æ˜¯ç‚ºäº†è®“æ¯ä¸€å±¤çš„è¼¸å‡ºéƒ½ä¿æŒé«˜æ–¯åˆ†ä½ˆï¼Œä¸»è¦çš„ç›®çš„æ˜¯ç‚ºäº†é¿å… gradient vanishingã€‚åšæ³•æ˜¯å°‡ fordward pass æ™‚ç”¨ä¾†è¨“ç·´çš„æ‰¹æ¬¡è³‡æ–™è¨ˆç®— mean ä»¥åŠ varianceï¼Œåˆ©ç”¨ mini-batch çš„ mean åŠ vairance ä¾†æ›´æ–°æ•´é«”çš„ mean åŠ varianceã€‚ Forward Pass è«–æ–‡ä¸­å…·é«”çš„å¯¦ä½œæ–¹æ³•å¦‚ä¸‹ï¼š batch_normalization 12345678910111213141516171819202122232425262728293031323334def batchnorm_forward(x, gamma, beta, bn_param): mode = bn_param['mode'] eps = bn_param.get('eps', 1e-5) momentum = bn_param.get('momentum', 0.9) N, D = x.shape running_mean = bn_param.get('running_mean', np.zeros(D, dtype=x.dtype)) running_var = bn_param.get('running_var', np.zeros(D, dtype=x.dtype)) out, cache = None, None if mode == 'train': sample_mean = x.mean(axis=0) sample_var = x.var(axis=0) sqrtvar = np.sqrt(sample_var + eps) xmu = x - sample_mean ivar = 1./sqrtvar x_hat = xmu * ivar out = gamma * x_hat + beta cache = (xmu, sample_var, ivar, sqrtvar, x_hat, gamma, eps) running_mean = momentum * running_mean + (1 - momentum) * sample_mean running_var = momentum * running_var + (1 - momentum) * sample_var elif mode == 'test': x_hat = (x - running_mean) / np.sqrt(running_var + eps) out = gamma * x_hat + beta else: raise ValueError('Invalid forward batchnorm mode "%s"' % mode) # Store the updated running means back into bn_param bn_param['running_mean'] = running_mean bn_param['running_var'] = running_var return out, cache Backward Pass è«–æ–‡ä¸­å°æ–¼è¨ˆç®— BN çš„åå‘å‚³æ’­ä¹Ÿæœ‰ä¸€äº›æè¿°ï¼š BN backward çœŸçš„æ˜¯æ»¿è¤‡é›œçš„ï¼Œæœ€å¥½é‚„æ˜¯è‡ªå·±ç•«éä¸€æ¬¡è¨ˆç®—åœ–ä¹‹å¾Œå†è©¦è‘—å»è¨ˆç®— backward passï¼Œé€™éƒ¨åˆ†çš„è©±é€™ç¯‡æ–‡ç« å¯«å¾—æ»¿ä¸éŒ¯çš„ï¼Œå¯ä»¥åƒè€ƒä¸€ä¸‹ã€‚ è‡ªå·±ç•«çš„è¨ˆç®—åœ– 123456789101112131415161718192021222324252627def batchnorm_backward(dout, cache): dx, dgamma, dbeta = None, None, None N, D = dout.shape xmu, var, ivar, sqrtvar, x_hat, gamma, eps = cache dbeta = np.sum(dout, axis=0) dgamma_x = dout dgamma = np.sum(dgamma_x * x_hat, axis=0) dx_hat = dgamma_x * gamma divar = np.sum(dx_hat * xmu, axis=0) dx_mu1 = dx_hat * ivar dsqrtvar = -divar / (sqrtvar ** 2) dvar = 0.5 * dsqrtvar / np.sqrt(var + eps) dsq = dvar * np.ones((N, D)) / N dx_mu2 = 2 * xmu * dsq dx1 = dx_mu1 + dx_mu2 dmu = -np.sum(dx_mu1 + dx_mu2, axis=0) dx2 = dmu * np.ones((N, D)) / N dx = dx1 + dx2 return dx, dgamma, dbeta ç°¡åŒ–ç‰ˆï¼š 12345678910def batchnorm_backward_alt(dout, cache): dx, dgamma, dbeta = None, None, None xmu, var, ivar, sqrtvar, x_hat, gamma, eps = cache N, D = dout.shape dbeta = np.sum(dout, axis=0) dgamma = np.sum(x_hat * dout, axis=0) dx = (gamma * ivar / N) * (N * dout - x_hat * dgamma - dbeta) return dx, dgamma, dbeta Layer Normalization batch normalization ä½¿å¾—é¡ç¥ç¶“ç¶²è·¯çš„è¨“ç·´æ›´æœ‰æ•ˆç‡ï¼Œä½†æ˜¯å°æ–¼è¤‡é›œçš„ç¶²è·¯çµæ§‹ä¾†èªªï¼Œåœ¨ batch size ä¸å¤ å¤§çš„æ™‚å€™æ•ˆæœå¯èƒ½ä¸æœƒå¤ªå¥½ã€‚å› æ­¤å¦ä¸€å€‹æ–¹æ³•æ˜¯å° feature é€²è¡Œ normalizeï¼Œåƒè€ƒè«–æ–‡ï¼šLayer Normalizationã€‚ 12345678910111213def layernorm_forward(x, gamma, beta, ln_param): out, cache = None, None eps = ln_param.get('eps', 1e-5) x_T = x.T sample_mean = np.mean(x_T, axis=0) sample_var = np.var(x_T, axis=0) x_norm_T = (x_T - sample_mean) / np.sqrt(sample_var + eps) x_norm = x_norm_T.T out = x_norm * gamma + beta cache = (x, x_norm, gamma, sample_mean, sample_var, eps) return out, cache 1234567891011121314151617def layernorm_backward(dout, cache): dx, dgamma, dbeta = None, None, None x, x_norm, gamma, sample_mean, sample_var, eps = cache x_T = x.T dout_T = dout.T N = x_T.shape[0] dbeta = np.sum(dout, axis=0) dgamma = np.sum(x_norm * dout, axis=0) dx_norm = dout_T * gamma[:,np.newaxis] dv = ((x_T - sample_mean) * -0.5 * (sample_var + eps)**-1.5 * dx_norm).sum(axis=0) dm = (dx_norm * -1 * (sample_var + eps)**-0.5).sum(axis=0) + (dv * (x_T - sample_mean) * -2 / N).sum(axis=0) dx_T = dx_norm / (sample_var + eps)**0.5 + dv * 2 * (x_T - sample_mean) / N + dm / N dx = dx_T.T return dx, dgamma, dbeta Dropout Dropout: A Simple Way to Prevent Neural Networks from Overfitting drouput æ˜¯ä¸€ç¨®æ­£è¦åŒ–çš„æ–¹æ³•ï¼Œåœ¨ forward pass æ™‚éš¨æ©Ÿå°‡æŸäº› neuron çš„å€¼ä¸Ÿæ‰ï¼Œè·Ÿ L1, L2 regularization ä¸€æ¨£ï¼Œç›®çš„éƒ½æ˜¯ç‚ºäº†é¿å… overfittingã€‚ dropout å¯¦ä½œæ–¹æ³•æ˜¯åœ¨ training æ™‚æ ¹æ“šä¸€å€‹æ©Ÿç‡ p ä¾†éš¨æ©Ÿç”¢ç”Ÿä¸€å€‹ mask (å€¼ç‚º True or False)ï¼Œå°‡ x ä¹˜ä¸Š mask å°±å¯ä»¥å°‡éƒ¨åˆ† neuron çš„å€¼è¨­ç‚º 0ï¼Œ predicting çš„æ™‚å€™å°±ç›´æ¥å°‡ x ä¹˜ä¸Š pã€‚ ä½†èˆ‡å…¶åœ¨ predicting æ™‚ä¹˜ä¸Š pï¼Œå…¶å¯¦æˆ‘å€‘å¯ä»¥åœ¨ training çš„æ™‚å€™å°±é™¤ä»¥ pï¼Œé€™æ¨£å°±å¯ä»¥æ¸›å°‘ predicting çš„è¨ˆç®—é‡ï¼Œå› ç‚ºæˆ‘å€‘é€šå¸¸æ¯”è¼ƒåœ¨æ„ predicting æ™‚çš„æ•ˆç‡ï¼Œé€™å€‹æŠ€å·§ç¨±ç‚º inverted dropoutã€‚ 123456789101112131415161718def dropout_forward(x, dropout_param): p, mode = dropout_param['p'], dropout_param['mode'] if 'seed' in dropout_param: np.random.seed(dropout_param['seed']) mask = None out = None if mode == 'train': mask = (np.random.rand(*x.shape) &lt; p) / p out = x * mask elif mode == 'test': out = x cache = (dropout_param, mask) out = out.astype(x.dtype, copy=False) return out, cache 12345678910def dropout_backward(dout, cache): dropout_param, mask = cache mode = dropout_param['mode'] dx = None if mode == 'train': dx = dout * mask elif mode == 'test': dx = dout return dx Convolutional Neural Network Convolution Layer Forward Pass å¯¦ä½œ CNN çš„ forward passï¼Œè¼¸å…¥ \(x\) çš„å¤§å°ç‚º \((N,C,H,W)\)ï¼Œä»¥åŠ \(F\) å€‹ filterï¼Œåˆèµ·ä¾†æˆç‚ºä¸€å€‹ \((F,C,HH,WW)\) çš„çŸ©é™£ï¼Œç¶“é convolution çš„è¨ˆç®—å¾Œï¼Œè¼¸å‡ºä¸€å€‹ \((N,F,H^\prime,W^\prime)\) çš„çŸ©é™£ã€‚ 12345678910111213141516171819202122def conv_forward_naive(x, w, b, conv_param): out = None N, C, H, W = x.shape F, C, HH, WW = w.shape pad, stride = conv_param['pad'], conv_param['stride'] x_pad = np.pad(x, ((0,), (0,), (pad,), (pad,)), 'constant', constant_values=0) H_prime = 1 + (H + 2 * pad - HH) // stride W_prime = 1 + (W + 2 * pad - WW) // stride out = np.empty((N, F, H_prime, W_prime)) for f in range(F): for i in range(H_prime): for j in range(W_prime): out[:, f, i, j] = np.sum(x_pad[:, :, i*stride:i*stride+HH, j*stride:j*stride+WW] * w[f], axis=(1,2,3)) out += b.reshape(F, 1, 1) cache = (x, w, b, conv_param) return out, cache Convolution Layer Backward Pass è¨ˆç®— convolution layer çš„ backpropagation å¯ä»¥åƒè€ƒé€™ç¯‡æ–‡ç« ã€‚å› ç‚º forward æ™‚çš„è¨ˆç®—ä¹Ÿç®—æ˜¯ \(x\) ä¹˜ä¸Š \(w\)ï¼Œå› æ­¤ backward æ™‚è¨ˆç®— \(dx\) å°±æ˜¯ç”¨ \(dout\) èˆ‡ \(w\) åšè¨ˆç®—ï¼›è¨ˆç®— \(dw\) æ™‚å‰‡æ˜¯ç”¨ \(dout\) èˆ‡ \(x\) åšè¨ˆç®—ï¼Œé›–ç„¶æ¦‚å¿µä¸Šä¸é›£ç†è§£ï¼Œä½†æ˜¯è¦é€énumpyå¯¦ä½œçš„è©±å°ç¶­åº¦è¦æœ‰ä¸€å®šçš„æŒæ¡æ‰è¡Œã€‚ 12345678910111213141516171819202122232425262728def conv_backward_naive(dout, cache): dx, dw, db = None, None, None x, w, b, conv_param = cache N, C, H, W = x.shape F, C, HH, WW = w.shape pad, stride = conv_param['pad'], conv_param['stride'] x_pad = np.pad(x, ((0,), (0,), (pad,), (pad,)), mode='constant', constant_values=0) H_prime = 1 + (H + 2 * pad - HH) // stride W_prime = 1 + (W + 2 * pad - WW) // stride dx = np.zeros_like(x) dx_pad = np.zeros_like(x_pad) dw = np.zeros_like(w) db = np.sum(dout, axis=(0,2,3)) for i in range(H_prime): for j in range(W_prime): for f in range(F): dw[f] += np.sum(dout[:, f, i, j].reshape(-1,1,1,1) * x_pad[:, :, i*stride:i*stride+HH, j*stride:j*stride+WW], axis=0) for n in range(N): dx_pad[n, :, i*stride:i*stride+HH, j*stride:j*stride+WW] += np.sum(w * dout[n, :, i, j].reshape(-1,1,1,1), axis=0) dx = dx_pad[:, :, pad:-pad, pad:-pad] return dx, dw, db Max Pooling Forward Pass 12345678910111213141516def max_pool_forward_naive(x, pool_param): out = None N, C, H, W = x.shape pool_height, pool_width, stride = pool_param['pool_height'], pool_param['pool_width'], pool_param['stride'] H_prime = 1 + (H - pool_height) // stride W_prime = 1 + (W - pool_width) // stride out = np.empty((N, C, H_prime, W_prime)) for i in range(H_prime): for j in range(W_prime): out[:, :, i, j] = np.max(x[:, :, i*stride:i*stride+pool_height, j*stride:j*stride+pool_width], axis=(2,3)) cache = (x, pool_param) return out, cache Max Pooling Backward Pass 123456789101112131415161718def max_pool_backward_naive(dout, cache): dx = None x, pool_param = cache N, C, H, W = x.shape pool_height, pool_width, stride = pool_param['pool_height'], pool_param['pool_width'], pool_param['stride'] H_prime = 1 + (H - pool_height) // stride W_prime = 1 + (W - pool_width) // stride dx = np.zeros_like(x) for i in range(H_prime): for j in range(W_prime): arg = np.max(x[:, :, i*stride:i*stride+pool_height, j*stride:j*stride+pool_width], axis=(2,3), keepdims=True) == \ x[:, :, i*stride:i*stride+pool_height, j*stride:j*stride+pool_width] dx[:, :, i*stride:i*stride+pool_height, j*stride:j*stride+pool_width] += arg * dout[:, :, i, j][:,:,np.newaxis,np.newaxis] return dx æœ€å¾Œé‚„æœ‰å¯¦ä½œ Spatial Batch Normalization ä»¥åŠ Group Normalizationï¼Œä½†é€™éƒ¨åˆ†ä¸æ˜¯å¾ˆç†Ÿæ‰€ä»¥ç•¥éã€‚]]></content>
      <categories>
        <category>å­¸æ ¡èª²ç¨‹</category>
        <category>åœ–åƒè¾¨è­˜</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Computer Vision</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS231n assignment 1]]></title>
    <url>%2F2019%2F04%2F20%2FCS231n-assignment-1%2F</url>
    <content type="text"><![CDATA[ç°¡ä»‹ é€™æ¬¡ä½œæ¥­ä¸»è¦å¯¦ä½œä»¥ä¸‹æ¼”ç®—æ³•ï¼š k-Nearest Neighbor (kNN) Support Vector Machine (SVM) Softmax classifier Two-Layer Neural Network Higher Level Representations: Image Features k-Nearest Neighbor (kNN) åœ¨knn.ipynbä¸­å·²ç¶“å°‡è³‡æ–™è¼‰å…¥å®Œæˆï¼Œä½¿ç”¨ CIFAR-10 åœ–ç‰‡é›†ä¸­çš„ 5000 ç­†ç•¶ä½œè¨“ç·´ï¼Œ500 ç­†ç•¶ä½œæ¸¬è©¦ã€‚æ¯å¼µåœ–ç‰‡çš„å¤§å°éƒ½æ˜¯ (32, 32, 3)ï¼Œ3 ä»£è¡¨ RGB ä¸‰å€‹é€šé“ã€‚ cifar-10 è³‡æ–™é›†çš„10ç¨®é¡åˆ¥ æˆ‘å€‘è¦å¯¦ä½œä¸‰å€‹ç‰ˆæœ¬çš„ kNNï¼Œåˆ†åˆ¥æ˜¯ä½¿ç”¨é›™è¿´åœˆã€å–®è¿´åœˆã€ç„¡è¿´åœˆçš„ç‰ˆæœ¬ï¼Œå¯¦ä½œçš„ç¨‹å¼ç¢¼åœ¨cs231n/classifiers/k_nearest_neighbor.pyã€‚ compute_distances_two_loops é¦–å…ˆæ˜¯é›™è¿´åœˆçš„ç‰ˆæœ¬ï¼ŒXæ˜¯è¼¸å…¥çš„ test dataï¼Œå¤§å°ç‚º (num_test, D)ï¼Œè¼¸å‡ºdistsç‚ºä¸€å€‹å¤§å°ç‚º (num_test, num_train) çš„ numpy arrayï¼Œå…ƒç´ dists[i,j]ä»£è¡¨ç¬¬ i å€‹ test data point èˆ‡ç¬¬ j å€‹ train data point çš„æ­å¹¾é‡Œå¾—è·é›¢ã€‚ 123456789def compute_distances_two_loops(self, X): num_test = X.shape[0] num_train = self.X_train.shape[0] dists = np.zeros((num_test, num_train)) for i in range(num_test): for j in range(num_train): dists[i][j] = np.sqrt(np.sum((X[i] - self.X_train[j]) ** 2)) return dists predict_labels æ¥è‘—å¯¦ä½œå‡½å¼predict_labelsï¼Œå…ˆæ‰¾å‡ºå‰ k å€‹èˆ‡æ¸¬è©¦è³‡æ–™æœ€æ¥è¿‘çš„é»ï¼Œå†é€é majority vote çš„æ–¹å¼é¸å‡ºæœ€æœ‰å¯èƒ½çš„é¡åˆ¥ã€‚ 12345678910111213141516171819def predict_labels(self, dists, k=1): num_test = dists.shape[0] y_pred = np.zeros(num_test) for i in range(num_test): # A list of length k storing the labels of the k nearest neighbors to # the ith test point. closest_y = [] indices = np.argsort(dists[i])[:k] closest_y = self.y_train[indices] y_count = &#123;&#125; for y in closest_y: y_count[y] = y_count.get(y, 0) + 1 max_value = max(y_count.values()) candidates = [y for y, v in y_count.items() if v == max_value] y_pred[i] = min(candidates) return y_pred ä¸‹åŠéƒ¨å…¶å¯¦å¯ä»¥ç”¨ä¸€è¡Œç¨‹å¼ç¢¼è§£æ±ºï¼š 1y_pred[i] = np.bincount(closest_y).argmax() åˆ©ç”¨å‰›å‰›å¾—åˆ°çš„distsï¼Œå¯ä»¥è¨ˆç®—å‡º test data çš„é æ¸¬çµæœï¼Œå†å°‡é æ¸¬çµæœèˆ‡æ­£ç¢ºç­”æ¡ˆæ¯”è¼ƒå°±å¯ä»¥ç®—å‡ºæº–ç¢ºç‡ã€‚ k = 1 æ™‚çš„æº–ç¢ºç‡ï¼š 1Got 137 / 500 correct =&gt; accuracy: 0.274000 k = 5 æ™‚çš„æº–ç¢ºç‡ï¼š 1Got 139 / 500 correct =&gt; accuracy: 0.278000 compute_distances_one_loop æ¥è‘—å¯¦ä½œå–®è¿´åœˆç‰ˆæœ¬çš„ kNNï¼š 12345678def compute_distances_one_loop(self, X): num_test = X.shape[0] num_train = self.X_train.shape[0] dists = np.zeros((num_test, num_train)) for i in range(num_test): dists[i] = np.sqrt(np.sum((X[i] - self.X_train) ** 2, axis=1)) return dists compute_distances_no_loops ç„¡è¿´åœˆçš„æ¦‚å¿µå°±æ˜¯å°‡å…©é»é–“çš„è·é›¢ä»¥å¹³æ–¹å·®å…¬å¼å±•é–‹ï¼š\((x-y)^2=x^2+y^2-2xy\)ã€‚ 12345678def compute_distances_no_loops(self, X): num_test = X.shape[0] num_train = self.X_train.shape[0] dists = np.zeros((num_test, num_train)) dists += np.sqrt(np.sum(self.X_train ** 2, axis=1) + np.sum(X ** 2, axis=1)[:, np.newaxis] \ - 2 * np.dot(X, self.X_train.T)) return dists åŸ·è¡Œtime_functionå¯ä»¥è§€å¯Ÿåˆ°ç„¡è¿´åœˆçš„ç‰ˆæœ¬åŸ·è¡Œé€Ÿåº¦ï¼Œé é å°‡å…¶ä»–å…©å€‹ç‰ˆæœ¬ç”©åœ¨å¾Œé ­ã€‚ 123Two loop version took 29.963571 secondsOne loop version took 23.200013 secondsNo loop version took 0.140244 seconds Cross Validation (äº¤å‰é©—è­‰) å¯¦ä½œäº¤å‰é©—è­‰ä¾†é€²è¡Œ hyperparameter çš„æœç´¢ï¼Œè¦æ‰¾çš„è¶…åƒæ•¸ç‚º k å€¼ã€‚ 1234567891011121314151617181920212223num_folds = 5k_choices = [1, 3, 5, 8, 10, 12, 15, 20, 50, 100]X_train_folds = []y_train_folds = []X_train_folds = np.array_split(X_train, num_folds)y_train_folds = np.array_split(y_train, num_folds)k_to_accuracies = &#123;&#125;for k in k_choices: k_to_accuracies[k] = [] for n in range(num_folds): classifier.train(np.concatenate(X_train_folds[:n] + X_train_folds[n+1:]), np.concatenate(y_train_folds[:n] + y_train_folds[n+1:])) pred = classifier.predict(X_train_folds[n], k=k) num_correct = np.sum(pred == y_train_folds[n]) k_to_accuracies[k].append(float(num_correct) / len(pred))# Print out the computed accuraciesfor k in sorted(k_to_accuracies): for accuracy in k_to_accuracies[k]: print('k = %d, accuracy = %f' % (k, accuracy)) åŸ·è¡Œæ­¤æ®µç¨‹å¼ç¢¼æœƒè¼¸å‡ºæ¯å€‹ k å€¼åœ¨æ¯å€‹ fold çš„è¡¨ç¾ã€‚ 1234567891011121314k = 1, accuracy = 0.263000k = 1, accuracy = 0.257000k = 1, accuracy = 0.264000k = 1, accuracy = 0.278000k = 1, accuracy = 0.266000k = 3, accuracy = 0.239000k = 3, accuracy = 0.249000k = 3, accuracy = 0.240000...k = 100, accuracy = 0.256000k = 100, accuracy = 0.270000k = 100, accuracy = 0.263000k = 100, accuracy = 0.256000k = 100, accuracy = 0.263000 è¦–è¦ºåŒ– å°‡çµæœè¦–è¦ºåŒ–ï¼Œå¯ä»¥è§€å¯Ÿåˆ°åœ¨ k = 10 çš„æ™‚å€™æœƒåœ¨è¨“ç·´é›†å¾—åˆ°æœ€å¥½çš„æº–ç¢ºç‡ã€‚ cross validation result å°‡ k å€¼è¨­ç‚º 10 ä¹‹å¾Œï¼Œåœ¨æ¸¬è©¦é›†çš„æº–ç¢ºç‡ç¢ºå¯¦æœ‰æå‡ä¸€äº›ã€‚ 1Got 141 / 500 correct =&gt; accuracy: 0.282000 Support Vector Machine (SVM) svm_loss_naive é¦–å…ˆå¯¦ä½œè¿´åœˆç‰ˆæœ¬çš„ svm loss functionã€‚ è¼¸å…¥çš„è³‡æ–™ç¶­åº¦æ˜¯ Dï¼Œç¸½å…±æœ‰ C ç¨®é¡åˆ¥ï¼Œæ¯å€‹ minibatch æœ‰ N ç­†è³‡æ–™ã€‚ åƒæ•¸Wæ˜¯å¤§å°ç‚º (D,C) çš„æ¬Šé‡ï¼ŒXæ˜¯å¤§å°ç‚º (N,D) çš„ minibatch dataï¼Œyå¤§å°ç‚º (N,1) ä»£è¡¨ training labelsã€‚ SVM çš„ loss function å¦‚ä¸‹ï¼š \(L_i=\sum_{j\neq y_i}max(0,s_j-s_{y_i}+\Delta)\) Loss function çš„ gradient å¦‚ä¸‹ï¼š \(\nabla_{w_{y_i}}L_i=-(\sum_{j\neq y_i} 1(w_j^Tx_i-w_{y_i}^Tx_i+\Delta&gt;0))x_i\) \(\nabla_{w_j}L_i=1(w_j^Tx_i-w_{y_i}^Tx_i+\Delta&gt;0)x_i\) 1234567891011121314151617181920212223242526272829def svm_loss_naive(W, X, y, reg): dW = np.zeros(W.shape) # initialize the gradient as zero # compute the loss and the gradient num_classes = W.shape[1] num_train = X.shape[0] loss = 0.0 for i in range(num_train): scores = X[i].dot(W) correct_class_score = scores[y[i]] for j in range(num_classes): if j == y[i]: continue margin = scores[j] - correct_class_score + 1 # note delta = 1 if margin &gt; 0: loss += margin dW[:, j] += X[i] dW[:, y[i]] -= X[i] # Right now the loss is a sum over all training examples, but we want it # to be an average instead so we divide by num_train. loss /= num_train dW /= num_train # Add regularization to the loss. loss += reg * np.sum(W * W) dW += reg * W return loss, dW svm_loss_vectorized åˆ©ç”¨numpy vectorized çš„è¨ˆç®—æ–¹å¼æå‡é‹ç®—é€Ÿåº¦ã€‚ 12345678910111213141516171819202122def svm_loss_vectorized(W, X, y, reg): """ Structured SVM loss function, vectorized implementation. Inputs and outputs are the same as svm_loss_naive. """ loss = 0.0 dW = np.zeros(W.shape) # initialize the gradient as zero num_train = X.shape[0] scores = X.dot(W) correct_class_score = scores[np.arange(num_train), y].reshape(-1, 1) margin = scores - correct_class_score + 1 margin[np.arange(num_train), y] = 0 loss += margin[margin &gt; 0].sum() / num_train loss += reg * np.sum(W * W) counts = (margin &gt; 0).astype(int) counts[range(num_train), y] = - np.sum(counts, axis = 1) dW += np.dot(X.T, counts) / num_train + reg * W return loss, dW Stochastic Gradient Descent (SGD) cs331n/linear_classifier.py éš¨æ©Ÿæ¢¯åº¦ä¸‹é™ï¼Œæ¯æ¬¡æ›´æ–°Wæ™‚åªåˆ©ç”¨ä¸€éƒ¨åˆ†çš„è³‡æ–™ä¾†è¨ˆç®— loss åŠ gradientï¼Œèƒ½å¤ æ¸›å°‘é‹ç®—é‡ã€‚ 123456789101112131415161718192021222324252627def train(self, X, y, learning_rate=1e-3, reg=1e-5, num_iters=100, batch_size=200, verbose=False): num_train, dim = X.shape num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes if self.W is None: # lazily initialize W self.W = 0.001 * np.random.randn(dim, num_classes) # Run stochastic gradient descent to optimize W loss_history = [] for it in range(num_iters): X_batch = None y_batch = None idx = np.random.choice(np.arange(num_train), batch_size) X_batch, y_batch = X[idx], y[idx] # evaluate loss and gradient loss, grad = self.loss(X_batch, y_batch, reg) loss_history.append(loss) self.W -= learning_rate * grad if verbose and it % 100 == 0: print('iteration %d / %d: loss %f' % (it, num_iters, loss)) return loss_history 12345def predict(self, X): y_pred = np.zeros(X.shape[0]) y_pred = X.dot(self.W).argmax(axis=1) return y_pred Parameter Tuning ä½¿ç”¨ Grid Search çš„æ–¹æ³•å°‹æ‰¾è¶…åƒæ•¸ã€‚ 123456789101112131415161718192021222324252627282930313233343536learning_rates = [1e-7, 5e-5]regularization_strengths = [2.5e4, 5e4]results = &#123;&#125;best_val = -1 # The highest validation accuracy that we have seen so far.best_svm = None # The LinearSVM object that achieved the highest validation rate.for lr in learning_rates: for reg in regularization_strengths: print("hyperparameter tuning: lr=&#123;&#125;, reg=&#123;&#125;".format(lr, reg)) svm = LinearSVM() tic = time.time() loss_hist = svm.train(X_train, y_train, learning_rate=lr, reg=reg, num_iters=1500, verbose=True) y_train_pred = svm.predict(X_train) train_acc = np.mean(y_train == y_train_pred) y_val_pred = svm.predict(X_val) val_acc = np.mean(y_val == y_val_pred) results[(lr, reg)] = (train_acc, val_acc) if val_acc &gt; best_val: best_val = val_acc best_svm = svm print('-'*40)# Print out results.for lr, reg in sorted(results): train_accuracy, val_accuracy = results[(lr, reg)] print('lr %e reg %e train accuracy: %f val accuracy: %f' % ( lr, reg, train_accuracy, val_accuracy)) print('best validation accuracy achieved during cross-validation: %f' % best_val) å°‡å­¸ç¿’åˆ°çš„æ¬Šé‡è¦–è¦ºåŒ– learned_weight Softmax classifier èˆ‡ svm ç›¸åŒéƒ½æ˜¯è¦å¯¦ä½œå…©ç¨®æ–¹æ³•ï¼š softmax_loss_naive æ¨¡å‹çš„W,X,yéƒ½èˆ‡ SVM ç›¸åŒï¼Œå”¯ä¸€ä¸åŒçš„é»æ˜¯ softmax classifier ä½¿ç”¨çš„ loss function ä¸æ˜¯ hinge lossï¼Œè€Œæ˜¯ cross-entropy lossï¼š \(L_i=-log(\dfrac{e^{f_{y_i}}}{\sum_j e^{f_j}})\) ä¹Ÿå¯ä»¥å¯«æˆ \(L_iï¼-f_{y_i}+log\sum_j e^{f_j}\) 1234567891011121314151617181920212223def softmax_loss_naive(W, X, y, reg): # Initialize the loss and gradient to zero. loss = 0.0 dW = np.zeros_like(W) num_classes = W.shape[1] num_train = X.shape[0] for i in range(num_train): scores = X[i].dot(W) scores -= np.max(scores) correct_class_score = scores[y[i]] exp_sum = np.sum(np.exp(scores)) loss += -np.log(np.exp(correct_class_score) / exp_sum) for j in range(num_classes): dW[:, j] += (np.exp(scores[j]) / exp_sum - (y[i] == j)) * X[i] loss /= num_train dW /= num_train loss += reg * np.sum(W * W) dW += reg * W return loss, dW softmax_loss_vectorized 12345678910111213141516171819def softmax_loss_vectorized(W, X, y, reg): # Initialize the loss and gradient to zero. loss = 0.0 dW = np.zeros_like(W) num_train = X.shape[0] scores = X.dot(W) scores -= np.max(scores, axis=1, keepdims=True) prob = np.exp(scores) / np.sum(np.exp(scores), axis=1, keepdims=True) loss += np.sum(-np.log(prob[np.arange(num_train), y])) ind = np.zeros_like(prob) ind[np.arange(num_train), y] = 1 dW = X.T.dot(prob - ind) loss = loss / num_train + reg * np.sum(W * W) dW = dW / num_train + reg * W return loss, dW æ¥ä¸‹ä¾†ä¹Ÿæ˜¯å¯¦ä½œ Grid Search æœå°‹è¶…åƒæ•¸ï¼Œèˆ‡ SVM çš„éƒ¨ä»½ç›¸åŒï¼Œå°±ä¸å†è´…è¿°ã€‚ Two-Layer Neural Network Fordward Pass å¯¦ä½œä¸€å€‹ two-layer çš„ NNï¼Œä½¿ç”¨ ReLU nonlinearityã€‚ è¨ˆç®—æ–¹æ³•å¦‚ä¸‹ï¼š \(h_1=ReLU(X\cdot W_1+b_1)\) \(S=h_1\cdot W_2+b_2\) 12345678910111213141516def loss(self, X, y=None, reg=0.0): # Unpack variables from the params dictionary W1, b1 = self.params['W1'], self.params['b1'] W2, b2 = self.params['W2'], self.params['b2'] N, D = X.shape # Compute the forward pass scores = None relu = lambda x: np.maximum(0, x) h1 = relu(X.dot(W1) + b1) scores = h1.dot(W2) + b2 # If the targets are not given then jump out, we're done if y is None: return scores Compute Loss ä½¿ç”¨ softmax classifier lossï¼š\(L_i=-log(\dfrac{e^{f_{y_i}}}{\sum_j e^{f_j}})\) 1234567# Compute the lossloss = Nonescores_ = scores - np.max(scores, axis=1, keepdims=True)prob = np.exp(scores_) / np.sum(np.exp(scores_), axis=1, keepdims=True)loss = np.sum(-np.log(prob[np.arange(N), y]))loss = loss / N + 0.5 * reg * (np.sum(W1 * W1) + np.sum(W2 * W2)) Backpropagation è¨ˆç®— Loss å°æ¯å€‹åƒæ•¸çš„åå¾®åˆ†ï¼š\(\dfrac{\partial L}{\partial W_2},\dfrac{\partial L}{\partial b_2},\dfrac{\partial L}{\partial W_1},\dfrac{\partial L}{\partial b_1}\) ä»¥ä¸‹é€²è¡Œå››å€‹åå¾®åˆ†çš„æ¨å°ï¼š \(\mathbf{\dfrac{\partial L}{\partial W_2}}=\dfrac{\partial L}{\partial S}\dfrac{\partial S}{\partial W_2}=dS\cdot h_1^\top\) è€Œ \(dS\) ç‚º softmax function å° score çš„åå¾®åˆ†ï¼š \(dS=\begin{cases} \dfrac{e^{s_i}}{\sum_j e^{s_j}} - 1, &amp; j=y_i \\ \dfrac{e^{s_i}}{\sum_j e^{s_j}}, &amp; j\neq y_i \end{cases}\) \(\mathbf{\dfrac{\partial L}{\partial b_2}}=\dfrac{\partial L}{\partial S}\dfrac{\partial S}{\partial b_2}=dS\) \(\mathbf{\dfrac{\partial L}{\partial W_1}}=\dfrac{\partial L}{\partial S}\dfrac{\partial S}{\partial h_1}\dfrac{\partial h_1}{\partial W_1}=dS\cdot W_2^\top\cdot \dfrac{\partial h_1}{\partial W_1}\) è€Œ \(\dfrac{\partial h_1}{\partial W_1}=\begin{cases} 0, &amp; h_1=0 \\ x^\top, &amp; h_1 &gt; 0 \end{cases}\) å› ç‚º \(ReLU(x)=max(0,x),\dfrac{\partial ReLU}{\partial x}=\begin{cases} 0, &amp; x&lt;0 \\ 1, &amp; x&gt;0 \end{cases}\) \(\mathbf{\dfrac{\partial L}{\partial b_1}}=\dfrac{\partial L}{\partial S}\dfrac{\partial S}{\partial h_1}\dfrac{\partial h_1}{\partial b_1}=dS\cdot W_2^\top\cdot\dfrac{\partial h_1}{\partial b_1}\) 123456789101112131415# Backward pass: compute gradientsgrads = &#123;&#125;dS = probdS[np.arange(N), y] -= 1dS /= Ngrads['W2'] = h1.T.dot(dS) + reg * W2grads['b2'] = np.sum(dS, axis=0)dh1 = dS.dot(W2.T) * (h1 &gt; 0)grads['W1'] = X.T.dot(dh1) + reg * W1grads['b1'] = np.sum(dh1, axis=0)return loss, grads Higher Level Representations: Image Features è¨ˆç®—åœ–ç‰‡çš„ Histogram of Oriented Gradients (HOG) ç•¶ä½œ featureï¼Œä¸Ÿé€²æ¨¡å‹åšè¨“ç·´ã€‚é€™éƒ¨åˆ†çš„ç¨‹å¼ç¢¼éƒ½å·²ç¶“å…ˆå¯«å¥½äº†ï¼Œæˆ‘å€‘åªè¦ Tuning æ¨¡å‹çš„ Hyperparameter ä½¿æº–ç¢ºç‡åˆ°é æœŸçš„å€¼å³å¯ã€‚]]></content>
      <categories>
        <category>å­¸æ ¡èª²ç¨‹</category>
        <category>åœ–åƒè¾¨è­˜</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Computer Vision</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS224n assignment 2]]></title>
    <url>%2F2018%2F12%2F31%2FCS224n-assignment-2%2F</url>
    <content type="text"><![CDATA[CS224n assignment 2 é€™æ¬¡çš„ä½œæ¥­ä¸»è¦ç›®çš„æ˜¯è®“æˆ‘å€‘å¯¦ä½œ Dependency Parsing ä»¥åŠç†Ÿæ‚‰ Tensorflow çš„é‹ä½œåŸç†ã€‚ 1. Tensorflow Softmax In this question, we will implement a linear classifier with loss function \[ J(\mathbf{W}) = CE(\mathbf{y},softmax(\mathbf{xW} + \mathbf{b})) \] Where \(\mathbf{x}\) is a vector of features, \(\mathbf{W}\) is the modelâ€™s weight matrix, and \(\mathbf{b}\) is a bias term. We will use TensorFlowâ€™s automatic differentiation capability to fit this model to provided data. (a) ä½¿ç”¨ Tensorflow å¯¦ä½œ Softmax Implement the softmax function using TensorFlow in q1_softmax.py. Remember that \[ softmax(x)_i = \dfrac{e^{x_i}}{\sum_j{e^{x_j}}} \] Note that you may not use tf.nn.softmax or related built-in functions. You can run basic (nonexhaustive tests) by running python q1_softmax.py. 1.(a) solution 123def softmax(x): out = tf.exp(x) / tf.reduce_sum(tf.exp(x), axis=1, keepdims=True) return out (b) ä½¿ç”¨ Tensorflow å¯¦ä½œ Cross Entropy Loss Implement the cross-entropy loss using TensorFlow in q1_softmax.py. Remember that \[ CE(\mathbf{y},\mathbf{\hat{y}})=-\sum\limits_{i=1}^{N_c} y_i log(\hat{y_i})\] where \(\mathbf{y} \in \mathbb{R}^{N_c}\) is a one-hot label vector and \(Nc\) is the number of classes. This loss is summed over all examples in a minibatch. Note that you may not use TensorFlowâ€™s built-in cross-entropy functions for this question. You can run basic (non-exhaustive tests) by running python q1_softmax.py. 1.(b) solution 123def cross_entropy_loss(y, yhat): out = -tf.reduce_sum(tf.multiply(tf.to_float(y), tf.log(yhat))) return out (c) Placeholder Variable èˆ‡ Feed Dictionary Carefully study the Model class in model.py. Briefly explain the purpose of placeholder variables and feed dictionaries in TensorFlow computations. Fill in the implementations for add_placeholders and create_feed_dict in q1_classifier.py. 1.(c) solution Hint: Note that configuration variables are stored in the Config class. You will need to use these configuration variables in the code. 123def add_placeholders(self): self.input_placeholder = tf.placeholder(dtype=tf.float32, shape=(self.config.batch_size, self.config.n_features)) self.labels_placeholder = tf.placeholder(dtype=tf.float32, shape=(self.config.batch_size, self.config.n_classes)) 12345def create_feed_dict(self, inputs_batch, labels_batch=None): feed_dict = &#123;self.input_placeholder: inputs_batch, self.labels_placeholder: labels_batch&#125; return feed_dict (d) ä½¿ç”¨ Tensorflow å»ºç«‹ç¶²è·¯æ¶æ§‹ Implement the transformation for a softmax classifier in the function add_prediction_op in q1_classifier.py. Add cross-entropy loss in the function add_loss_op in the same file. Use the implementations from the earlier parts of the problem (already imported for you), not TensorFlow built-ins. 1.(d) solution 1234567def add_prediction_op(self): with tf.variable_scope('transformation'): b = tf.Variable(tf.zeros(shape=[self.config.n_classes])) W = tf.Variable(tf.zeros(shape=[self.config.n_features, self.config.n_classes])) z = tf.matmul(self.input_placeholder, W) + b pred = softmax(z) return pred 123def add_loss_op(self, pred): loss = cross_entropy_loss(self.labels_placeholder, pred) return loss (e) ä½¿ç”¨ Tensorflow åŠ å…¥ Optimizer Fill in the implementation for add_training_op in q1_classifier.py. Explain in a few sentences what happens when the modelâ€™s train_op is called (what gets computed during forward propagation, what gets computed during backpropagation, and what will have changed after the op has been run?). Verify that your model is able to fit to synthetic data by running python q1_classifier.py and making sure that the tests pass. Hint: Make sure to use the learning rate specified in Config. 1.(e) solution 123def add_training_op(self, loss): train_op = tf.train.GradientDescentOptimizer(learning_rate=self.config.lr).minimize(loss) return train_op 2. Neural Transition-Based Dependency Parsing In this section, youâ€™ll be implementing a neural-network based dependency parser. A dependency parser analyzes the grammatical structure of a sentence, establishing relationships between â€œheadâ€ words and words which modify those heads. Your implementation will be a transition-based parser, which incrementally builds up a parse one step at a time. At every step it maintains a partial parse, which is represented as follows: A stack of words that are currently being processed. A buffer of words yet to be processed. A list of dependencies predicted by the parser. Initially, the stack only contains ROOT, the dependencies lists is empty, and the buffer contains all words of the sentence in order. At each step, the parse applies a transition to the partial parse until its buffer is empty and the stack is size 1. The following transitions can be applied: SHIFT: removes the first word from the buffer and pushes it onto the stack. LEFT-ARC: marks the second (second most recently added) item on the stack as a dependent of the first item and removes the second item from the stack. RIGHT-ARC: marks the first (most recently added) item on the stack as a dependent of the second item and removes the first item from the stack. Your parser will decide among transitions at each state using a neural network classifier. First, you will implement the partial parse representation and transition functions. (a) è©¦è©¦çœ‹ Go through the sequence of transitions needed for parsing the sentence â€œI parsed this sentence correctlyâ€. The dependency tree for the sentence is shown below. At each step, give the configuration of the stack and buffer, as well as what transition was applied this step and what new dependency was added (if any). The first three steps are provided below as an example. 2.(a) solution stack buffer new dependency transition [ROOT] [I, parsed, this, setence, correctly] Initial Configuration [ROOT, I] [parsed, this, setence, correctly] SHIFT [ROOT, I, parsed] [this, setence, correctly] SHIFT [ROOT, parsed] [this, setence, correctly] parsed -&gt; I LEFT-ARC [ROOT, parsed, this] [setence, correctly] SHIFT [ROOT, parsed, this] [setence, correctly] SHIFT [ROOT, parsed, this, sentence] [correctly] SHIFT [ROOT, parsed, setence] [correctly] sentence -&gt; this LEFT-ARC [ROOT, parsed] [correctly] parse -&gt; sentence RIGHT-ARC [ROOT, parsed, correctly] [] SHFIT [ROOT, parsed] [] parse -&gt; correctly RIGHT-ARC [ROOT] [] ROOT -&gt; parsed RIGHT-ARC (b) æ™‚é–“è¤‡é›œåº¦ A sentence containing n words will be parsed in how many steps (in terms of n)? Briefly explain why. 2.(b) solution æ¯å€‹è©éƒ½æœƒé€²è¡Œä¸€æ¬¡ SHIFT åŠ LEFT/RIGHT-ARCï¼Œå› æ­¤å…± 2n æ¬¡ã€‚ (c) å¯¦ä½œ Dependency Parsing Implement the __init__ and parse_step functions in the PartialParse class in q2_parser_transitions.py. This implements the transition mechanics your parser will use. You can run basic (not-exhaustive) tests by running python q2_parser_transitions.py. 2.(c) solution 12345678910111213141516def __init__(self, sentence): self.sentence = sentence self.stack = ['ROOT'] self.buffer = sentence[:] self.dependencies = []def parse_step(self, transition): if transition == "S": self.stack.append(self.buffer[0]) self.buffer.pop(0) elif transition == "LA": self.dependencies.append((self.stack[-1], self.stack[-2])) self.stack.pop(-2) else: self.dependencies.append((self.stack[-2], self.stack[-1])) self.stack.pop(-1) (d) Minibatch Dependency Parsing Our network will predict which transition should be applied next to a partial parse. We could use it to parse a single sentence by applying predicted transitions until the parse is complete. However, neural networks run much more efficiently when making predictions about batches of data at a time (i.e., predicting the next transition for a many different partial parses simultaneously). We can parse sentences in minibatches with the following algorithm. Implement this algorithm in the minibatch_parse function in q2_ parser_transitions.py. You can run basic (not-exhaustive) tests by running python q2_parser_transitions.py. Note: You will need minibatch_parse to be correctly implemented to evaluate the model you will build in part (h). However, you do not need it to train the model, so you should be able to complete most of part (h) even if minibatch parse is not implemented yet. 2.(d) solution 123456789101112131415def minibatch_parse(sentences, model, batch_size): partial_parse = [PartialParse(sentence) for sentence in sentences] dependencies = [] while len(partial_parse) &gt; 0: mini_batch = partial_parse[:batch_size] while len(mini_batch) &gt; 0: transitions = model.predict(mini_batch) for i, action in enumerate(transitions): mini_batch[i].parse_step(action) mini_batch = [parse for parse in mini_batch if len(parse.stack) &gt; 1 or len(parse.buffer) &gt; 0] dependencies.extend(p.dependencies for p in partial_parse[:batch_size]) partial_parse = partial_parse[batch_size:] return dependencies (e) Xavier initialization In order to avoid neurons becoming too correlated and ending up in poor local minimina, it is often helpful to randomly initialize parameters. One of the most frequent initializations used is called Xavier initialization. Given a matrix \(A\) of dimension \(m Ã— n\), Xavier initialization selects values \(A_{ij}\) uniformly from \([-\epsilon,\epsilon]\), where \[\epsilon=\dfrac{\sqrt{6}}{\sqrt{m+n}}\] Implement the initialization in xavier weight init in q2 _initialization.py. You can run basic (nonexhaustive tests) by running python q2_initialization.py. This function will be used to initialize \(W\) and \(U\). 2.(e) solution 12345678def xavier_weight_init(): def _xavier_initializer(shape, **kwargs): epsilon = tf.sqrt(6 / tf.cast(np.sum(shape), tf.float32)) out = tf.random_uniform(shape=shape, minval=-epsilon, maxval=epsilon) return out # Returns defined initializer function. return _xavier_initializer (f) Dropout We will regularize our network by applying Dropout. During training this randomly sets units in the hidden layer \(h\) to zero with probability \(p_{drop}\) and then multiplies \(h\) by a constant \(\gamma\) (dropping different units each minibatch). We can write this as \[h_{drop}=\gamma d \circ h\] where \(d âˆˆ \{0, 1\}^{D_h}\) (\(D_h\) is the size of \(h\)) is a mask vector where each entry is 0 with probability \(p_{drop}\) and 1 with probability (1 âˆ’ \(p_{drop}\)). \(\gamma\) is chosen such that the value of hdrop in expectation equals \(h\): \[E_{p_{drop}}[h_{drop}]_i=h_i\] for all \(0 &lt; i &lt; D_h\). What must \(\gamma\) equal in terms of \(p_{drop}\)? Briefly justify your answer. 2.(f) solution \(E_p{h_p}_i=E_p[\gamma d_i h_i] = p \times 0 + (1-p) \times \gamma h_i = (1-p)\gamma h_i = h_i\) \(\Rightarrow r = \dfrac{1}{1-p}\) (g) Adam Optmizer We will train our model using the Adam optimizer. Recall that standard SGD uses the update rule \[\theta \leftarrow \theta - \alpha \nabla_\theta J_{minibatch}(\theta)\] where \(\theta\) is a vector containing all of the model parameters, \(J\) is the loss function, \(\nabla_\theta J_{minibatch}(\theta)\) is the gradient of the loss function with respect to the parameters on a minibatch of data, and \(\alpha\) is the learning rate. Adam uses a more sophisticated update rule with two additional steps. First, Adam uses a trick called momentum by keeping track of m, a rolling average of the gradients: \[m \leftarrow \beta_1 m + (1-\beta_1) \nabla_\theta J_{minibatch}(\theta)\] \[\theta \leftarrow \theta - \alpha m\] where \(\beta_1\) is a hyperparameter between 0 and 1 (often set to 0.9). Briefly explain (you donâ€™t need to prove mathematically, just give an intuition) how using \(m\) stops the updates from varying as much. Why might this help with learning? Adam also uses adaptive learning rates by keeping track of v, a rolling average of the magnitudes of the gradients: \[m \leftarrow \beta_1 m + (1-\beta_1) \nabla_\theta J_{minibatch}(\theta)\] \[m \leftarrow \beta_2 v + (1-\beta_2) \nabla_\theta J_{minibatch}(\theta) \circ \nabla_\theta J_{minibatch}(\theta)\] \[\theta \leftarrow \theta - \alpha \circ m / \sqrt{v}\] where \(\circ\) and \(/\) denote elementwise multiplication and division (so \(z \circ z\) is elementwise squaring) and \(\beta_2\) is a hyperparameter between 0 and 1 (often set to 0.99). Since Adam divides the update by \(\sqrt{v}\), which of the model parameters will get larger updates? Why might this help with learning? (h) Parser æ¨¡å‹å¯¦ä½œ In q2_parser_model.py implement the neural network classifier governing the dependency parser by filling in the appropriate sections. We will train and evaluate our model on the Penn Treebank (annotated with Universal Dependencies).Run python q2_parser_model.py to train your model and compute predictions on the test data (make sure to turn off debug settings when doing final evaluation). Hints: When debugging, pass the keyword argument debug=True to the main method (it is set to true by default). This will cause the code to run over a small subset of the data, so the training the model wonâ€™t take as long. This code should run within 1 hour on a CPU. When running with debug=True, you should be able to get a loss smaller than 0.2 and a UAS larger than 65 on the dev set (although in rare cases your results may be lower, there is some randomness when training). When running with debug=False, you should be able to get a loss smaller than 0.08 on the train set and an Unlabeled Attachment Score larger than 87 on the dev set. For comparison, the model in the original neural dependency parsing paper gets 92.5 UAS. If you want, you can tweak the hyperparameters for your model (hidden layer size, hyperparameters for Adam, number of epochs, etc.) to improve the performance (but you are not required to do so). 2.(h) solution 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144import cPickleimport osimport timeimport tensorflow as tffrom model import Modelfrom q2_initialization import xavier_weight_initfrom utils.parser_utils import minibatches, load_and_preprocess_dataclass Config(object): n_features = 36 n_classes = 3 dropout = 0.5 # (p_drop in the handout) embed_size = 50 hidden_size = 200 batch_size = 1024 n_epochs = 10 lr = 0.0005class ParserModel(Model): def add_placeholders(self): self.input_placeholder = tf.placeholder(dtype=tf.int32, shape=(None, self.config.n_features)) self.labels_placeholder = tf.placeholder(dtype=tf.float32, shape=(None, self.config.n_classes)) self.dropout_placeholder = tf.placeholder(dtype=tf.float32) def create_feed_dict(self, inputs_batch, labels_batch=None, dropout=0): feed_dict = &#123;self.input_placeholder: inputs_batch, self.dropout_placeholder: dropout&#125; if labels_batch is not None: feed_dict[self.labels_placeholder] = labels_batch return feed_dict def add_embedding(self): embedding = tf.Variable(self.pretrained_embeddings) embeddings = tf.reshape(tf.nn.embedding_lookup(embedding, self.input_placeholder), [-1, self.config.n_features * self.config.embed_size]) return embeddings def add_prediction_op(self): x = self.add_embedding() xavier_initializer = xavier_weight_init() W = tf.Variable(xavier_initializer((self.config.n_features * self.config.embed_size, self.config.hidden_size))) b1 = tf.Variable(tf.zeros(self.config.hidden_size)) U = tf.Variable(xavier_initializer((self.config.hidden_size, self.config.n_classes))) b2 = tf.Variable(tf.zeros(self.config.n_classes)) h = tf.nn.relu(tf.matmul(x, W) + b1) h_drop = tf.nn.dropout(h, 1 - self.dropout_placeholder) pred = tf.matmul(h_drop, U) + b2 return pred def add_loss_op(self, pred): loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=self.labels_placeholder)) return loss def add_training_op(self, loss): train_op = tf.train.AdamOptimizer(learning_rate=self.config.lr).minimize(loss) return train_op def train_on_batch(self, sess, inputs_batch, labels_batch): feed = self.create_feed_dict(inputs_batch, labels_batch=labels_batch, dropout=self.config.dropout) _, loss = sess.run([self.train_op, self.loss], feed_dict=feed) return loss def run_epoch(self, sess, parser, train_examples, dev_set): n_minibatches = 1 + len(train_examples) / self.config.batch_size prog = tf.keras.utils.Progbar(target=n_minibatches) for i, (train_x, train_y) in enumerate(minibatches(train_examples, self.config.batch_size)): loss = self.train_on_batch(sess, train_x, train_y) prog.update(i + 1, [("train loss", loss)]) print "Evaluating on dev set", dev_UAS, _ = parser.parse(dev_set) print "- dev UAS: &#123;:.2f&#125;".format(dev_UAS * 100.0) return dev_UAS def fit(self, sess, saver, parser, train_examples, dev_set): best_dev_UAS = 0 for epoch in range(self.config.n_epochs): print "Epoch &#123;:&#125; out of &#123;:&#125;".format(epoch + 1, self.config.n_epochs) dev_UAS = self.run_epoch(sess, parser, train_examples, dev_set) if dev_UAS &gt; best_dev_UAS: best_dev_UAS = dev_UAS if saver: print "New best dev UAS! Saving model in ./data/weights/parser.weights" saver.save(sess, './data/weights/parser.weights') print def __init__(self, config, pretrained_embeddings): self.pretrained_embeddings = pretrained_embeddings self.config = config self.build()def main(debug=True): print 80 * "=" print "INITIALIZING" print 80 * "=" config = Config() parser, embeddings, train_examples, dev_set, test_set = load_and_preprocess_data(debug) if not os.path.exists('./data/weights/'): os.makedirs('./data/weights/') with tf.Graph().as_default() as graph: print "Building model...", start = time.time() model = ParserModel(config, embeddings) parser.model = model init_op = tf.global_variables_initializer() saver = None if debug else tf.train.Saver() print "took &#123;:.2f&#125; seconds\n".format(time.time() - start) graph.finalize() with tf.Session(graph=graph) as session: parser.session = session session.run(init_op) print 80 * "=" print "TRAINING" print 80 * "=" model.fit(session, saver, parser, train_examples, dev_set) if not debug: print 80 * "=" print "TESTING" print 80 * "=" print "Restoring the best model weights found on the dev set" saver.restore(session, './data/weights/parser.weights') print "Final evaluation on test set", UAS, dependencies = parser.parse(test_set) print "- test UAS: &#123;:.2f&#125;".format(UAS * 100.0) print "Writing predictions" with open('q2_test.predicted.pkl', 'w') as f: cPickle.dump(dependencies, f, -1) print "Done!"if __name__ == '__main__': main(False) (i) åŠ åˆ†é¡Œ 3. Recurrent Neural Networks: Language Modeling]]></content>
      <categories>
        <category>å­¸æ ¡èª²ç¨‹</category>
        <category>è‡ªç„¶èªè¨€è™•ç†</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS224n assignment 1]]></title>
    <url>%2F2018%2F12%2F05%2FCS224n-assignment-1%2F</url>
    <content type="text"><![CDATA[CS224n assignment 1 é€™æ˜¯ Stanford University çš„ä¸€é–€é–‹æ”¾å¼èª²ç¨‹ï¼Œå«åš Natural Language Processing with Deep Learningï¼Œæ‰€æŒ‡æ´¾çš„ç¬¬ä¸€å€‹ä½œæ¥­ã€‚é€™å€‹ä½œæ¥­é€éä»¥æ•¸å­¸æ¨å°é¡ç¥ç¶“ç¶²è·¯çš„é‹ç®—éç¨‹ï¼Œä»¥åŠå¯¦ä½œwor2vecä¾†é€²è¡Œæƒ…æ„Ÿåˆ†æï¼Œç‚ºåˆå­¸è€…å­¸ç¿’è‡ªç„¶èªè¨€è™•ç†æ‰“ä¸‹åŸºç¤ã€‚ é€™å€‹ä½œæ¥­ä¸»è¦åˆ†ç‚º 4 å€‹éƒ¨åˆ†ï¼š Softmax Neural Network Bascis word2vec Sentiment Analysis 1. Softmax (a) softmax çš„æ€§è³ª Prove that softmax is invariant to constant offsets in the input, that is, for any input vector \(x\) and any constant \(c\), \[softmax(x) = softmax(x + c)\] where \(x + c\) means adding the constant \(c\) to every dimension of \(x\). Remember that \[softmax(x)_i=\dfrac{e^{x_i}}{\sum_j{e^{x_j}}}\] Note: In practice, we make use of this property and choose \(c = âˆ’ max_i x_i\) when computing softmax probabilities for numerical stability (i.e., subtracting its maximum element from all elements of \(x\)). solution è­‰æ˜è‹¥å°‡ softmax çš„è¼¸å…¥ x çš„æ¯ä¸€é …éƒ½åŠ ä¸Šä¸€å€‹å¸¸æ•¸å¾Œï¼Œçµæœæœƒèˆ‡åŸæœ¬ç›¸åŒã€‚ \(softmax(x+c)_i=\dfrac{e^{x_i+c}}{\sum_j{e^{x_j+c}}}=\dfrac{e^{x_i}e^c}{\sum_j{e^{x_j}e^c}}=\dfrac{e^ce^{x_i}}{e^c\sum_j{e^{x_j}}}=\dfrac{e^{x_i}}{\sum_j{e^{x_j}}}=softmax(x)\) é€™å€‹ç‰¹æ€§åœ¨å¯¦éš›è¨ˆç®— softmax æ™‚å¸¸è¢«ä½¿ç”¨ï¼Œå°‡è¼¸å…¥ x çš„æ¯ä¸€é …éƒ½æ¸›å» x ä¸­çš„æœ€å¤§å€¼ï¼Œå¯ä»¥æ¸›å°‘è¨ˆç®—é‡ã€‚ (b) å¯¦ä½œ softmax function Given an input matrix of N rows and D columns, compute the softmax prediction for each row using the optimization in part (a). Write your implementation in q1_softmax.py. You may test by executing python q1_softmax.py. Note: The provided tests are not exhaustive. Later parts of the assignment will reference this code so it is important to have a correct implementation. Your implementation should also be efficient and vectorized whenever possible (i.e., use numpy matrix operations rather than for loops). A non-vectorized implementation will not receive full credit! solution è¼¸å…¥ï¼šx -- ä¸€å€‹ç¶­åº¦ç‚º N x D çš„ numpy çŸ©é™£ è¼¸å‡ºï¼šx -- softmax(x)ï¼Œå¯ä»¥ in-place ä¿®æ”¹ x çš„å€¼ æ³¨æ„åœ¨xçš„ç¶­åº¦ç‚º 1 x D èˆ‡ N x D (N â‰¥ 2)æ™‚çš„è™•ç†æ–¹å¼ä¸åŒã€‚ 12345678910111213def softmax(x): orig_shape = x.shape if len(x.shape) &gt; 1: # Matrix x = np.apply_along_axis(softmax, 1, x) else: # Vector x -= np.amax(x) x = np.exp(x) / np.sum(np.exp(x)) assert x.shape == orig_shape return x 2. Neural Network Basics (a) æ¨å° sigmoid function çš„ gradient Derive the gradients of the sigmoid function and show that it can be rewritten as a function of the function value (i.e., in some expression where only \(\sigma(x)\), but not \(x\), is present). Assume that the input \(x\) is a scalar for this question. Recall, the sigmoid function is \[\sigma(x)=\dfrac{1}{1+e^{-x}}\] solution \(\dfrac{\partial\sigma(x)}{\partial x}=\dfrac{-1}{(1+e^{-x})^2}\times e^{-x}\times (-1)=\dfrac{e^{-x}}{(1+e^{-x})^2}=\dfrac{1}{1+e^{-x}}\times{\dfrac{1+e^{-x}-1}{1+e^{-x}}}=\sigma(x)(1-\sigma(x))\) (b) æ¨å° cross entropy loss çš„ gradient Derive the gradient with regard to the inputs of a softmax function when cross entropy loss is used for evaluation, i.e., find the gradients with respect to the softmax input vector \(\theta\), when the prediction is made by \(\hat y = softmax(\theta)\). Remember the cross entropy function is \[CE(y,\hat{y})=-\sum_i y_i log(\hat{y_i})\] where \(y\) is the one-hot label vector, and \(\hat{y}\) is the predicted probability vector for all classes. (Hint: you might want to consider the fact many elements of \(y\) are zeros, and assume that only the k-th dimension of \(y\) is one.) solution è¨ˆç®— \(\dfrac{\partial CE(y,\hat{y})}{\partial \theta}\) åˆ†æˆå…©ç¨®æƒ…æ³ï¼š 1. m = n \(\dfrac{\partial CE(y_m,\hat{y_m})}{\partial \theta_n}=\dfrac{\partial -y_nlog\hat{y_n}}{\partial \theta_n}=-y_n \times \dfrac{1}{\hat{y_n}}\times \dfrac{\partial \dfrac{e^{\theta_n}}{\sum_j{e^{\theta j}}}}{\partial \theta_n}=-y_n \times \dfrac{1}{\hat{y_n}}\times\dfrac{e^{\theta n}\sum_j{e^{\theta_j}-e^{\theta n}e^{\theta n}}}{(\sum_j{e^{\theta_j}})^2}\) \(=-y_n \times \dfrac{1}{\hat{y_n}} \times \dfrac{e^{\theta n}}{\sum_j{e^{\theta_j}}}\times\dfrac{\sum_j{e^{\theta j}-e^{\theta n}}}{\sum_j{e^{\theta_j}}}=-y_n \times \dfrac{1}{\hat{y_n}} \times \hat{y_n} \times (1-\hat{y_n}) =-y_n(1-\hat{y_n})\) 2. m â‰  n \(\dfrac{\partial CE(y_m,\hat{y_m})}{\partial\theta\_n}=\dfrac{- \sum\_{m \neq n}{y_m log\hat{y_m}}}{\partial\theta\_n}=-\sum_{m \neq n} y_m \times \dfrac{1}{\hat{y_m}} \times \dfrac{\partial \dfrac{e^{\theta_m}}{\sum_j{e^{\theta_j}}}}{\partial \theta\_n}=-\sum_{m \neq n} y_m \times \dfrac{1}{\hat{y_m}} \times \dfrac{0-e^{\theta_n}e^{\theta_m}}{(\sum_j{e^{\theta\_j}})^2}\) \(=-\sum_{m \neq n} y_m \times \dfrac{1}{\hat{y_m}} \times \dfrac{-e^{\theta_n}}{\sum_j{e^{\theta_j}}} \times \dfrac{e^{\theta_m}}{\sum_j{e^{\theta\_j}}}=-\sum_{m \neq n} y_m \times \dfrac{1}{\hat{y_m}} \times (-\hat{y_n}) \times \hat{y\_m}=\sum_{m \neq n}y_m \hat{y_n}\) å› æ­¤ \(\dfrac{\partial CE}{\partial \theta_i}=-y_i(1-\hat{y\_i}) + \sum_{k \neq i}y_k \hat{y_i}=-y_i+\hat{y\_i}^2 + \sum_{k \neq i}y_k \hat{y_i}=\sum_k{y_k \hat{y_i}}-y_i=\hat{y_i}\sum_k{y_k}-y_i=\hat{y_i}-y_i\) (c) Backpropagation Derive the gradients with respect to the inputs \(x\) to an one-hidden-layer neural network (that is, find \(\frac{\partial J}{\partial x}\) where \(J = CE(y, \hat{y})\) is the cost function for the neural network). The neural network employs sigmoid activation function for the hidden layer, and softmax for the output layer. Assume the one-hot label vector is \(y\), and cross entropy cost is used. (Feel free to use \(\sigma&#39;(x)\) as the shorthand for sigmoid gradient, and feel free to define any variables whenever you see fit.) Recall that the forward propagation is as follows \[h = sigmoid(xW_1 + b_1), \hat{y} = softmax(hW_2 + b_2)\] Note that here weâ€™re assuming that the input vector (thus the hidden variables and output probabilities) is a row vector to be consistent with the programming assignment. When we apply the sigmoid function to a vector, we are applying it to each of the elements of that vector. \(W_i\) and \(b_i (i = 1, 2)\) are the weights and biases, respectively, of the two layers. solution ä»¤ \(z_1=xW_1+b_1,z_2=hW_2+b_2\) å‰‡ \(\dfrac{\partial J}{\partial x}=\dfrac{\partial J}{\partial z2}\dfrac{\partial z2}{\partial h}\dfrac{\partial h}{\partial z1}\dfrac{\partial z1}{\partial x}=(\hat{y}-y)\times W_2^\top\times h(1-h)\times W_1^\top\) (d) é¡ç¥ç¶“ç¶²è·¯çš„åƒæ•¸æ•¸é‡ How many parameters are there in this neural network, assuming the input is \(Dx\)-dimensional, the output is \(Dy\)-dimensional, and there are \(H\) hidden units? solution æ ¹æ“šä¸Šä¸€é¡Œçš„åœ–ï¼Œæ­¤ç¥ç¶“ç¶²è·¯æœ‰ä¸‰å€‹ layerï¼š\(x,h,\hat{y}\)ï¼Œå…¶ä¸­çš„åƒæ•¸æœ‰ \(W_1,b_1,W_2,b_2\) å…±å››å€‹ï¼š \(W_1: D_x \times H\) \(b_1: H\) \(W_2: H \times D_y\) \(b_2: D_y\) ç¸½å…±æœ‰ \((D_x \times H)+H+(H \times D_y)+D_y=(D_x+1)H+(H+1)D_y\) å€‹åƒæ•¸ã€‚ (e) å¯¦ä½œ sigmoid function Fill in the implementation for the sigmoid activation function and its gradient in q2_sigmoid.py. Test your implementation using python q2_sigmoid.py. Again, thoroughly test your code as the provided tests may not be exhaustive. solution å¯¦ä½œå‡ºsigmoidä»¥åŠå…¶æ¢¯åº¦sigmoid_gradå…©å€‹å‡½å¼ï¼Œç›´æ¥åˆ©ç”¨(a)å°é¡Œçš„çµæœå¯ä»¥è¼•é¬†å¯¦ç¾ã€‚ 123def sigmoid(x): s = 1 / (1 + np.exp(-x)) return s 123def sigmoid_grad(s): ds = s * (1 - s) return ds (f) Gradient Check To make debugging easier, we will now implement a gradient checker. Fill in the implementation for gradcheck naive in q2_gradcheck.py. Test your code using python q2_gradcheck.py. solution è¨»è§£ä¸­æŒ‡å®šä½¿ç”¨ centered difference æ–¹æ³•ä¾†å¯¦ä½œ gradient checkï¼Œå› ç‚ºä»–æ¯” forward / backward difference æ–¹æ³•èª¤å·®æ›´å°ã€‚ Forward difference approximation: \[f&#39;(x)\approx \dfrac{f(x+h)âˆ’f(x)}{h}\] Central difference approximations \[f&#39;(x)\approx \dfrac{f(x+h)-f(x-h)}{2h}\] Backward difference approximations: \[f&#39;(x)\approx \dfrac{f(x)âˆ’f(xâˆ’h)}{h}\] 1234567891011121314151617181920212223242526272829303132333435363738def gradcheck_naive(f, x): rndstate = random.getstate() random.setstate(rndstate) fx, grad = f(x) # Evaluate function value at original point h = 1e-4 # Do not change this! # Iterate over all indexes ix in x to check the gradient. it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite']) while not it.finished: ix = it.multi_index ### YOUR CODE HERE: x_upper, x_lower = x.copy(), x.copy() x_upper[ix] += h random.setstate(rndstate) f_upper = f(x_upper)[0] x_lower[ix] -= h random.setstate(rndstate) f_lower = f(x_lower)[0] numgrad = (f_upper - f_lower) / (2 * h) ### END YOUR CODE # Compare gradients reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix])) if reldiff &gt; 1e-5: print "Gradient check failed." print "First gradient error found at index %s" % str(ix) print "Your gradient: %f \t Numerical gradient: %f" % ( grad[ix], numgrad) return it.iternext() # Step to next dimension print "Gradient check passed!" (g) å¯¦ä½œé¡ç¥ç¶“ç¶²è·¯çš„ Forward åŠ Backward Pass Now, implement the forward and backward passes for a neural network with one sigmoid hidden layer. Fill in your implementation in q2_neural.py. Sanity check your implementation with python q2_neural.py. solution 1234567891011121314151617181920212223242526272829303132333435def forward_backward_prop(X, labels, params, dimensions): ### Unpack network parameters (do not modify) ofs = 0 Dx, H, Dy = (dimensions[0], dimensions[1], dimensions[2]) W1 = np.reshape(params[ofs:ofs+ Dx * H], (Dx, H)) ofs += Dx * H b1 = np.reshape(params[ofs:ofs + H], (1, H)) ofs += H W2 = np.reshape(params[ofs:ofs + H * Dy], (H, Dy)) ofs += H * Dy b2 = np.reshape(params[ofs:ofs + Dy], (1, Dy)) # Note: compute cost based on `sum` not `mean`. ### YOUR CODE HERE: forward propagation h = sigmoid(np.dot(X, W1) + b1) y_hat = softmax(np.dot(h, W2) + b2) cost = cost = -np.sum(labels * np.log(y_hat)) ### END YOUR CODE ### YOUR CODE HERE: backward propagation d3 = y_hat - labels gradW2 = np.dot(h.T, d3) gradb2 = np.sum(d3, axis=0) dh = np.dot(d3, W2.T) grad_h = sigmoid_grad(h) * dh gradW1 = np.dot(X.T, grad_h) gradb1 = np.sum(grad_h, axis=0) ### END YOUR CODE ### Stack gradients (do not modify) grad = np.concatenate((gradW1.flatten(), gradb1.flatten(), gradW2.flatten(), gradb2.flatten())) return cost, grad 3. word2vec (a) è¨ˆç®— Loss Function å° \(v_c\) çš„ gradient Assume you are given a â€œpredictedâ€ word vector \(v_c\) corresponding to the center word \(c\) for Skip-Gram, and word prediction is made with the softmax function found in word2vec models \[\hat{y_o}=p(o|c)=\dfrac{exp(u_o^\top v\_c)}{\sum\limits_{w=1}^V{exp(u_w^\top v_c)}}\] where \(u_w (w = 1, . . . , V )\) are the â€œoutputâ€ word vectors for all words in the vocabulary. Assuming cross entropy cost is applied to this prediction and word o is the expected word (the o-th element of the one-hot label vector is one), derive the gradients with respect to \(v_c\). Hint: It will be helpful to use notation from question 2. For instance, letting \(\hat{y}\) be the vector of softmax predictions for every word, \(y\) as the expected word vector, and the loss function \[J_{softmaxâˆ’CE}(o, v_c, U) = CE(y,\hat{y})\] where \(U = [u_1,u_2, Â· Â· Â· ,u_V ]\) is the matrix of all the output vectors. Make sure you state the orientation of your vectors and matrices. solution æ±‚ \(\dfrac{\partial J}{\partial v_c}\) \(J=-\sum_i y_i log\hat{y_i}=-log \dfrac{exp(u_o^\top v\_c)}{\sum\limits_{w=1}^V exp(u_w^\top v_c)}=-log(exp(u_o^\top v\_c))+log(\sum\limits_{w=1}^V exp(u_w^\top v_c))\) \(\dfrac{\partial J}{\partial v_c}=\dfrac{\partial -log(exp(u_o^\top v_c))}{\partial v\_c}+\dfrac{\partial log(\sum_{w=1}^V exp(u_w^\top v_c))}{\partial v_c}\) \(\dfrac{-1}{exp(u_o^\top v_c)} \times exp(u_o^\top v_c) \times u_o=-u_o\) \(\dfrac{1}{\sum\limits_{w=1}^V exp(u_w^\top v\_c)} \times \sum\limits_{x=1}^V exp(u_x^\top v_c) \times u\_x=\sum\limits\_{x=1}^V \dfrac{exp(u_x^\top v\_c)}{\sum\limits_{w=1}^V exp(u_w^\top v_c)}\times u\_x=\sum\limits\_{x=1}^V \hat{y_x} u_x\) \(\dfrac{\partial J}{\partial v_c}=-u\_o+\sum\limits_{x=1}^V \hat{y_x} u_x=U(\hat{y}-y)\) (b) è¨ˆç®— Loss Function å° \(u_k\) çš„ Gradient As in the previous part, derive gradients for the â€œoutputâ€ word vectors \(u_k\)â€™s (including \(u_o\)). solution \(\dfrac{\partial J}{\partial u_w}=\dfrac{\partial -log(exp(u_o^\top v_c))}{\partial u\_w}+\dfrac{\partial log(\sum_{w=1}^V exp(u_w^\top v_c))}{\partial u_w}\) \(1. w=o\) \(=-v\_c+\dfrac{1}{\sum_{x=1}^V exp(u_x^\top v\_c)} \times \sum\limits_{w=1}^{V}exp(u_w^\top v_c) \times v_c=v\_c \times \sum\limits_{w=1}^V \dfrac{exp(v_w^\top v\_c)}{\sum\limits_{x=1}^V exp(u_x^\top v\_c)}-v\_c=\sum\limits\_{w=1}^V \hat{y_w}v_c-v_c=v_c(\hat{y}-y)^\top\) \(2. w\neq o\) \(=0+\dfrac{1}{\sum_{x=1}^V exp(u_x^\top v\_c)} \times \sum\limits_{w=1}^{V}exp(u_w^\top v_c) \times v_c=v\_c \times \sum\limits_{w=1}^V \dfrac{exp(v_w^\top v\_c)}{\sum\limits_{x=1}^V exp(u_x^\top v\_c)}=\sum\limits\_{w=1}^V \hat{y_w}v_c\) (c) Negative Sampling Repeat part (a) and (b) assuming we are using the negative sampling loss for the predicted vector \(v_c\), and the expected output word index is \(o\). Assume that \(K\) negative samples (words) are drawn, and they are \(1, 2, ..., K\) respectively for simplicity of notation \((o \notin {1, . . . , K})\). Again, for a given word, \(o\), denote its output vector as \(u_o\). The negative sampling loss function in this case is \[J_{neg-sample}(o,V_c,U)=-log(\sigma(u_o^\top v\_c))-\sum\limits_{k=1}^K log(\sigma(-u_k^\top v_c))\] where \(Ïƒ(Â·)\) is the sigmoid function. After youâ€™ve done this, describe with one sentence why this cost function is much more efficient to compute than the softmax-CE loss (you could provide a speed-up ratio, i.e., the runtime of the softmaxCE loss divided by the runtime of the negative sampling loss). Note: the cost function here is the negative of what Mikolov et al had in their original paper, because we are doing a minimization instead of maximization in our code. solution \(1. \mathbf{\dfrac{\partial J}{\partial v_c}}=-dfrac{1}{\sigma(u_o^\top v_c)}\times \sigma(u_o^\top v_c) \times (1-\sigma(u_o^\top v_c)) \times v\_o - \sum\limits\_{k=1}^K \dfrac{1}{\sigma(-u_k^\top v_c)} \times \sigma(-u_k^\top v_c) \times (1-\sigma(-u_k^\top v_c)) \times (-u_k)\) \(=-u_o(1-\sigma(u_o^\top v\_c))+\sum\limits\_{k=1}^K u_k(1-\sigma(-u_k^\top v_c))\) \(2. \mathbf{\dfrac{\partial J}{\partial u_o}}=-\dfrac{1}{\sigma(u_o^\top v_c)}\times \sigma(u_o^\top v_c) \times (1-\sigma(u_o^\top v_c)) \times v_c\) \(=-v_c(1-\sigma(u_o^\top v_c))=(\sigma(u_o^\top v_c)-1)v_c\) \(3. \mathbf{\dfrac{\partial J}{\partial u_k}}=-\dfrac{1}{\sigma(-u_k^\top v_c)}\times \sigma(-u_k^\top v_c) \times (1-\sigma(-u_k^\top v_c)) \times (-v_c)\) \(=(1-\sigma(-u_k^\top v_c))v_c,k=1,2,...,K\) (d) è¨ˆç®— Skip-gram åŠ CBOW çš„ Gradient Suppose the center word is \(c = w\_t\) and the context words are \([w\_{tâˆ’m}, . . ., w_{tâˆ’1}, w\_t, w\_{t+1},. . ., w_{t+m}]\), where \(m\) is the context size. Derive gradients for all of the word vectors for Skip-Gram and CBOW given the previous parts. Hint: feel free to use \(F(o, v\_c)\) (where o is the expected word) as a placeholder for the \(J_{softmaxâˆ’CE}(o, v\_c, ...)\) or \(J_{negâˆ’sample}(o, v_c, ...)\) cost functions in this part â€” youâ€™ll see that this is a useful abstraction for the coding part. That is, your solution may contain terms of the form \(\frac{\partial F(o,v_c)} {\partial ...}\) . Recall that for skip-gram, the cost for a context centered around \(c\) is \[J\_{skip-gram}(w\_{tâˆ’m...t+m}) = \sum\_{âˆ’mâ‰¤jâ‰¤m,j\neq 0}F(w_{t+j} , v_c)\] where \(w_{t+j}\) refers to the word at the j-th index from the center. CBOW is slightly different. Instead of using \(v_c\) as the predicted vector, we use \(\hat{v}\) defined below. For (a simpler variant of) CBOW, we sum up the input word vectors in the context \[\hat{v} = \sum\_{âˆ’mâ‰¤jâ‰¤m,j\neq =0}v_{w_t+j}\] then the CBOW cost is \[J\_{CBOW}(w_{câˆ’m...c+m}) = F(w_t, \hat{v})\] Note: To be consistent with the \(\hat{v}\) notation such as for the code portion, for skip-gram \(\hat{v} = v_c\). solution Skip-gram \(\dfrac{\partial J\_{skip-gram}(w\_{t-m...t+m})}{\partial U}=\sum\_{-mâ‰¤jâ‰¤m,j\neq 0} \dfrac{\partial F(w\_{t+j},v_c)}{\partial U}\) \(\dfrac{\partial J\_{skip-gram}(w\_{t-m...t+m})}{\partial v\_c}=\sum\_{-mâ‰¤jâ‰¤m,j\neq 0} \dfrac{\partial F(w\_{t+j},v_c)}{\partial v_c}\) \(\dfrac{\partial J\_{skip-gram}(w\_{t-m...t+m})}{\partial v_{t+j}}=0,\forall j \neq c\) CBOW \(\dfrac{\partial J\_{CBOW}(w\_{t-m...t+m})}{\partial U}=\dfrac{\partial F(w_t,\hat{v})}{\partial U}\) \(\dfrac{\partial J\_{CBOW}(w\_{t-m...t+m})}{\partial v\_{w\_{t+j}}}=\dfrac{\partial F(w_t,\hat{v})}{\partial \hat{v}},\forall j \in \{-m,...,-1,1,...,m\}\) \(\dfrac{\partial J\_{CBOW}(w\_{t-m...t+m})}{\partial U}=0,\forall j \notin \{-m,...,-1,1,...,m\}\) (e) å¯¦ä½œ Skip-gram In this part you will implement the word2vec models and train your own word vectors with stochastic gradient descent (SGD). First, write a helper function to normalize rows of a matrix in q3_word2vec.py. In the same file, fill in the implementation for the softmax and negative sampling cost and gradient functions. Then, fill in the implementation of the cost and gradient functions for the skipgram model. When you are done, test your implementation by running python q3_word2vec.py. Note: If you choose not to implement CBOW (part h), simply remove the NotImplementedError so that your tests will complete. solution å¯¦ä½œq3_word2vec.pyä¸­çš„ä»¥ä¸‹å‡½å¼ï¼š normalizeRows softmaxCostAndGradient negSamplingCostAndGradient skipgram normalizeRows å°æ¯å€‹åˆ—å‘é‡é€²è¡Œæ­£è¦åŒ–ï¼Œå…¬å¼ï¼š\(\hat{v}=\dfrac{v}{|v|}\)ã€‚ 1234def normalizeRows(x): length = np.sqrt(np.sum(np.power(x, 2), axis=1)) x /= length[:,None] return x softmaxCostAndGradient predicted: \(v_c\)ã€target: \(o\)ã€outputVector: \(U^\top\)ã€gradz2: \(\hat{y}-y\)ã€gradPred: \(\dfrac{\partial J}{\partial v_c}\)ã€grad: \(\dfrac{\partial J}{\partial u_k}\) åˆ©ç”¨ (a)(b) å°é¡Œçš„çµæœï¼š \(\dfrac{\partial J}{\partial v_c}=U(\hat{y}-y)\) \(\dfrac{\partial J}{\partial u_k}=v_c(\hat{y}-y)^\top\) 123456789101112def softmaxCostAndGradient(predicted, target, outputVectors, dataset): ### YOUR CODE HERE y_hat = softmax(np.dot(outputVectors, predicted)) cost = -np.log(y_hat[target]) gradz2 = y_hat.copy() gradz2[target] -= 1.0 gradPred = np.dot(outputVectors.T, gradz2) grad = np.outer(gradz2, predicted) ### END YOUR CODE return cost, gradPred, grad negSamplingCostAndGradient åˆ©ç”¨ (c) å°é¡Œçš„çµæœï¼š gradPred: \(\mathbf{\dfrac{\partial J}{\partial v_c}}=-u_o(1-\sigma(u_o^\top v\_c))+\sum\limits\_{k=1}^K u_k(1-\sigma(-u_k^\top v_c))\) grad: \(\mathbf{\dfrac{\partial J}{\partial u_o}}=(\sigma(u_o^\top v_c)-1)v_c\) grad: \(\mathbf{\dfrac{\partial J}{\partial u_k}}=(1-\sigma(-u_k^\top v_c))v_c,k=1,2,...,K\) cost function ç‚º \(CE(y,\hat{y})\) 1234567891011121314151617181920212223def negSamplingCostAndGradient(predicted, target, outputVectors, dataset, K=10): # Sampling of indices is done for you. Do not modify this if you # wish to match the autograder and receive points! indices = [target] indices.extend(getNegativeSamples(target, dataset, K)) ### YOUR CODE HERE grad = np.zeros(outputVectors.shape) gradPred = np.zeros(predicted.shape) z = sigmoid(np.dot(outputVectors[target].T, predicted)) cost = -np.log(z) grad[target] += (z - 1.0) * predicted gradPred += (z - 1.0) * outputVectors[target] for k in xrange(1, K + 1): index = indices[k] z = sigmoid(np.dot(-outputVectors[index].T, predicted)) cost -= np.log(z) grad[index] -= (z - 1.0) * predicted gradPred -= (z - 1.0) * outputVectors[index] ### END YOUR CODE return cost, gradPred, grad skipgram åˆ©ç”¨ (d) å°é¡Œçš„çµæœï¼š Skip-gram gradOut:\(\dfrac{\partial J\_{skip-gram}(w\_{t-m...t+m})}{\partial U}=\sum\_{-mâ‰¤jâ‰¤m,j\neq 0} \dfrac{\partial F(w\_{t+j},v_c)}{\partial U}\) gradIn: \(\dfrac{\partial J\_{skip-gram}(w\_{t-m...t+m})}{\partial v\_c}=\sum\_{-mâ‰¤jâ‰¤m,j\neq 0} \dfrac{\partial F(w\_{t+j},v_c)}{\partial v_c}\) gradIn: \(\dfrac{\partial J\_{skip-gram}(w\_{t-m...t+m})}{\partial v_{t+j}}=0,\forall j \neq c\) 1234567891011121314151617181920def skipgram(currentWord, C, contextWords, tokens, inputVectors, outputVectors, dataset, word2vecCostAndGradient=softmaxCostAndGradient): cost = 0.0 gradIn = np.zeros(inputVectors.shape) gradOut = np.zeros(outputVectors.shape) ### YOUR CODE HERE current_word_index = tokens[currentWord] vc = inputVectors[current_word_index] for j in contextWords: target_index = tokens[j] c_cost, c_grad_in, c_grad_out = word2vecCostAndGradient(vc, target_index, outputVectors, dataset) cost += c_cost gradIn[current_word_index] += c_grad_in gradOut += c_grad_out ### END YOUR CODE return cost, gradIn, gradOut åŸ·è¡Œpython q3_word2vec.pyï¼Œç”¢ç”Ÿä»¥ä¸‹çµæœä»£è¡¨æˆåŠŸã€‚ 1234567Testing normalizeRows...[[ 0.6 0.8 ] [ 0.4472136 0.89442719]]==== Gradient check for skip-gram ====Gradient check passed!Gradient check passed! (f) Stochastic Gradient Descent Complete the implementation for your SGD optimizer in q3_sgd.py. Test your implementation by running python q3_sgd.py. solution stepå°±æ˜¯ learning rateï¼ŒåŸå§‹ç¨‹å¼ç¢¼å·²ç¶“çµ¦å®šï¼Œç›´æ¥å°‡å…¶ä¹˜ä¸Š gradient å³å¯ã€‚ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849def sgd(f, x0, step, iterations, postprocessing=None, useSaved=False, PRINT_EVERY=10): # Anneal learning rate every several iterations ANNEAL_EVERY = 20000 if useSaved: start_iter, oldx, state = load_saved_params() if start_iter &gt; 0: x0 = oldx step *= 0.5 ** (start_iter / ANNEAL_EVERY) if state: random.setstate(state) else: start_iter = 0 x = x0 if not postprocessing: postprocessing = lambda x: x expcost = None for iter in xrange(start_iter + 1, iterations + 1): # Don't forget to apply the postprocessing after every iteration! # You might want to print the progress every few iterations. cost = None ### YOUR CODE HERE cost, grad = f(x) x -= step * grad postprocessing(x) ### END YOUR CODE if iter % PRINT_EVERY == 0: if not expcost: expcost = cost else: expcost = .95 * expcost + .05 * cost print "iter %d: %f" % (iter, expcost) if iter % SAVE_PARAMS_EVERY == 0 and useSaved: save_params(iter, x) if iter % ANNEAL_EVERY == 0: step *= 0.5 return x (g) è¨“ç·´è©å‘é‡ Show time! Now we are going to load some real data and train word vectors with everything you just implemented! We are going to use the Stanford Sentiment Treebank (SST) dataset to train word vectors, and later apply them to a simple sentiment analysis task. You will need to fetch the datasets first. To do this, run sh get datasets.sh. There is no additional code to write for this part; just run python q3_run.py. Note: The training process may take a long time depending on the efficiency of your implementation (an efficient implementation takes approximately an hour). Plan accordingly! When the script finishes, a visualization for your word vectors will appear. It will also be saved as q3_word vectors.png in your project directory. Include the plot in your homework write up. Briefly explain in at most three sentences what you see in the plot. solution åŸ·è¡Œsh get datasets.shä¸‹è¼‰ Stanford Sentiment Treebank (SST) è³‡æ–™é›†ã€‚ æ¥è‘—å†åŸ·è¡Œpython q3_run.pyé–‹å§‹è¨“ç·´ï¼Œé è¨­iteration=40000ï¼Œæ¯ 10 æ¬¡ iteration æœƒå°å‡ºä¸€æ¬¡ç•¶å‰çš„ costï¼Œæ²’æ„å¤–çš„è©± cost æ‡‰è©²è¦éš¨è‘—è¨“ç·´çš„éç¨‹éæ¸›ã€‚ 12345678iter 39950: 9.435311iter 39960: 9.492463iter 39970: 9.520291iter 39980: 9.524589iter 39990: 9.550077iter 40000: 9.577164sanity check: cost at convergence should be around or below 10training took 4038 seconds è¨“ç·´æ™‚é–“ç´„ç‚º 67 åˆ†é˜ï¼Œè¨“ç·´ä½¿ç”¨çš„è™•ç†å™¨ç‚º i7-8750Hï¼Œå¯ä»¥åƒè€ƒä¸€ä¸‹ã€‚ è¨“ç·´å®Œæˆå¾Œæœƒç”¢ç”Ÿä¸€å¼µåœ–q3_word_vectors.pngï¼š q3_word_vectors.png åœ¨q3_run.pyä¸­å¯ä»¥ç™¼ç¾åœ¨åŸå§‹ç¨‹å¼ç¢¼ä¸­ï¼ŒæŒ‘é¸äº†ä¸€äº›è©ä¾†é€²è¡Œè¦–è¦ºåŒ–ï¼Œé€é SVD é™ç¶­å¾Œä½¿å¾—èƒ½å¤ åœ¨äºŒç¶­å¹³é¢ä¸Šè§€å¯Ÿé€™äº›è©çš„ç›¸å°è·é›¢ã€‚ (h) å¯¦ä½œ CBOW Implement the CBOW model in q3_word2vec.py. Note: This part is optional but the gradient derivations for CBOW in part (d) are not!. solution åŒæ¨£åˆ©ç”¨ (d) å°é¡Œçš„çµæœï¼š gradOut: \(\dfrac{\partial J\_{CBOW}(w\_{t-m...t+m})}{\partial U}=\dfrac{\partial F(w_t,\hat{v})}{\partial U}\) gradIn: \(\dfrac{\partial J\_{CBOW}(w\_{t-m...t+m})}{\partial v\_{w\_{t+j}}}=\dfrac{\partial F(w_t,\hat{v})}{\partial \hat{v}},\forall j \in \{-m,...,-1,1,...,m\}\) gradIn: \(\dfrac{\partial J\_{CBOW}(w\_{t-m...t+m})}{\partial U}=0,\forall j \notin \{-m,...,-1,1,...,m\}\) 1234567891011121314151617def cbow(currentWord, C, contextWords, tokens, inputVectors, outputVectors, dataset, word2vecCostAndGradient=softmaxCostAndGradient): cost = 0.0 gradIn = np.zeros(inputVectors.shape) gradOut = np.zeros(outputVectors.shape) ### YOUR CODE HERE indices = [tokens[contextWord] for contextWord in contextWords] v_hat = np.sum(inputVectors[indices], axis=0) target_index = tokens[currentWord] cost, grad_in, gradOut = word2vecCostAndGradient(v_hat, target_index, outputVectors, dataset) for j in indices: gradIn[j] += grad_in ### END YOUR CODE return cost, gradIn, gradOut åŸ·è¡Œpython q3_word2vec.pyï¼Œåœ¨å®Œæˆ (e)(h) å°é¡Œå¾Œçš„çµæœæ‡‰å¦‚ä¸‹ï¼š 1234567891011Testing normalizeRows...[[ 0.6 0.8 ] [ 0.4472136 0.89442719]]==== Gradient check for skip-gram ====Gradient check passed!Gradient check passed!==== Gradient check for CBOW ====Gradient check passed!Gradient check passed! 4. Sentiment Analysis Now, with the word vectors you trained, we are going to perform a simple sentiment analysis. For each sentence in the Stanford Sentiment Treebank dataset, we are going to use the average of all the word vectors in that sentence as its feature, and try to predict the sentiment level of the said sentence. The sentiment level of the phrases are represented as real values in the original dataset, here weâ€™ll just use five classes: â€œvery negative (âˆ’âˆ’)â€, â€œnegative (âˆ’)â€, â€œneutralâ€, â€œpositive (+)â€, â€œvery positive (++)â€ which are represented by 0 to 4 in the code, respectively. For this part, you will learn to train a softmax classifier, and perform train/dev validation to improve generalization. (a) ä»¥ç‰¹å¾µå‘é‡ä¾†è¡¨ç¤ºä¸€å€‹å¥å­ Implement a sentence featurizer. A simple way of representing a sentence is taking the average of the vectors of the words in the sentence. Fill in the implementation in q4_sentiment.py. solution å°‡å¥å­ä¸­æ‰€æœ‰è©çš„è©å‘é‡é€²è¡Œå¹³å‡ï¼Œå°‡å¾—åˆ°çš„å‘é‡ç”¨ä¾†è¡¨ç¤ºæ•´å€‹å¥å­ã€‚ Inputs: tokens: ä¸€å€‹ dictionaryï¼Œkey ç‚ºè©ï¼Œvalue ç‚ºè©²è©çš„ç´¢å¼• wordVectors: è©å‘é‡ sentence: ä¸€å€‹å¥å­ Output: sentVector: å¥å­çš„ç‰¹å¾µå‘é‡ 12345678910111213def getSentenceFeatures(tokens, wordVectors, sentence): sentVector = np.zeros((wordVectors.shape[1],)) ### YOUR CODE HERE for word in sentence: index = tokens[word] sentVector += wordVectors[index] sentVector /= float(len(sentence)) ### END YOUR CODE assert sentVector.shape == (wordVectors.shape[1],) return sentVector (b) æ­£è¦åŒ– (Regularization) Explain in at most two sentences why we want to introduce regularization when doing classification (in fact, most machine learning tasks). solution æ­£è¦åŒ–å¯ä»¥é¿å… overfittingï¼Œä½¿å¾—æ¨¡å‹æ›´åŠ  generalizationã€‚ (c) å°‹æ‰¾è¶…åƒæ•¸ (hyperparameter) Fill in the hyperparameter selection code in q4_sentiment.py to search for the â€œoptimalâ€ regularization parameter. You need to implement both getRegularizationValues and chooseBestModel. Attach your code for chooseBestModel to your written write-up. You should be able to attain at least 36.5% accuracy on the dev and test sets using the pretrained vectors in part (d). solution åƒè€ƒå…¶ä»–äººçš„åšæ³•ï¼Œä½¿ç”¨ log function ç”¢ç”Ÿéå¢çš„å€¼ã€‚ 123456def getRegularizationValues(): values = None # Assign a list of floats in the block below ### YOUR CODE HERE values = np.logspace(-5, 2, num=100, base=10) ### END YOUR CODE return sorted(values) 12345678def chooseBestModel(results): bestResult = None ### YOUR CODE HERE bestResult = max(results, key=lambda x: x["dev"]) ### END YOUR CODE return bestResult (d) æ¨¡å‹æ¯”è¼ƒ Run python q4_sentiment.py --yourvectors to train a model using your word vectors from q3. Now, run python q4_sentiment.py --pretrained to train a model using pretrained GloVe vectors (on Wikipedia data). Compare and report the best train, dev, and test accuracies. Why do you think the pretrained vectors did better? Be specific and justify with 3 distinct reasons. yourvectors Reg Train Dev Test 1.00E-05 31.004 32.516 30.452 1.15E-04 31.063 32.516 30.362 1.12E-03 31.133 32.516 30.362 1.10E-02 30.922 32.334 29.955 1.07E-01 30.290 31.789 29.864 1.05E+00 28.816 29.609 27.059 12Best regularization value: 2.26E-05Test accuracy (%): 30.316742 pretrained GloVe vectors (on Wikipedia data) Reg Train Dev Test 1.00E-05 39.923 36.421 37.059 1.15E-04 39.958 36.512 37.014 1.12E-03 39.899 36.331 37.014 1.10E-02 39.923 36.331 37.195 1.07E-01 39.782 36.240 37.149 1.05E+00 39.478 36.512 37.330 12Best regularization value: 1.20E+01Test accuracy (%): 37.556561 è§€å¯Ÿå…©è€…çš„è¡¨ç¾å¯ä»¥ç™¼ç¾ä½¿ç”¨ pretrained çš„æ¨¡å‹æ•ˆæœæ˜é¡¯æ¯”è¼ƒå¥½ï¼Œå¯èƒ½åŸå› å¦‚ä¸‹ï¼š æ›´é«˜ç¶­åº¦çš„è©å‘é‡å¯èƒ½åŒ…å«æ›´å¤šçš„è³‡è¨Š GloVe ä½¿ç”¨æ›´å¤§çš„èªæ–™åº«é€²è¡Œè¨“ç·´ word2vec èˆ‡ GloVe çš„å·®ç•° (åƒè€ƒè³‡æ–™) (e) Regularization Term çš„è®ŠåŒ–åœ¨ train set åŠ dev set çš„è¡¨ç¾å·®ç•° Plot the classification accuracy on the train and dev set with respect to the regularization value for the pretrained GloVe vectors, using a logarithmic scale on the x-axis. This should have been done automatically. Include q4_reg acc.png in your homework write up. Briefly explain in at most three sentences what you see in the plot. solution (f) Confusion Matrix We will now analyze errors that the model makes (with pretrained GloVe vectors). When you ran python q4_sentiment.py --pretrained, two files should have been generated. Take a look at q4_dev_conf.png and include it in your homework writeup. Interpret the confusion matrix in at most three sentences. solution ç”±é æ¸¬çµæœèˆ‡æ¨™æº–ç­”æ¡ˆçš„æ•¸é‡æ¯”è¼ƒæ‰€ç”¢ç”Ÿçš„ confusion matrixï¼Œç”±å·¦ä¸Šåˆ°å³ä¸‹çš„å°è§’ç·šæ‰€ç¶“éçš„æ ¼å­ç‚º True Positive çš„æ•¸é‡ã€‚ (g) ç‚ºä½•åˆ†é¡éŒ¯èª¤ï¼Ÿ Next, take a look at q4_dev_pred.txt. Choose 3 examples where your classifier made errors and briefly explain the error and what features the classifier would need to classify the example correctly (1 sentence per example). Try to pick examples with different reasons. solution å°‡æ¯ç­†åˆ†é¡çµæœèˆ‡æ¨™æº–ç­”æ¡ˆçš„å·®è·å–çµ•å°å€¼ï¼Œçµ±è¨ˆå¦‚ä¸‹ï¼š å·®è· 0 1 2 3 4 æ•¸é‡ 409 444 188 59  1 å¯ä»¥çœ‹åˆ°å…¶å¯¦å¤§éƒ¨åˆ†çš„èª¤å·®éƒ½åœ¨ 1 åˆ†ä»¥å…§ï¼Œè‹¥å°‡æ¨™æº–é™ä½ç‚ºå·®è· 1 åˆ†æˆ–ä»¥ä¸‹å°±ç®—åˆ†é¡æ­£ç¢ºï¼Œæº–ç¢ºç‡æœƒå¾åŸä¾†çš„ 37% å¤§å¹…æé«˜åˆ° 77%ï¼Œå› æ­¤æ•´é«”çš„é æ¸¬å…¶å¯¦é‚„ç®—æ˜¯æº–ç¢ºçš„ã€‚ Answer Predicted Sentence Possible Issue 0 4 a lackluster , unessential sequel to the classic disney adaptation of j.m. barrie 's peter pan . å”¯ä¸€ä¸€ç­†é æ¸¬èˆ‡å¯¦éš›åˆ†æ•¸å·®è·4åˆ†çš„è³‡æ–™ï¼Œçœ‹èµ·ä¾† lackluster, unessential éƒ½æ˜¯è² é¢çš„è©å½™ï¼Œclassic å‰‡æ˜¯åå‘æ­£é¢çš„è©å½™ï¼Œè€Œ j.m. barrie 's peter pan å‰‡æ¯”è¼ƒç®—æ˜¯é›œè¨Šï¼Œå…¶é¤˜å‰‡æ˜¯ç¨å¾®ä¸­æ€§çš„è©èªï¼Œè‹¥å°‡å°ˆæœ‰åè©å»æ‰æˆ–è¨±èƒ½æå‡é æ¸¬çš„åˆ†æ•¸ 4 1 the draw -lrb- for `` big bad love '' -rrb- is a solid performance by arliss howard . å¤ªå¤šç„¡æ„ç¾©çš„ç¬¦è™Ÿå¹²æ“¾ 4 1 it is amusing , and that 's all it needs to be . amusing æ‡‰è©²æ¯”è¼ƒåå‘æ­£é¢çš„è©å½™ï¼Œä½†æ˜¯å…¶é¤˜çš„è©æ„Ÿè¦ºå¤§éƒ¨åˆ†éƒ½æ˜¯å†—è©ã€‚]]></content>
      <categories>
        <category>å­¸æ ¡èª²ç¨‹</category>
        <category>è‡ªç„¶èªè¨€è™•ç†</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode#204 Count Primes]]></title>
    <url>%2F2018%2F09%2F10%2FLeetCode-204-Count-Primes%2F</url>
    <content type="text"><![CDATA[å•é¡Œ ç¶“å…¸å•é¡Œï¼Œçµ¦ä¸€å€‹éè² æ•´æ•¸ n ï¼Œå•å°æ–¼ n çš„è³ªæ•¸æœ‰å¹¾å€‹ã€‚ è¼¸å…¥èˆ‡è¼¸å‡º Input: 10 Output: 4 Explanation: There are 4 prime numbers less than 10, they are 2, 3, 5, 7. æ–¹æ³• å»ºè³ªæ•¸è¡¨ã€‚ æ–¹æ³•ä¸€ï¼š é€ä¸€æª¢æŸ¥ 1 ~ n - 1 çš„æ•´æ•¸ä¸­æœ‰å¤šå°‘å€‹è³ªæ•¸ï¼Œä½†æ•ˆç‡ä¸å¥½ï¼Œå› æ­¤æ¡ç”¨ç¬¬äºŒç¨®æ–¹æ³•ã€‚ æ–¹æ³•äºŒï¼š åŸƒæ‹‰æ‰˜æ–¯ç‰¹å°¼ç¯©æ³•ï¼Œå¾ 2 é–‹å§‹å°‡å·²çŸ¥è³ªæ•¸çš„å€æ•¸æ¨™è¨˜æˆåˆæ•¸ï¼Œå¯ä»¥æ¸›å°‘å¾ˆå¤šä¸å¿…è¦çš„è¨ˆç®—ã€‚ 123456789101112131415161718int countPrimes(int n)&#123; int i, j, prime_count = 0; bool* primes = malloc(n * sizeof(bool)); memset(primes, true, n * sizeof(bool)); for (i = 2; i &lt; n; i++) &#123; if (primes[i]) &#123; prime_count++; for (j = 2 * i; j &lt; n; j += i) primes[j] = false; &#125; &#125; return prime_count;&#125;]]></content>
      <categories>
        <category>ç¨‹å¼è§£é¡Œ</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>C</tag>
        <tag>Prime</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode#198 House Robber]]></title>
    <url>%2F2018%2F09%2F10%2FLeetCode-198-House-Robber%2F</url>
    <content type="text"><![CDATA[å•é¡Œ æœ‰ä¸€å€‹è·æ¥­ç›œè³Šæƒ³è¦åœ¨æŸæ¢è¡—ä¸Šè¡Œç«Šï¼Œé€™æ¢è¡—ä¸Šæœ‰è‘—ä¸€æ•´æ’çš„æˆ¿å±‹ï¼Œæ¯æ£Ÿæˆ¿å±‹å…§éƒ½æœ‰ç‰¹å®šæ•¸é‡çš„é‡‘éŒ¢ï¼Œä»–å¸Œæœ›èƒ½å¤ å·åˆ°è¶Šå¤šéŒ¢è¶Šå¥½ã€‚ä½†æ˜¯æƒ³åœ¨é€™æ¢è¡—è¡Œç«Šæœ‰å€‹é™åˆ¶ï¼Œæ¯å…©æ£Ÿç›¸é„°çš„æˆ¿å±‹ä¹‹é–“éƒ½æœ‰é€£çµä¿å…¨ç³»çµ±ï¼Œè‹¥ä»–è¡Œç«Šå…©æ£Ÿç›¸é„°çš„æˆ¿å±‹ï¼Œä¿å…¨ç³»çµ±å°±æœƒè‡ªå‹•è¯ç¹«è­¦å¯Ÿã€‚ çµ¦å®šä¸€å€‹éè² æ•´æ•¸çš„listï¼Œè¡¨ç¤ºæ¯é–“æˆ¿å±‹å…§çš„é‡‘éŒ¢æ•¸é‡ï¼Œè«‹å•åœ¨ä¸é©šå‹•ä¿å…¨ç³»çµ±çš„å‰æä¸‹ï¼Œç«Šè³Šèƒ½å¤ å·åˆ°æœ€å¤šçš„é‡‘éŒ¢æ•¸é‡æ˜¯å¤šå°‘ï¼Ÿ è¼¸å…¥èˆ‡è¼¸å‡º Input: [1,2,3,1] Output: 4 Explanation: Rob house 1 (money = 1) and then rob house 3 (money = 3). Total amount you can rob = 1 + 3 = 4. Input: [2,7,9,3,1] Output: 12 Explanation: Rob house 1 (money = 2), rob house 3 (money = 9) and rob house 5 (money = 1). Total amount you can rob = 2 + 9 + 1 = 12. æ–¹æ³• ä½¿ç”¨å‹•æ…‹è¦åŠƒï¼Œå‡è¨­ an ç‚ºè¡Œç«Šç¬¬ n æ£Ÿæˆ¿å±‹èƒ½å¤ ç²å¾—çš„é‡‘éŒ¢æ•¸é‡ï¼Œå¯å¾— an = max(an-2, an-3)ã€‚ 1234567891011121314151617181920212223242526int max(int a, int b)&#123; if (a &gt; b) return a; else return b;&#125;int rob(int* nums, int numsSize) &#123; if (numsSize == 0) return 0; int i; if (numsSize &gt;= 3) nums[2] += nums[0]; for (i = 3; i &lt; numsSize; i++) nums[i] += max(nums[i - 2], nums[i - 3]); if (numsSize == 1) return nums[0]; else if (numsSize == 2) return max(nums[0], nums[1]); else return max(nums[i - 1], nums[i - 2]);&#125;]]></content>
      <categories>
        <category>ç¨‹å¼è§£é¡Œ</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>C</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æŒ‘æˆ°LeetCode Weekly Contest 50]]></title>
    <url>%2F2017%2F09%2F23%2F%E6%8C%91%E6%88%B0LeetCode-Weekly-Contest-50%2F</url>
    <content type="text"><![CDATA[è§£é¡Œå¿ƒå¾— é€™æ¬¡çš„é¡Œç›®æœ‰å…©é¡Œéƒ½å¯ä»¥ç”¨éè¿´ä¾†å¯«ï¼Œä½†å°æˆ‘ä¾†èªªè…¦è¢‹çœŸçš„è½‰ä¸éä¾†å•Šï¼ æœç„¶å¦‚ä¿—è©±æ‰€èªªï¼šã€Œéè¿´åªæ‡‰å¤©ä¸Šæœ‰ï¼Œå‡¡äººæ‡‰ç•¶ç”¨è¿´åœˆã€ï¼Œæè¿°çš„çœŸæ˜¯è²¼åˆ‡ï¼Œä½†éè¿´ä¹‹ç¾ï¼Œç¸½æ˜¯è®“æˆ‘å€‘é€™äº›å‡¡äººæµé€£å¿˜è¿”ï¼Œéè¿´å•Šéè¿´ï¼Œç©¶ç«Ÿä½•æ™‚æ‰èƒ½è®“é¡˜æ„äººå€‘çœŸæ­£çœ‹æ¸…æ‚¨çš„çœŸé¢ç›®å‘¢ï¼Ÿ ç¬¬1é¡Œ 680. Valid Palindrome II å•é¡Œ çµ¦ä¸€å€‹éç©ºçš„å­—ä¸²sï¼Œåœ¨æœ€å¤šåªèƒ½åˆªé™¤ä¸€å€‹å­—å…ƒçš„æ¢ä»¶ä¸‹ï¼Œsæ˜¯å¦èƒ½æˆç‚ºä¸€å€‹å›æ–‡å­—ä¸²ã€‚ å­—ä¸²åªåŒ…å«å°å¯«å­—æ¯a-zï¼Œå­—ä¸²é•·åº¦ä¸è¶…é50000ã€‚ è¼¸å…¥èˆ‡è¼¸å‡º Input: &quot;aba&quot; Output: True Input: &quot;abca&quot; Output: True Explanation: You could delete the character &#39;c&#39;. æ–¹æ³• ä½¿ç”¨two pointerçš„æŠ€å·§ï¼Œlowèˆ‡highåˆ†åˆ¥å¾å‰é¢èˆ‡å¾Œé¢é–‹å§‹èµ°ï¼Œè‹¥é‡åˆ°ä¸ä¸€æ¨£çš„å­—å…ƒï¼Œå‰‡æª¢æŸ¥åœ¨åˆªé™¤å…¶ä¸­ä»»ä¸€å€‹å­—å…ƒå¾Œæ˜¯å¦èƒ½æ»¿è¶³å›æ–‡çš„æ¢ä»¶ã€‚ 1234567891011121314151617181920212223class Solution: def validPalindrome(self, s): low = 0 high = len(s) - 1 while low &lt; high: if s[low] == s[high]: low += 1 high -= 1 else: if self.isPalindrome(s, low+1, high): return True if self.isPalindrome(s, low, high-1): return True return False return True def isPalindrome(self, s, low, high): while low &lt; high: if s[low] != s[high]: return False low += 1 high -= 1 return True ç¬¬2é¡Œ 677. Map Sum Pairs å•é¡Œ å¯¦ä½œä¸€å€‹å­—å…¸ï¼Œèƒ½å¤ æ’å…¥ä¸€å°(key, value)ï¼Œå…¶ä¸­keyç‚ºå­—ä¸²ï¼Œvalueç‚ºæ•´æ•¸ã€‚ è¼¸å…¥ä¸€å€‹å­—ä¸²prefixï¼Œè¼¸å‡ºå­—å…¸ä¸­æ‰€æœ‰ä»¥prefixç‚ºé–‹é ­çš„æ‰€æœ‰keyæ‰€å°æ‡‰çš„valueçš„å’Œã€‚ è¼¸å…¥èˆ‡è¼¸å‡º Input: &quot;aba&quot; Output: True Input: &quot;abca&quot; Output: True Explanation: You could delete the character &#39;c&#39;. æ–¹æ³• ç›´æ¥ä½¿ç”¨pythonçš„å…§å»ºdictã€‚ 1234567891011121314151617class MapSum: def __init__(self): self.d = dict() def insert(self, key, val): self.d[key] = val def sum(self, prefix): summation = 0 length = len(prefix) for k, v in self.d.items(): if k[:length] == prefix: summation += v return summation ç¬¬3é¡Œ 678. Valid Parenthesis String å•é¡Œ è¼¸å…¥ä¸€å€‹å­—ä¸²ï¼Œå­—ä¸²å…§çš„å…ƒç´ åªåŒ…å«(,),*ä¸‰ç¨®ï¼Œåˆ¤æ–·æ­¤å­—ä¸²æ˜¯å¦æ»¿è¶³ä»¥ä¸‹æ¢ä»¶ï¼š 1. ä»»ä½•ä¸€å€‹å·¦æ–¹çš„(éƒ½å¿…é ˆå°æ‡‰åˆ°ä¸€å€‹å³æ–¹çš„) 2. ä»»ä½•ä¸€å€‹å³æ–¹çš„)éƒ½å¿…é ˆå°æ‡‰åˆ°ä¸€å€‹å·¦æ–¹çš„( 3. å·¦æ–¹çš„(å¿…é ˆå‡ºç¾åœ¨å³æ–¹çš„)ä¹‹å‰ 4. *å¯ä»¥æ˜¯(æˆ–)æˆ–ç©ºå­—ä¸²å…¶ä¸­ä¸€ç¨® 5. ç©ºå­—ä¸²æ˜¯åˆæ³•çš„ è¼¸å…¥èˆ‡è¼¸å‡º Input: &quot;()&quot; Output: True Input: &quot;(*)&quot; Output: True Input: &quot;(*))&quot; Output: True æ–¹æ³• ä½¿ç”¨ä¸€å€‹è®Šæ•¸pç´€éŒ„æ‹¬è™Ÿçš„æ¶ˆé•·ï¼Œå‡ºç¾ä¸€çµ„(,)ä¾¿å¯æŠµæ¶ˆã€‚ è‡³æ–¼å‡ºç¾*çš„è©±ï¼Œå› ç‚ºæœ‰ä¸‰ç¨®å¯èƒ½ï¼Œå› æ­¤ç”¨backtrackingåˆ†æ”¯æˆä¸‰ç¨®å¯èƒ½ï¼Œåªè¦å…¶ä¸­ä¸€ç¨®èƒ½å¤ æ»¿è¶³æ¢ä»¶å°±ä»£è¡¨åˆæ³•ã€‚ 123456789101112131415161718class Solution: def checkValidString(self, s): return self.backtracking(list(s), 0, 0) def backtracking(self, c, p, index): if p &lt; 0: return False for i in range(index, len(c)): x = c[i] if x == '(': p += 1 elif x == ')': if p &lt;= 0: return False p -= 1 else: return self.backtracking(c, p + 1, i + 1) or self.backtracking(c, p - 1, i + 1) or self.backtracking(c, p, i + 1) return p == 0 ä»¥ä¸Šæ˜¯ä½¿ç”¨pythonçš„è§£æ³•ï¼Œå¯æƒœè·³å‡º TLEï¼Œå› æ­¤å°‡åŒæ¨£çš„æ–¹æ³•æ”¹å¯«æˆC++çš„ç‰ˆæœ¬ï¼Œå°±å¯ä»¥é€šéäº†ã€‚ 123456789101112131415161718192021222324class Solution &#123;public: bool checkValidString(string s) &#123; return backtracking(s, 0, 0); &#125; bool backtracking(string c, int p, int index) &#123; if (p &lt; 0) return false; for (int i = index; i &lt; c.length(); i++) &#123; char x = c[i]; if (x == '(') p++; else if (x == ')') &#123; if (p &lt;= 0) return false; p--; &#125; else return backtracking(c, p+1, i+1) || backtracking(c, p-1, i+1) || backtracking(c, p, i+1); &#125; return p == 0; &#125;&#125;; ç¬¬4é¡Œ 679. 24 Game å•é¡Œ çµ¦å››å€‹ç¯„åœ1~9çš„æ•´æ•¸ï¼Œå•æ˜¯å¦èƒ½ç¶“é+,-,*,/,(,)çš„é‹ç®—å¾Œçš„å¾—åˆ°çµæœç‚º24ã€‚ è¼¸å…¥èˆ‡è¼¸å‡º Input: [4, 1, 8, 7] Output: True Explanation: (8-4) * (7-1) = 24 Input: [1, 2, 1, 2] Output: False æ–¹æ³• ç¸½å…±æœ‰ 4 å€‹æ•¸å­—ä»¥åŠ 4 å€‹é‹ç®—å­ï¼Œæ‹¬è™Ÿçš„éƒ¨ä»½ä»£è¡¨é‹ç®—çš„å…ˆå¾Œé †åºï¼Œå› æ­¤è¨ˆç®—çš„æ­¥é©Ÿå¦‚ä¸‹ï¼š å¾ 4 å€‹æ•¸å­—ä¸­å– 2 å€‹(é †åºæœ‰åˆ†)ï¼Œå†å¾ 4 å€‹é‹ç®—å­ç•¶ä¸­é¸ 1 å€‹ï¼Œæ‰€ä»¥æœ‰ 4 x 5 x 3 ç¨®é¸æ³• ç¬¬ 1 æ­¥é©Ÿé‹ç®—å®Œå¾Œæ•¸å­—å‰©ä¸‹ 3 å€‹ï¼Œå†å¾ 3 å€‹æ•¸å­—ä¸­å– 2 å€‹(é †åºæœ‰åˆ†)ï¼ŒåŒæ¨£å¾ 4 å€‹é‹ç®—å­ç•¶ä¸­é¸ 1 å€‹ï¼Œæ‰€ä»¥æœ‰ 3 x 2 x 4 ç¨®é¸æ³• ç¬¬ 2 æ­¥é©Ÿé‹ç®—å®Œå¾Œæ•¸å­—å‰©ä¸‹ 2 å€‹ï¼Œå†å¾ 2 å€‹æ•¸å­—ç•¶ä¸­å– 2 å€‹(é †åºæœ‰åˆ†)ï¼Œå¾ 4 å€‹é‹ç®—å­ä¸­é¸ä¸€å€‹ï¼Œæ‰€ä»¥æœ‰ 2 x 4 ç¨®é¸æ³• å› æ­¤ç¸½å…±æœ‰ 4 x 3 x 3 x 3 x 2 x 4 x 2 x 4 = 9216 ç¨®çµ„åˆæ–¹æ³• æ‰€ä»¥ç”¨éè¿´æšèˆ‰å‡ºæ‰€æœ‰çš„æ–¹æ³•å°±å¯ä»¥å°‡æ‰€æœ‰ç­”æ¡ˆè¨ˆç®—å‡ºä¾†ã€‚è¦æ³¨æ„çš„æ˜¯é™¤æ•¸ä¸èƒ½ç‚º 0 ï¼Œé‚„æœ‰ç”±æ–¼æµ®é»æ•¸é‹ç®—çš„èª¤å·®ï¼Œå› æ­¤è¨ˆç®—çµæœæœªå¿…å‰›å¥½ç­‰æ–¼ 24 ï¼Œå¯èƒ½æœƒæœ‰éå¸¸å¾®å°çš„èª¤å·®å€¼ï¼Œæ‰€ä»¥è¦çµ¦ä¸€å€‹å®¹å¿å€¼æ‰è¡Œã€‚ 1234567891011121314class Solution(object): def judgePoint24(self, A): if len(A) == 1: if abs(A[0] - 24) &lt; 1e-6: return True for i in range(len(A)): for j in range(len(A)): if i != j: B = [A[x] for x in range(len(A)) if x != i and x != j] if self.judgePoint24([A[i] + A[j]] + B) or self.judgePoint24([A[i] - A[j]] + B) or self.judgePoint24([A[i] * A[j]] + B): return True if A[j] and self.judgePoint24([A[i] / A[j]] + B): return True return False]]></content>
      <categories>
        <category>ç¨‹å¼è§£é¡Œ</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>Python</tag>
        <tag>LeetCode Weekly Contest</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æŒ‘æˆ°LeetCode Weekly Contest 49]]></title>
    <url>%2F2017%2F09%2F10%2F%E6%8C%91%E6%88%B0LeetCode-Weekly-Contest-49%2F</url>
    <content type="text"><![CDATA[ä¸‰åº¦æŒ‘æˆ° é€™æ¬¡çš„é¡Œç›®ï¼Œå‰å…©é¡ŒçœŸçš„ç°¡å–®ï¼Œè‡³æ–¼å¾Œå…©é¡Œï¼ŒçœŸå¿ƒé›£QQ ï¼Œæœ€çµ‚æˆç¸¾ 753 / 2362ã€‚ ç¬¬1é¡Œ 674. Longest Continuous Increasing Subsequence å•é¡Œ çµ¦ä¸€å€‹æœªæ’åºçš„æ•´æ•¸é™£åˆ—ï¼Œæ‰¾å‡ºæœ€é•·çš„é€£çºŒéå¢å­åºåˆ—çš„é•·åº¦ã€‚ è¼¸å…¥ç¯„ä¾‹ 1 [1,3,5,4,7] è¼¸å‡ºç¯„ä¾‹ 1 3 è¼¸å…¥ç¯„ä¾‹ 2 [2,2,2,2,2] è¼¸å‡ºç¯„ä¾‹ 2 1 æ–¹æ³• éå¢åºåˆ—çš„ç‰¹æ€§å°±æ˜¯è¼ƒæ™šå‡ºç¾çš„å…ƒç´ ä¸€å®šå¤§æ–¼è¼ƒå…ˆå‡ºç¾çš„å…ƒç´ ï¼Œéæ­·æ•´å€‹é™£åˆ—ï¼Œå› ç‚ºè¦æ±‚ç‚ºã€Œé€£çºŒã€ï¼Œæ‰€ä»¥åªè¦é‡åˆ°ä¸ç¬¦åˆéå¢æ¢ä»¶çš„å…ƒç´ ï¼Œå°±è¦é‡æ–°è¨ˆç®—ã€‚ 1234567891011121314class Solution(object): def findLengthOfLCIS(self, nums): if not nums: return 0 maximum = 1 current = 1 for i in range(1, len(nums)): if nums[i - 1] &lt; nums[i]: current += 1 else: maximum = max(maximum, current) current = 1 maximum = max(maximum, current) return maximum ç¬¬2é¡Œ 676. Implement Magic Dictionary å•é¡Œ å®šç¾©ä¸€å€‹é™£åˆ—ï¼Œç”±è¨±å¤šå­—ä¸²çµ„æˆï¼Œè¼¸å…¥ä¸€å€‹å­—ä¸²ï¼Œè‹¥é™£åˆ—ä¸­ä»»ä¸€å­—ä¸²æ›´æ”¹ä¸€å€‹å­—æ¯å¾Œï¼Œèˆ‡è¼¸å…¥çš„å­—ä¸²ç›¸åŒï¼Œå›å‚³Trueï¼Œåä¹‹å‰‡å›å‚³Falseã€‚ è¼¸å…¥èˆ‡è¼¸å‡º Input: buildDict([&quot;hello&quot;, &quot;leetcode&quot;]), Output: Null Input: search(&quot;hello&quot;), Output: False Input: search(&quot;hhllo&quot;), Output: True Input: search(&quot;hell&quot;), Output: False Input: search(&quot;leetcoded&quot;), Output: False æ–¹æ³• é€™é¡Œæ²’æœ‰ä»€éº¼ç‰¹æ®Šçš„æŠ€å·§ï¼Œåªè¦å°‡è¼¸å…¥å­—ä¸²èˆ‡é™£åˆ—å…§çš„å­—ä¸²ä¸€ä¸€æ¯”å°å³å¯ã€‚ 1234567891011121314151617181920class MagicDictionary(object): def __init__(self): self.magic = [] def buildDict(self, dic): self.magic = dic def search(self, word): for m in self.magic: modify = 0 if len(m) == len(word): for i in range(len(m)): if m[i] != word[i]: modify += 1 if modify == 1: return True return False ## ç¬¬3é¡Œ 675. Cut Off Trees for Golf Event ## ### å•é¡Œ ### è¼¸å…¥ä¸€å€‹äºŒç¶­é™£åˆ—ï¼Œ0ä»£è¡¨ç„¡æ³•é€šéçš„éšœç¤™ç‰©ï¼Œ1ä»£è¡¨å¯ä»¥é€šè¡Œçš„è‰çš®ï¼Œå¤§æ–¼1çš„æ•¸ä»£è¡¨ä¸€æ£µæ¨¹ï¼ŒåŒæ¨£å¯ä»¥é€šè¡Œï¼Œæ•¸å­—å¤§å°ä»£è¡¨æ¨¹çš„é«˜åº¦ã€‚ ä»Šå¤©æˆ‘å€‘è¦æ ¹æ“šæ¨¹çš„é«˜åº¦ç”±ä½åˆ°é«˜é€²è¡Œä¿®å‰ªï¼Œç¶“éä¿®å‰ªå¾Œçš„æ¨¹æœƒè®Šæˆè‰çš®ï¼Œæˆ‘å€‘è¦å¾(0, 0)å‡ºç™¼ï¼Œä¾åºèµ°è¨ªæ‰€æœ‰çš„æ¨¹ï¼Œè¼¸å‡ºä¿®å‰ªæ‰€æœ‰çš„æ¨¹æ‰€éœ€ç¶“éæœ€çŸ­çš„è·é›¢ï¼Œè‹¥æœ‰ä»»ä½•ä¸€æ£µæ¨¹ç„¡æ³•æŠµé”ï¼Œå‰‡è¼¸å‡º-1ã€‚ ### è¼¸å…¥èˆ‡è¼¸å‡º ### Input: [ [1,2,3], [0,0,4], [7,6,5]] Output: 6 Input: [ [1,2,3], [0,0,0], [7,6,5]] Output: -1 Input: [ [2,3,4], [0,0,5], [8,7,6]] Output: 6 Explanation: You started from the point (0,0) and you can cut off the tree in (0,0) directly without walking. ### æ–¹æ³• ### æˆ‘å…ˆå°‡æ‰€æœ‰é0æˆ–1çš„æ•¸åŠ å…¥ä¸€å€‹é™£åˆ—ä¸­ï¼Œå°‡å…¶æ’åºå¾Œç”±å°è‡³å¤§å–å‡ºï¼Œåˆ©ç”¨BFSè¨ˆç®—å¾ç›®å‰ä½ç½®åˆ°å„é»çš„æœ€çŸ­è·é›¢ï¼Œå–å‡ºç›®å‰ä½ç½®åˆ°æ¬²ä¿®å‰ªçš„è·é›¢ã€‚ å¾ˆä¸å¹¸çš„æˆ‘çš„æ–¹æ³•åƒäº†ä¸€ç™¼ TLE ï¼Œæ‡‰è©²æ˜¯åšçš„ BFS æœ‰å¤ªå¤šå¤šé¤˜çš„ç¯„åœäº†ï¼Œæ‡‰è©²åªè¦æ‰¾å‡ºç›®å‰ä½ç½®åˆ°ç›®çš„åœ°çš„æœ€çŸ­è·é›¢å°±å¥½ï¼Œä¹‹å¾Œè‹¥æœ‰æ™‚é–“å†æƒ³è¾¦æ³•è£œä¸Šæ–°çš„æ–¹æ³•å§ã€‚ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354class Solution(object): def cutOffTree(self, forest): import math self.forest = forest self.r, self.c = len(self.forest), len(self.forest[0]) f = [col for row in forest for col in row if col != 0 and col != 1] f.sort(reverse=True) total_step = 0 cur_pos = (0, 0) while f: self.num_step = [[math.inf for j in range(self.c)] for i in range(self.r)] self.num_step[cur_pos[0]][cur_pos[1]] = 0 self.min_step(cur_pos) next_cut = f.pop() next_pos = self.find_pos(next_cut) if not self.num_step[next_pos[0]][next_pos[1]] &lt; math.inf: return -1 total_step += self.num_step[next_pos[0]][next_pos[1]] cur_pos = next_pos return total_step def min_step(self, pos): i, j = pos[0], pos[1] # print(i, j, self.num_step[i][j]) if not (0 &lt;= i &lt; self.r and 0 &lt;= j &lt; self.c): return if 0 &lt;= i - 1 &lt; self.r and 0 &lt;= j &lt; self.c and self.forest[i - 1][j]: if self.num_step[i - 1][j] &gt;= self.num_step[i][j] + 1: self.num_step[i - 1][j] = self.num_step[i][j] + 1 self.min_step((i - 1, j)) if 0 &lt;= i + 1 &lt; self.r and 0 &lt;= j &lt; self.c and self.forest[i + 1][j]: if self.num_step[i + 1][j] &gt;= self.num_step[i][j] + 1: self.num_step[i + 1][j] = self.num_step[i][j] + 1 self.min_step((i + 1, j)) if 0 &lt;= i &lt; self.r and 0 &lt;= j - 1 &lt; self.c and self.forest[i][j - 1]: if self.num_step[i][j - 1] &gt;= self.num_step[i][j] + 1: self.num_step[i][j - 1] = self.num_step[i][j] + 1 self.min_step((i, j - 1)) if 0 &lt;= i &lt; self.r and 0 &lt;= j + 1 &lt; self.c and self.forest[i][j + 1]: if self.num_step[i][j + 1] &gt;= self.num_step[i][j] + 1: self.num_step[i][j + 1] = self.num_step[i][j] + 1 self.min_step((i, j + 1)) def find_pos(self, num): for i in range(len(self.forest)): if num in self.forest[i]: return (i, self.forest[i].index(num)) return -1 ç¬¬4é¡Œ 673. Number of Longest Increasing Subsequence å•é¡Œ çµ¦ä¸€å€‹æœªæ’åºçš„æ•´æ•¸é™£åˆ—ï¼Œæ‰¾å‡ºæœ€é•·çš„éå¢å­åºåˆ—çš„æ•¸é‡ã€‚ è¼¸å…¥èˆ‡è¼¸å‡º Input: [1,3,5,4,7] Output: 2 Explanation: The two longest increasing subsequence are [1, 3, 4, 7] and [1, 3, 5, 7]. Input: [2,2,2,2,2] Output: 5 Explanation: The length of longest continuous increasing subsequence is 1, and there are 5 subsequences&#39; length is 1, so output 5. æ–¹æ³• è¨ˆç®—LISçš„é•·åº¦æœ‰å…©ç¨®æ€ç¶­æ–¹å¼ï¼Œç¬¬ä¸€ç¨®æ˜¯æ‰¾å‡ºå“ªäº›æ•¸å­—èƒ½æ¥åœ¨nums[i]å¾Œé¢ï¼Œç¬¬äºŒç¨®æ˜¯æ‰¾å‡ºnums[i]èƒ½æ¥åœ¨å“ªäº›æ•¸å­—å¾Œé¢ï¼Œé€™è£¡æ¡ç”¨ç¬¬äºŒç¨®æ–¹æ³•ã€‚ é€™è£¡éœ€è¦ç”¨åˆ°å…©å€‹é™£åˆ—ï¼š 1. LIS: LIS[i]ä»£è¡¨ä»¥nums[i]çµæŸçš„æœ€é•·éå¢å­åºåˆ—çš„é•·åº¦ 2. cnt: cnt[i]ä»£è¡¨ä»¥nums[i]çµæŸçš„æœ€é•·éå¢å­åºåˆ—çš„æ•¸é‡ å‡è¨­nums[i]èƒ½æ¥åœ¨nums[j]å¾Œé¢ï¼Œä»£è¡¨nums[i] &gt; nums[j]ï¼Œé€™è£¡æœ‰ä¸‰ç¨®ç‹€æ³å¯ä»¥è¨è«–ï¼š 1. LIS[i] &gt; LIS[j] + 1 : nums[i]æ¥åœ¨ nums[j]å¾Œé¢æ¯”åŸæœ¬é‚„è¦çŸ­ï¼Œä¸æ¥ 2. LIS[i] = LIS[j] + 1 : nums[i]æ¥åœ¨ nums[j]å¾Œé¢æ¯”è·ŸåŸæœ¬ä¸€æ¨£é•·ï¼Œåªè¦æŠŠæ•¸é‡ç›¸åŠ å°±å¥½ 3. LIS[i] &lt; LIS[j] + 1 : nums[i]æ¥åœ¨ nums[j]å¾Œé¢æ¯”åŸæœ¬é‚„è¦é•·ï¼Œé•·åº¦åŠ 1ï¼Œç¹¼æ‰¿å‰é¢çš„æ•¸é‡ æœ€å¾Œå°‡æ‰€æœ‰LIS[i]ç‚ºæœ€å¤§å€¼æ‰€å°æ‡‰çš„cnt[i]ç›¸åŠ å³ç‚ºç­”æ¡ˆã€‚ 123456789101112131415161718class Solution: def findNumberOfLIS(self, nums): if nums == []: return 0 # åˆå§‹åŒ–é™£åˆ—ï¼Œæ¯å€‹æ•¸å­—æœ¬èº«å°±æ˜¯é•·åº¦ç‚º1çš„LIS LIS, cnt = [1] * len(nums), [1] * len(nums) # nums[i]èƒ½æ¥åœ¨å“ªäº›æ•¸å­—å¾Œé¢ for i in range(1, len(nums)): for j in range(0, i): if nums[i] &gt; nums[j]: if LIS[i] == LIS[j] + 1: cnt[i] += cnt[j] elif LIS[i] &lt; LIS[j] + 1: cnt[i] = cnt[j] LIS[i] = LIS[j] + 1 return sum((y for x, y in zip(LIS, cnt) if x == max(LIS)))]]></content>
      <categories>
        <category>ç¨‹å¼è§£é¡Œ</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>Python</tag>
        <tag>LeetCode Weekly Contest</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[äºŒå…ƒæ¨¹(Binary Tree)]]></title>
    <url>%2F2017%2F09%2F08%2F%E4%BA%8C%E5%85%83%E6%A8%B9-Binary-Tree%2F</url>
    <content type="text"><![CDATA[æ¨¹ Tree ç”Ÿæ´»ä¸­å¸¸è¦‹å„å¼å„æ¨£çš„æ•¸ç‹€åœ–ï¼Œåƒæ˜¯æ—è­œã€è³½ç¨‹è¡¨ç­‰ç­‰ï¼Œè‡³æ–¼åœ¨è¨ˆç®—æ©Ÿç§‘å­¸ä¸­ï¼Œtreeå‰‡æ˜¯ä¸€ç¨®æŠ½è±¡è³‡æ–™å‹åˆ¥(Abstract Data Type)ï¼Œæ˜¯ä¸€ç¨®å…·æœ‰éšå±¤å¼ (Hierarchical) çš„æ•¸ç‹€è³‡æ–™é›†åˆã€‚ å®šç¾© æ¨¹ (tree) æ˜¯ä¸€å€‹ç”±ä¸€å€‹æˆ–å¤šå€‹ç¯€é» (node) æ‰€çµ„æˆçš„é›†åˆï¼Œæ»¿è¶³ä»¥ä¸‹å…©é»ï¼š 1. æœ‰ä¸€å€‹ç‰¹åˆ¥çš„ç¯€é»å«ä½œã€Œæ ¹ã€(root) 2. å…¶é¤˜çš„ç¯€é» (node) å¯è¢«åˆ†å‰²ç‚º n &gt;= 0 å€‹äº’æ–¥ (disjoint) çš„é›†åˆ T1, ..., Tn , æ¯å€‹é›†åˆéƒ½æ˜¯ä¸€æ£µæ¨¹ï¼Œä¸¦ä¸”ç¨±ä½œæ ¹çš„ã€Œå­æ¨¹ã€ (subtree) åè©ä»‹ç´¹ degree : ä¸€å€‹ node å­ç¯€é»çš„å€‹æ•¸ï¼Œe.g. degree(A) = 3, degree(C) = 1, degree(F) = 0 leaf (terminal) : degree = 0 çš„ nodeï¼Œe.g. leaf = {K, L, F, G, M, I, J} parent &amp; children : A æ˜¯ B çš„ parent, B æ˜¯ A çš„ children siblings : å…·æœ‰ç›¸åŒ parent çš„ nodes, e.g. B, C, D æ˜¯ siblings degree of tree : æ‰€æœ‰ node çš„ degree ä¸­æœ€å¤§å€¼ä»£è¡¨é€™æ£µæ•¸çš„ degreeï¼Œdegree ç‚º k çš„æ¨¹åˆç¨±ç‚º k-ary tree ancestors : ä¸€å€‹ node èµ°å› root æ‰€è¦ç¶“éçš„ nodes, e.g. M çš„ ancestors æœ‰ A, D, H level : root çš„ level ç‚º 0 æˆ– 1ï¼Œchildren çš„ level ç‚º root çš„ level + 1ï¼Œä»¥æ­¤é¡æ¨ height (depth) : æ‰€æœ‰ node ä¸­ level çš„æœ€å¤§å€¼ä»£è¡¨é€™æ£µæ•¸çš„ height (depth) è¡¨ç¤ºæ–¹æ³• List Representation Left Child-Right Sibling Representation Representation as Degree-Two Tree (Left Child-Right Child) äºŒå…ƒæ¨¹ Binary Tree ä»¥ä¸Šä»‹ç´¹çš„treeä¸¦æ²’æœ‰é™åˆ¶subtreeçš„æ•¸é‡ï¼Œä½†æˆ‘å€‘ä¸€èˆ¬è¼ƒå¸¸ç”¨çš„é‚„æ˜¯æ¥ä¸‹ä¾†è¦ä»‹ç´¹çš„äºŒå…ƒæ¨¹ (Binary Tree) ã€‚ äºŒå…ƒæ¨¹æœ‰ä»¥ä¸‹å¹¾é»ç‰¹æ€§ï¼š 1. æ¯å€‹ node çš„ degree ä¸è¶…é 2 2. binary tree å¯ä»¥ä¸å­˜åœ¨ä»»ä½•çš„ç¯€é» (empty) 3. éœ€è¦å€åˆ†å·¦å­æ¨¹èˆ‡å³å­æ¨¹ï¼Œä¹Ÿå°±æ˜¯å·¦å³å­æ¨¹äº’æ›ä½ç½®çš„è©±å°±æœƒå½¢æˆå¦ä¸€å€‹æ–°çš„æ¨¹ äºŒå…ƒæ¨¹çš„è¡¨ç¤ºæ–¹æ³• Array Representation ä½¿ç”¨arrayä¾†è¡¨ç¤ºbinary treeé€šå¸¸æœƒå…·æœ‰ä»¥ä¸‹çš„ç‰¹æ€§ï¼š * parent index = âŒŠ current index / 2 âŒ‹ * leftChild index = current index * 2 * rightChild index = current index * 2 + 1 ä½†æ˜¯ä½¿ç”¨arrayè¡¨ç¤ºï¼Œå¸¸å¸¸æœƒé€ æˆç©ºé–“ä¸Šçš„æµªè²»ï¼Œå¦‚ï¼š èƒ½å¤ æœ€æœ‰æ•ˆåˆ©ç”¨arrayç©ºé–“çš„æ¨¹æ˜¯complete binary treeï¼Œåƒæ˜¯heapå°±éå¸¸é©åˆç”¨arrayé€²è¡Œå¯¦ä½œã€‚ Linked Representation å¦ä¸€ç¨®è¡¨ç¤ºæ–¹æ³•æ˜¯ä½¿ç”¨linked listå¯¦ä½œï¼Œé›–ç„¶æ¯å€‹nodeéƒ½éœ€è¦é¡å¤–çš„ç©ºé–“ä¾†å„²å­˜linkï¼Œä½†æ˜¯å°æ–¼ç©ºé–“ä¸Šçš„åˆ©ç”¨å»èƒ½å¤ é€²è¡Œæ¯”è¼ƒæœ‰æ•ˆçš„ç®¡æ§ã€‚ å¯¦ä½œ ä»¥ä¸‹ä½¿ç”¨pythonä»¥Linked Listå¯¦ä½œBinary Treeã€‚ é¦–å…ˆå®šç¾©æ¨¹çš„ç¯€é»ï¼š 12345class TreeNode(): def __init__(self, data=None, left=None, right=None): self.data = data self.left = left self.right = right æ¥è‘—æ˜¯äºŒå…ƒæ¨¹çš„é¡åˆ¥ä»¥åŠåŸºæœ¬æ–¹æ³•ï¼š 12345678910111213141516class BinaryTree(): def __init__(self, root=None): self.root = root def isEmpty(self, node): return node is None def left_child(self, node): if self.isEmpty(node): return return node.left def right_child(self, node): if self.isEmpty(node): return return node.right äºŒå…ƒæ¨¹çš„éæ­· Traversal è‹¥éæ­·ä¸€æ£µæ¨¹å…·æœ‰ä»¥ä¸‹ä¸‰å€‹æ­¥é©Ÿï¼Œ * L: moving left * V: visiting the node * R: moving right å‰‡éæ­·çš„æ–¹æ³•å°±æœ‰ï¼ŒL,V,R ä¸‰ç¨®æ­¥é©Ÿçš„æ’åˆ—æ•¸ = 3! = 6 ç¨®: LVR, LRV, VLR, VRL, RLV, RVL ã€‚ å–å…¶ä¸­ä¸‰ç¨®ç•¶ä½œéæ­·çš„æ–¹æ³•ï¼š * preorder: VLR * inorder: LVR * postorder: LRV è¨˜çš„æ–¹æ³•å¾ˆç°¡å–®ï¼Œçœ‹ V çš„ä½ç½®å°±å°äº†ï¼Œè‹¥ V çš„ä½ç½®åœ¨å‰é¢å°±æ˜¯preorderï¼Œåœ¨ä¸­é–“å°±æ˜¯inorderï¼Œåœ¨å¾Œé¢å°±æ˜¯postorderã€‚ Inorder Traversal 12345def inorder(self, node): if node: self.inorder(node.left) print(node.data, end=" ") self.inorder(node.right) Preorder Traversal 12345def preorder(self, node): if node: print(node.data, end=" ") self.preorder(node.left) self.preorder(node.right) Postorder Traversal 12345def postorder(self, node): if node: self.postorder(node.left) self.postorder(node.right) print(node.data, end=" ") Level-Order Traversal ç¬¬å››ç¨®éæ­·çš„æ–¹æ³•ï¼Œä¾ç…§nodeçš„levelçš„é †åºä¾åºæ‹œè¨ªå„çµé»ã€‚ 12345678910111213def levelorder(self, node): if not node: return from collections import deque queue = deque() queue.append(node) while queue: node = queue.popleft() print(node.data, end=" ") if node.left: queue.append(node.left) if node.right: queue.append(node.right) å…¶ä»–æ“ä½œ è¤‡è£½ 12345# Return a Pointer to a same data from original nodedef copy(self, node): if node: return TreeNode(node.data, self.copy(node.left), self.copy(node.right)) return None ç›¸ç­‰ 1234@staticmethoddef equal(first, second): return (not first and not second) or (first and second and (first.data == second.data) \and BinaryTree.equal(first.left, second.left) and BinaryTree.equal(first.right, second.right)) (æœªå®Œæˆï¼Œå¾…è£œä¸Š...)]]></content>
      <categories>
        <category>å­¸æ ¡èª²ç¨‹</category>
        <category>è³‡æ–™çµæ§‹</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Data Structure</tag>
        <tag>Tree</tag>
        <tag>Binary Tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å †ç–Š(Stack)èˆ‡ä½‡åˆ—(Queue)]]></title>
    <url>%2F2017%2F09%2F06%2F%E5%A0%86%E7%96%8A-Stack-%E8%88%87%E4%BD%87%E5%88%97-Queue%2F</url>
    <content type="text"><![CDATA[ä»‹ç´¹ Stack èˆ‡ Queue éƒ½æ˜¯ä¸€ç¨® æŠ½è±¡è³‡æ–™å‹åˆ¥ (Abstract Data Type)ï¼Œå…©è€…çš„å€åˆ¥ç°¡å–®ä¾†èªªå°±æ˜¯Stackæ˜¯ Last-In-First-Out (LIFO)ï¼Œè€Œ Queue å‰‡æ˜¯ First-In-First-Out (FIFO)ã€‚ å¯¦ä½œ ä»¥ä¸‹ç”¨ Linked List ä¾†å¯¦ä½œ Stack èˆ‡ Queue Stack é¦–å…ˆæ˜¯ stack çš„ç¯€é»ã€‚ 1234class StackNode(): def __init__(self, value=None, next=None): self.value = value self.next = None æ¥è‘—å®šç¾©å‡½å¼ï¼Œå› ç‚ºæ˜¯å¾Œé€²å…ˆå‡ºï¼Œæ‰€ä»¥ç”¨ top ä¾†ç´€éŒ„æœ€å¾Œé€²å…¥çš„æ•¸ï¼Œå› æ­¤ä¸ç®¡æ˜¯ push é‚„æ˜¯ pop éƒ½æ˜¯æ›´å‹•åˆ° top çš„å€¼ã€‚ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class Stack(): def __init__(self, top=None): self.top = top def print_nodes(self): current = self.top while current: end = " -&gt; " if current.next else "\n" print(current.value, end=end) current = current.next def push(self, value): old_top = self.top self.top = StackNode(value, old_top) self.top.next = old_top def pop(self): if self.isEmpty(): print("Pop nothing, the stack is empty!") return self.top = self.top.next def top_value(self): if self.isEmpty(): return "Nothing at the top, the stack is empty!" return self.top.value def search(self, value): count = 0 current = self.top while current: if current.value == value: return count count += 1 current = current.next def isEmpty(self): return self.top is None def size(self): count = 0 current = self.top while current: count += 1 current = current.next return count Queue æ¥è‘—æ˜¯ queue çš„ç¯€é»ã€‚ 1234class QueueNode(): def __init__(self, value=None, next=None): self.value = value self.next = next å› ç‚º queue ç‚ºå…ˆé€²å…ˆå‡ºï¼Œå› æ­¤éœ€è¦æœ‰å…©å€‹è®Šæ•¸ä¾†ç´€éŒ„é ­è·Ÿå°¾çš„ä½ç½®ï¼Œå¯ä»¥å°‡ queue æƒ³åƒæˆæ’éšŠçš„æ™‚å€™ï¼Œadd å°±æ˜¯å°‡æ–°çš„ç¯€é»æ¥åœ¨éšŠä¼çš„å¾Œæ–¹ï¼Œdelete å‰‡æ˜¯å°‡ç¯€é»å¾éšŠä¼çš„é ­ç§»é™¤ã€‚ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class Queue(): def __init__(self, front=None, rear=None): self.front = front self.rear = rear def print_queue(self): current = self.front while current: end = " -&gt; " if current.next else "\n" print(current.value, end=end) current = current.next def add(self, value): if self.isEmpty(): self.front = QueueNode(value) self.rear = self.front else: self.rear.next = QueueNode(value) self.rear = self.rear.next def delete(self): if self.isEmpty(): print("Delete nothing, the queue is empty.") else: self.front = self.front.next def search(self, value): count = 0 current = self.front while current: if current.value == value: return count count += 1 current = current.next def top_value(self): if self.isEmpty(): return "Nothing at the top, the stack is empty!" return self.front.value def end_value(self): if self.isEmpty(): return "Nothing at the end, the stack is empty!" return self.rear.value def isEmpty(self): return self.front is None ç¨‹å¼ç¢¼ï¼šStack Queue]]></content>
      <categories>
        <category>å­¸æ ¡èª²ç¨‹</category>
        <category>è³‡æ–™çµæ§‹</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Data Structure</tag>
        <tag>Stack</tag>
        <tag>Queue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode#167 Two Sum II - Input array is sorted feat. Two Pointers]]></title>
    <url>%2F2017%2F09%2F05%2FLeetCode-167-Two-Sum-II-Input-array-is-sorted-feat-Two-Pointers%2F</url>
    <content type="text"><![CDATA[å•é¡Œ çµ¦å®šä¸€å€‹éå¢çš„æ•´æ•¸é™£åˆ—nums(å·²æ’åº)ï¼Œå…¶ä¸­æœ‰å…©å€‹æ•¸çš„å’Œæœƒç­‰æ–¼ç›®æ¨™targetï¼Œå›å‚³ä»–å€‘çš„ä½ç½®ï¼Œä¸”åŒå€‹ä½ç½®ä¸èƒ½é‡è¤‡é¸å–ã€‚ è¼¸å…¥ numbers = [2, 7, 11, 15] target = 9 è¼¸å‡º [1, 2] æ–¹æ³• é€™é¡Œæ˜¯LeetCode#1 Two Sumsçš„æ”¹ç‰ˆï¼Œå·®åˆ¥æ˜¯é€™é¡Œè¼¸å…¥çš„é™£åˆ—å·²ç¶“ç¶“éæ’åºäº†ï¼Œé€™é¡Œé‚„æ˜¯å¯ä»¥ç”¨hash tableä¾†è§£ï¼Œä¸éé€™è£¡æˆ‘å€‘è¦ç”¨æ¯”è¼ƒä¸ä¸€æ¨£çš„æ–¹æ³•ï¼Œå«ä½œtwo pointers ã€‚ two pointersåœ¨é€™è£¡çš„ä½¿ç”¨æ–¹æ³•ï¼Œå°±æ˜¯å…©å€‹ç®­é ­åˆ†åˆ¥æŒ‡å‘é™£åˆ—çš„é ­è·Ÿå°¾ï¼Œè—‰ç”±å°‡å·¦é‚Šçš„ç®­é ­å‘å³èª¿æ•´ä»¥åŠå³é‚Šçš„ç®­é ­å‘å·¦èª¿æ•´ï¼Œé€¼è¿‘å‡ºæ‰€è¦æ±‚çš„å€¼ã€‚ 12345678910class Solution(object): def twoSum(self, numbers, target): left, right = 0, len(numbers) - 1 while left &lt; right: if numbers[left] + numbers[right] &lt; target: left += 1 elif numbers[left] + numbers[right] &gt; target: right -= 1 else: return [left + 1, right + 1] ä»¥ä¸‹çš„å•é¡Œä¹Ÿå¸¸å¸¸ç”¨two pointersä¾†è§£ï¼š 1. å­—ä¸²ã€é™£åˆ—åè½‰ 2. å…©å€‹è·‘è€…ä¸€å¿«ä¸€æ…¢åœ¨æ“å ´ä¸Šå¥”é¦³ï¼Œå•å…©äººä½•æ™‚ç›¸é‡ 3. å­—ä¸²å›æ–‡(Palindrome)]]></content>
      <categories>
        <category>ç¨‹å¼è§£é¡Œ</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>Python</tag>
        <tag>Two Pointers</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æŒ‘æˆ°LeetCode Weekly Contest 48]]></title>
    <url>%2F2017%2F09%2F04%2F%E6%8C%91%E6%88%B0LeetCode-Weekly-Contest-48%2F</url>
    <content type="text"><![CDATA[å†åº¦æŒ‘æˆ° é€™æ¬¡çš„é¡Œç›®ç¨ç¨æ¯”ä¸Šæ¬¡ç°¡å–®ä¸€é»ï¼Œå‰å…©é¡Œè§£å®Œå¤§æ¦‚é‚„å‰© 20 åˆ†é˜ï¼Œçœ‹çœ‹ç¬¬ä¸‰é¡Œå¥½åƒä¹Ÿä¸é›£ï¼Œä½†å¯æƒœé‚„æ˜¯æ²’å¯«å®Œï¼Œæœ€çµ‚æˆç¸¾ 1170 / 2668ï¼Œå—¯...å¥½åƒæ²’é€²æ­¥ Orz ã€‚ ç¬¬1é¡Œ 671. Second Minimum Node In a Binary Tree å•é¡Œ çµ¦ä¸€æ£µäºŒå…ƒæ¨¹ï¼Œæ¯ä¸€å€‹nodeçš„å­ç¯€é»ä¸æ˜¯ 0 å€‹å°±æ˜¯ 2 å€‹ï¼Œè¼¸å‡ºæ‰€æœ‰ç¯€é»ç•¶ä¸­ç¬¬äºŒå°çš„å€¼ï¼Œè‹¥ä¸å­˜åœ¨å‰‡è¼¸å‡º-1ã€‚ è¼¸å…¥ tree çš„ root ç¯„ä¾‹ï¼š 2 / \ 2 5 / \ 5 7 è¼¸å‡º 5 æ–¹æ³• éæ­·æ‰€æœ‰çš„ç¯€é»ï¼Œä¸æ–·åˆ·æ–°æœ€å°å€¼(ä¸‹ç•Œ)èˆ‡ç¬¬äºŒå°çš„å€¼(ä¸Šç•Œ)ï¼Œå¤¾æ“ å‡ºçœŸæ­£ç¬¬äºŒå°çš„æ•¸å€¼ã€‚ {% codeblock lang:python %} class Solution(object): def findSecondMinimumValue(self, root): if not root or not root.left: return -1 self.smallest = root.val self.second = max(root.left.val, root.right.val) self.find(root) if self.smallest == self.second: return -1 return self.second def find(self, root): if root.left and root.right: if root.left.val < self.smallest: self.smallest = root.left.val if root.right.val < self.smallest: self.smallest = root.right.val if root.left.val > self.smallest and root.left.val self.smallest and root.right.val]]></content>
      <categories>
        <category>ç¨‹å¼è§£é¡Œ</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>Python</tag>
        <tag>LeetCode Weekly Contest</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[é€£çµä¸²åˆ—(Linked List)]]></title>
    <url>%2F2017%2F09%2F02%2F%E9%80%A3%E7%B5%90%E4%B8%B2%E5%88%97-Linked-List%2F</url>
    <content type="text"><![CDATA[é€£çµä¸²åˆ—ä¸€èˆ¬æŒ‡çš„æ˜¯å–®å‘é€£çµä¸²åˆ—(Single Linked List)ï¼Œç”±nodeæ‰€çµ„æˆï¼Œæ¯å€‹nodeéƒ½å…·æœ‰å…©ç¨®å±¬æ€§ï¼Œåˆ†åˆ¥æ˜¯ã€Œè³‡æ–™ã€ä»¥åŠã€ŒæŒ‡æ¨™ã€ã€‚è³‡æ–™æ˜¯å„²å­˜ç›®å‰é€™å€‹ç¯€é»çš„å€¼ï¼ŒæŒ‡æ¨™æ˜¯æŒ‡å‘ä¸‹ä¸€å€‹ç¯€é»çš„é€£çµï¼Œè‡³æ–¼æœ€å¾Œä¸€å€‹çµé»å‰‡æœƒæŒ‡å‘nullã€‚ é€£çµä¸²åˆ—æ¯”èµ·ä¸€èˆ¬çš„é™£åˆ—ï¼Œå„ªé»æ˜¯èƒ½å¤ éš¨è‘—éœ€æ±‚å‹•æ…‹é…ç½®è¨˜æ†¶é«”ï¼Œæ’å…¥æˆ–ç§»é™¤å…ƒç´ æ™‚ä¹Ÿç›¸å°æ–¹ä¾¿ï¼›ç¼ºé»æ˜¯å–å‡ºå…ƒç´ æ™‚ç„¡æ³•ç›´æ¥æŒ‡å®šä½ç½®ï¼Œéœ€è¦éæ­·æ•´å€‹ä¸²åˆ—ã€‚ ä»¥ä¸‹ç”¨pythonä¾†å¯¦ä½œLinked Listã€‚ é¦–å…ˆæˆ‘å€‘è¦å¯¦ä½œnodeï¼Œå®£å‘Šä¸€å€‹classåç¨±ListNodeï¼Œæ¯å€‹nodeéƒ½éœ€è¦å…©å€‹å…ƒç´ ï¼Œè³‡æ–™valueèˆ‡æŒ‡å‘ä¸‹ä¸€å€‹nodeçš„æŒ‡æ¨™nextã€‚ 1234class ListNode(object): def __init__(self, value=None, next=None): self.val = value self.next = next æˆ‘å€‘é‚„è¦å®£å‘Šä¸€å€‹åç‚ºLinkedListçš„é¡åˆ¥ï¼Œç”¨ä¾†ç´€éŒ„ä¸²åˆ—çš„èµ·å§‹ä½ç½®ã€‚ 123class LinkedList(object): def __init__(self, head=None): self.head = head æ¥è‘—åœ¨LinkedListå¯¦ä½œä¸‹é¢çš„æ–¹æ³•ï¼š print_nodes() at(index) append(value) insert(index, value) removePos(index) remove(value, all=False) indexOf(value) clear() isEmpty() size() é¦–å…ˆæ˜¯print_nodesæ–¹æ³•ï¼Œèµ°è¨ªæ‰€æœ‰çš„ç¯€é»ä¸¦å°å‡ºæ¯å€‹çµé»çš„è³‡æ–™ã€‚ 12345678def print_nodes(self): if not self.head: print(self.head) node = self.head while node: end = " -&gt; " if node.next else "\n" print(node.val, end=end) node = node.next æ¥è‘—æ˜¯atæ–¹æ³•ï¼Œå›å‚³indexä½ç½®çš„valueã€‚ 12345678def at(self, index): count = 0 node = self.head while node: if count == index: return node.val count += 1 node = node.next ç¬¬ä¸‰å€‹æ˜¯appendï¼Œå°‡å€¼ç‚ºvalueçš„ç¯€é»æ¥åœ¨é™£åˆ—çš„æœ€å°¾ç«¯ã€‚ 12345678def append(self, value): if not self.head: self.head = ListNode(value) return node = self.head while node.next: node = node.next node.next = ListNode(value) ç¬¬å››å€‹æ˜¯insertï¼Œå°‡valueæ’å…¥indexçš„ä½ç½®ã€‚ 123456789101112131415161718def insert(self, index, value): if index &gt;= self.size(): self.append(value) return count = 0 node = self.head previous = None while node: if count == index: if previous: new_node = ListNode(value, previous.next) previous.next = new_node else: self.head = ListNode(value, node) return count += 1 previous = node node = node.next ç¬¬äº”å€‹èˆ‡ç¬¬å…­å€‹æ˜¯removePosä»¥åŠremoveï¼Œåˆ†åˆ¥æ˜¯ç§»é™¤ä½ç½®ç‚ºindexçš„ç¯€é»ï¼Œä»¥åŠç§»é™¤å€¼ç‚ºvalueçš„ç¯€é»ã€‚ 12345678910111213141516def removePos(self, index): count = 0 node = self.head previous = None while node: if count == index: if previous: previous.next = node.next node = node.next else: self.head = node.next node = self.head return else: previous = node node = node.next 12345678910111213141516def remove(self, val, all=False): node = self.head previous = None while node: if node.val == val: if previous: previous.next = node.next node = node.next else: self.head = node.next node = self.head if not all: return else: previous = node node = node.next ç¬¬ä¸ƒå€‹æ˜¯indexOfï¼Œå›å‚³ç¬¬ä¸€å€‹å‡ºç¾çš„valueåœ¨ä¸²åˆ—ä¸­çš„çš„ä½ç½®ã€‚ 12345678def indexOf(self, value): node = self.head count = 0 while node: if node.val == value: return count count += 1 node = node.next æœ€å¾Œä¸‰å€‹æ˜¯clear, isEmpty, ä»¥åŠsizeã€‚ 12def clear(self): self.head = None 12def isEmpty(self): return self.head is None 1234567def size(self): count = 0 node = self.head while node: count += 1 node = node.next return count ç¨‹å¼ç¢¼é€£çµ]]></content>
      <categories>
        <category>å­¸æ ¡èª²ç¨‹</category>
        <category>è³‡æ–™çµæ§‹</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Linked List</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode#78 Subsets]]></title>
    <url>%2F2017%2F08%2F31%2FLeetCode-78-Subsets%2F</url>
    <content type="text"><![CDATA[å•é¡Œ çµ¦ä¸€ä¸²å…ƒç´ ç›¸ç•°çš„æ•¸åˆ—ï¼Œæ±‚ç”±æ­¤æ•¸åˆ—ä¹‹å…ƒç´ æ‰€çµ„æˆçš„æ‰€æœ‰å­é›†åˆã€‚ è¼¸å…¥ [1,2,3] è¼¸å‡º [ [3], [1], [2], [1,2,3], [1,3], [2,3], [1,2], [] ] æ–¹æ³• æ¯å€‹å…ƒç´ å¯ä»¥å‡ºç¾æˆ–æ˜¯ä¸å‡ºç¾ï¼Œæ¯ä¸€ç¨®çµ„åˆæ–¹æ³•éƒ½æ˜¯ä¸€å€‹å­é›†åˆï¼Œæ‰€ä»¥æ™‚é–“è¤‡é›œåº¦æ˜¯O(2n) ã€‚ åˆ©ç”¨Backtrackingéè¿´æšèˆ‰æ‰€æœ‰å¯èƒ½ã€‚ 1234567891011121314class Solution(object): def subsets(self, nums): self.length = len(nums) self.ans = [] self.backtrack(nums, []) return(self.ans) def backtrack(self, nums, n): if len(n) == self.length: a = [nums[i] for i, v in enumerate(n) if v] self.ans.append(a) else: self.backtrack(nums, n + [True]) self.backtrack(nums, n + [False]) å…¶ä»–è§£æ³•ä»¥åŠé¡ä¼¼å•é¡Œè«‹çœ‹é€™è£¡ã€‚]]></content>
      <categories>
        <category>ç¨‹å¼è§£é¡Œ</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>Python</tag>
        <tag>Backtracking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode#442 Find All Duplicates in an Array]]></title>
    <url>%2F2017%2F08%2F30%2FLeetCode-442-Find-All-Duplicates-in-an-Array%2F</url>
    <content type="text"><![CDATA[å•é¡Œ çµ¦ä¸€å€‹é•·åº¦ç‚º n çš„æ•´æ•¸é™£åˆ—ï¼Œè£¡é¢çš„å…ƒç´ ç”± 1~n æ‰€çµ„æˆï¼Œæ¯å€‹æ•¸å­—å‡ºç¾ 0~2 æ¬¡ï¼Œæ‰¾å‡ºæ‰€æœ‰å‡ºç¾ 2 æ¬¡çš„æ•¸å­—ã€‚(ä¸ä½¿ç”¨é¡å¤–ç©ºé–“ä¸”æ™‚é–“è¤‡é›œåº¦å¿…é ˆç‚ºO(n)) è¼¸å…¥ [4, 3, 2, 7, 8, 2, 3, 1] è¼¸å‡º [2, 3] æ–¹æ³• è² è™Ÿæ¨™è¨˜æ³• å°‡é™£åˆ—numsçš„å€¼éæ­·ä¸€æ¬¡ï¼Œå°‡æ¯å€‹å‡ºç¾éçš„å€¼ç•¶ä½œindexï¼Œå°‡å°æ‡‰åˆ°çš„num[index]çš„å€¼ä¹˜ä¸Š-1ï¼Œè‹¥å°æ‡‰åˆ°çš„å€¼å·²ç¶“ç‚ºè² æ•¸çš„è©±ï¼Œä»£è¡¨å·²ç¶“å‡ºç¾é 1 æ¬¡ï¼Œç›®å‰æ˜¯ç¬¬ 2 æ¬¡ï¼Œé€™æ¨£å°±èƒ½æ‰¾å‡ºæ‰€æœ‰å‡ºç¾é 2 æ¬¡çš„å…ƒç´ äº†ã€‚ 123456789class Solution(object): def findDuplicates(self, nums): ans = [] for i in nums: if nums[abs(i) - 1] &lt; 0: ans.append(abs(i)) else: nums[abs(i) - 1] *= -1 return ans #448 Find All Numbers Disappeared in an Arrayï¼Œä¹Ÿå¯ä»¥ç”¨åŒæ¨£çš„æ–¹æ³•ä¾†è§£ã€‚]]></content>
      <categories>
        <category>ç¨‹å¼è§£é¡Œ</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>Python</tag>
        <tag>Array</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[æŒ‘æˆ°LeetCode Weekly Contest 47]]></title>
    <url>%2F2017%2F08%2F28%2F%E6%8C%91%E6%88%B0LeetCode-Weekly-Contest-47%2F</url>
    <content type="text"><![CDATA[å‰è¨€ é€™æ¬¡å¿ƒè¡€ä¾†æ½®å ±åäº† LeetCode æ¯å‘¨éƒ½æœƒèˆ‰è¾¦çš„æ¯”è³½ï¼Œè¦å‰‡æ˜¯åœ¨1.5å°æ™‚ä¹‹å…§è¦è§£å‡º4é¡Œï¼Œé€™å°æˆ‘ä¾†èªªæ˜¯ä¸€å¤§æŒ‘æˆ°å•Šï¼å› ç‚ºä»¥å¾€åƒåŠ çš„æ¯”è³½ï¼Œæ™‚é–“éƒ½æ˜¯3å°æ™‚å·¦å³ï¼Œé€™éº¼çŸ­çš„æ™‚é–“å…§è¦è§£å‡º4é¡ŒçœŸçš„ä¸å®¹æ˜“ï¼Œä¸éæ—¢ç„¶éƒ½å ±åäº†å°±å…¨åŠ›ä»¥èµ´å§ï¼ ç¬¬1é¡Œ 665. Non-decreasing Array å•é¡Œ çµ¦å®šä¸€å€‹é•·åº¦ç‚º n çš„æ•´æ•¸é™£åˆ—ï¼Œå•æ˜¯å¦èƒ½åœ¨æœ€å¤šæ›´æ”¹ 1 å€‹æ•¸çš„æƒ…æ³ä¸‹ï¼Œå°‡é™£åˆ—è®Šæˆ non-decreasing arrayï¼Œä¹Ÿå°±æ˜¯éå¢çš„é™£åˆ—ã€‚ è¼¸å…¥ [4, 2, 3] è¼¸å‡º True æ–¹æ³• æˆ‘çš„ç­–ç•¥æ˜¯åˆ†åˆ¥å¾é ­å°¾å‡ºç™¼èµ°ä¸€æ¬¡ï¼Œè‹¥é‡åˆ°ä¸‹ä¸€å€‹æ•¸ pj å°æ–¼/å¤§æ–¼ç›®å‰é€™å€‹æ•¸ pi çš„è©±ï¼Œå°±å°‡ä¸‹ä¸€å€‹æ•¸çš„å€¼æ”¹ç‚ºç›®å‰é€™å€‹æ•¸ï¼Œä½¿å¾—ç›®å‰ç‚ºæ­¢çš„å…ƒç´ ç‚ºéå¢çš„ç‹€æ…‹ï¼Œæ­¤æ™‚ä¿®æ”¹çš„æ¬¡æ•¸å°±æœƒå¢åŠ ä¸€æ¬¡ã€‚è‹¥åˆ†åˆ¥å¾é ­å°¾èµ°ï¼Œä¿®æ”¹çš„æ¬¡æ•¸éƒ½å¤§æ–¼ä¸€æ¬¡çš„è©±ï¼Œä»£è¡¨ç„¡æ³•ç¬¦åˆé¡Œç›®çš„æ¢ä»¶ï¼Œå›å‚³Falseï¼Œåä¹‹å›å‚³Trueã€‚ é‚£ç‚ºä»€éº¼è¦åˆ†åˆ¥å¾é ­å°¾èµ°ä¸€æ¬¡å‘¢ï¼Ÿ ä»¥ç¯„ä¾‹çš„è¼¸å…¥ç•¶ä¾‹å­ï¼Œè‹¥åªå¾æ­£æ–¹å‘èµ°ï¼Œ (4, 2) ä¸ç¬¦åˆéå¢ï¼Œå°‡ 2 æ”¹æˆ 4ï¼Œé™£åˆ—è®Šæˆ [4, 4, 3]ï¼Œæ¥è‘—é‡åˆ° (4, 3) åˆä¸ç¬¦åˆéå¢ï¼Œå› æ­¤åªä¿®æ”¹ä¸€å€‹å€¼ä¸¦ä¸èƒ½ä½¿åŸæœ¬çš„é™£åˆ—è®Šæˆéå¢æ•¸åˆ—ã€‚ ä½†æ˜¯éŒ¯å•¦ï¼å…¶å¯¦åªè¦æŠŠç¬¬ä¸€å€‹æ•¸ 4 æ”¹æˆ 1 æˆ– 2 å°±å¯ä»¥äº†ï¼Œæƒ³åˆ°å¾åæ–¹å‘èµ°çš„è©±å¥½åƒå¯ä»¥é”åˆ°é€™å€‹æ•ˆæœï¼Œæ‰€ä»¥å¿…é ˆè¦æª¢æŸ¥å…©å€‹æ–¹å‘æ‰èƒ½ç¢ºä¿æ²’æœ‰æ¼ç¶²ä¹‹é­šã€‚ 1234567891011121314151617181920class Solution: def checkPossibility(self, nums): n = list(nums) edit_time_1 = 0 for i in range(len(nums) - 1): if nums[i] &gt; nums[i + 1]: nums[i + 1] = nums[i] edit_time_1 += 1 nums = n edit_time_2 = 0 for i in range(len(nums) - 1, 0, -1): if nums[i] &lt; nums[i - 1]: nums[i - 1] = nums[i] edit_time_2 += 1 if edit_time_1 &lt;= 1 or edit_time_2 &lt;= 1: return True else: return False ç¬¬2é¡Œ 666. Path Sum IV å•é¡Œ è¼¸å…¥ç‚ºä¸€å€‹ Depth &lt; 5 çš„ treeï¼Œæ¯å€‹ node å¯ç”¨ä¸‰ä½æ•¸ä¾†è¡¨ç¤ºã€‚ ç™¾ä½æ•¸Dä»£è¡¨é€™å€‹ node æ‰€åœ¨çš„æ·±åº¦ï¼Œ1 &lt;= D &lt;= 4 ã€‚ åä½æ•¸Pä»£è¡¨é€™å€‹ node åœ¨é€™å€‹æ·±åº¦çš„ç¬¬å¹¾å€‹ä½ç½®, 1 &lt;= P &lt;= 8 ã€‚ å€‹ä½æ•¸Vä»£è¡¨é€™å€‹ node çš„å€¼, 0 &lt;= V &lt;= 9 ã€‚ æ‰¾å‡ºæ‰€æœ‰å¾ root åˆ° leaf çš„è·¯å¾‘å’Œï¼Œä¹Ÿå°±æ˜¯æŠŠè·¯å¾‘ä¸Šçš„å€¼ç›¸åŠ ã€‚ è¼¸å…¥ [113, 215, 221] è¼¸å‡º 12 æ–¹æ³• ä»¥ä¸‹æ˜¯ç¯„ä¾‹è¼¸å…¥çš„è§£é‡‹ï¼š The tree that the list represents is: 3 / \ 5 1 The path sum is (3 + 5) + (3 + 1) = 12. å…ˆå°‡è¼¸å…¥çš„é»åˆ†åˆ¥å°æ‡‰ 1 ~ 15 çš„å€¼(ä¹Ÿå°±æ˜¯ç”¨é™£åˆ—è¡¨ç¤º tree, ä½†é€™è£¡ç›´æ¥ç”¨dictionaryæ¯”è¼ƒæ–¹ä¾¿)ï¼Œæ¥è‘—æ‰¾å‡ºæ‰€æœ‰çš„ leaf ï¼Œå†å°‡æ¯å€‹ leaf åˆ° root çš„è·¯å¾‘ä¸Šæ‰€ç¶“éçš„é»å…¨éƒ¨åŠ èµ·ä¾†å³å¯ã€‚ 1234567891011class Solution: def pathSum(self, nums): order = ['00', '11', '21', '22', '31', '32', '33', '34', '41', '42', '43', '44', '45', '46', '47', '48'] tree = dict([(order.index(str(n)[:2]), n % 10) for n in nums]) summation = 0 leaves = [n for n in tree.keys() if not (2 * n in tree or 2 * n + 1 in tree)] for leaf in leaves: while leaf &gt;= 1: summation += tree[leaf] leaf = int(leaf / 2) return summation ç¬¬3é¡Œ 667. Beautiful Arrangement II å•é¡Œ è¼¸å…¥æ•´æ•¸nèˆ‡kï¼Œæ‰¾åˆ°ä¸€å€‹ç”± 1 ~ n çµ„æˆçš„é™£åˆ—[a1, a2, ..., an]ï¼Œé™£åˆ—å…ƒç´ ä¸å¾—é‡è¤‡ï¼Œ ä½¿å¾— [|a1-a2|, |a2-a3|, ..., |an-1-an|] å‰›å¥½ç”± k ç¨®ä¸åŒçš„æ•¸çµ„æˆã€‚ è¼¸å…¥ n = 3, k = 1 è¼¸å‡º [1, 2, 3] æ–¹æ³• é€™é¡Œæ¨å°äº†å¾ˆä¹…ï¼Œæƒ³åˆ°äº†ä¸€å€‹æ–¹æ³•ï¼Œ[1, 2, ..., n] çš„å·®æœ‰ k ç¨®å€¼ï¼Œ1 &lt;= k &lt;= n - 1ï¼Œåªè¦å»ºç«‹ä¸€å€‹è¦å‰‡ç”¢ç”Ÿæ‰€æœ‰çš„å·®ï¼Œæ¥è‘—å†æƒ³è¾¦æ³•æ¹Šå‡ºé€™å€‹æ•¸åˆ—ï¼Œèˆ‰ä¾‹ï¼š output: 5 1 4 3 2 diff: 4 3 1 1 å‡è¨­è¦ä½¿å¾—[1, 2, 3, 4, 5]é„°è¿‘çš„å·®åªæœ‰ä¸‰ç¨®ï¼Œé‚£å°±å…ˆå°‡å…©ç¨®å·®è¨­ç‚º n-1 èˆ‡ n-2ï¼Œä¹Ÿå°±æ˜¯ 4 è·Ÿ 3ï¼Œå‰©ä¸‹çš„å·®éƒ½å¡« 1 ä»£è¡¨ç¬¬ä¸‰ç¨®å·®ã€‚è‡³æ–¼è¦ç”¢ç”Ÿçš„é™£åˆ—ï¼Œå…ˆå¡«å…¥æœ€å¤§çš„æ•¸ 5ï¼Œæ¥è‘—æ ¹æ“šæ¯å€‹å·®ä¾åºæ¹Šå‡ºé™£åˆ—æ¥ä¸‹ä¾†çš„å€¼ï¼Œæœ€å¾Œå°±æœƒå¾—åˆ°æ»¿è¶³æ¢ä»¶çš„é™£åˆ—äº†ã€‚ 1234567891011121314class Solution: def constructArray(self, n, k): diff = [] for i in range(k - 1): diff.append(n - i - 1) for i in range(n - k): diff.append(1) nums = [n] for i in range(n - 1): new_value = abs(nums[i] + diff[i]) if new_value &gt; n or new_value in nums: new_value = abs(nums[i] - diff[i]) nums.append(new_value) return nums ç¬¬4é¡Œ 668. Kth Smallest Number in Multiplication Table å•é¡Œ æ‰¾å‡ºåœ¨ä¸€å€‹ m * n çš„ä¹˜æ³•è¡¨ä¸­ï¼Œç¬¬ k å°çš„æ•¸å­—ã€‚ è¼¸å…¥ m = 3, n = 3, k = 5 è¼¸å‡º 3 æ–¹æ³• ä»¥ 3 * 3 çš„ä¹˜æ³•è¡¨èˆ‰ä¾‹ï¼š The Multiplication Table: 1 2 3 2 4 6 3 6 9 The 5-th smallest number is 3 (1, 2, 2, 3, 3). é€™é¡Œæˆ‘è©¦äº†å¾ˆå¤šæ–¹æ³•ï¼Œä¸ç®¡æ€éº¼æ¨£ç¸½æ˜¯ä¸å¤ å‘¨å»¶ï¼Œæœ€å¾Œçœ‹äº†åˆ¥äººçš„è§£æ³•ï¼Œä½¿ç”¨ binary search ï¼Œå¯¦åœ¨å¤ªå²å®³äº†å®Œå…¨æƒ³ä¸åˆ°ã€‚ çœ‹æ‡‚äº†ä¹‹å¾Œå¯«äº†å€‹ python çš„ç‰ˆæœ¬ï¼Œæ–¹æ³•ç°¡å–®ä¾†èªªå°±æ˜¯åˆ©ç”¨äºŒåˆ†æœå°‹æ³•ï¼Œæ‰¾å‡ºä¸­é–“å€¼ mid ï¼Œæ¥è‘—è¨ˆç®—æ¯ä¸€åˆ—å°æ–¼ç­‰æ–¼ mid çš„å€‹æ•¸çš„å’Œï¼Œè‹¥ç¸½æ•¸å°æ–¼ k ï¼Œä»£è¡¨å€¼å¤ªå°äº†é‚„è¦å†å¤šä¸€é»ï¼›è‹¥ç¸½æ•¸å¤§æ–¼ k ï¼Œä»£è¡¨å€¼å¤ªå¤§äº†è¦å°ä¸€é»ï¼Œè¿´åœˆé‡è¤‡ç›´åˆ° low &lt; high çš„æ™‚å€™ä»£è¡¨æ‰¾åˆ°äº†ã€‚ 1234567891011121314151617class Solution: def findKthNumber(self, m, n, k): low, high = 1, m * n + 1 while low &lt; high: mid = (low + high) // 2 c = self.count_less_than_middle(mid, m, n) if c &gt;= k: high = mid else: low = mid + 1 return high def count_less_than_middle(self, middle, m, n): num = 0 for i in range(1, m + 1): num += min(middle // i, n) return num çµæœ æœ€å¾Œåœ¨æ™‚é–“å…§åªæœ‰è§£å‡ºä¸€é¡Œï¼Œæ’ååªæœ‰ 1157 / 2554ï¼Œå‰©ä¸‹çš„éƒ½æ˜¯æ¯”è³½çµæŸå¾Œæ‰æƒ³å‡ºä¾†QQï¼Œå¤ªæ…˜çƒˆäº†ã€‚çœ‹ä¾†åœ¨ä¸‹åšçš„é¡Œç›®é‚„ä¸å¤ å¤šï¼Œçœ‹åˆ°é¡Œç›®æ²’è¾¦æ³•é¦¬ä¸Šæœ‰ sense è¦ç”¨ä»€éº¼æ–¹å¼è§£ï¼Œåªå¥½å…ˆä¾†é–‰é—œä¿®ç…‰ä¸€ä¸‹äº†QQ ã€‚]]></content>
      <categories>
        <category>ç¨‹å¼è§£é¡Œ</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>Python</tag>
        <tag>LeetCode Weekly Contest</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode#566 Reshape the Matrix]]></title>
    <url>%2F2017%2F08%2F27%2FLeetCode-566-Reshape-the-Matrix%2F</url>
    <content type="text"><![CDATA[å•é¡Œ çµ¦å®šä¸€å€‹äºŒç¶­çš„çŸ©é™£ä»¥åŠæ­£æ•´æ•¸ r èˆ‡ c ï¼Œå°‡æ­¤çŸ©é™£è½‰ç‚º r * c çš„çŸ©é™£ã€‚ è‹¥ç„¡æ³•è½‰æ›ï¼Œå‰‡è¼¸å‡ºåŸæœ¬çš„çŸ©é™£ã€‚ ## è¼¸å…¥ ## nums = [[1,2], [3,4]] r = 1, c = 4 ## è¼¸å‡º ## [[1,2,3,4]] ## æ–¹æ³• ## æª¢æŸ¥å…©å€‹çŸ©é™£çš„è¡Œæ•¸èˆ‡åˆ—æ•¸ç›¸ä¹˜çš„çµæœæ˜¯å¦ç›¸åŒï¼Œè‹¥æˆç«‹ä»£è¡¨é€™å…©å€‹çŸ©é™£çš„å…ƒç´ å€‹æ•¸ç›¸åŒï¼Œä¹Ÿå°±æ˜¯å…©å€‹çŸ©é™£èƒ½å¤ äº’ç›¸è½‰æ›ã€‚ æ¥è‘—å°‡åŸæœ¬ 2D çš„çŸ©é™£è½‰æˆ 1D ï¼Œé€™æ¨£åœ¨è½‰æ›æˆå¦ä¸€å€‹å¤§å°çš„çŸ©é™£æ™‚æœƒæ¯”è¼ƒæ–¹ä¾¿ï¼Œåªè¦ä¾åºå–å‡ºå³å¯ã€‚ 1234567891011121314class Solution(object): def matrixReshape(self, nums, r, c): row = len(nums) col = len(nums[0]) if row * col == r * c: one_d_array = [nums[i][j] for i in range(row) for j in range(col)] ans = [[0 for j in range(c)] for i in range(r)] count_element = 0 for i in range(r): for j in range(c): ans[i][j] = one_d_array[count_element] count_element += 1 return ans return nums]]></content>
      <categories>
        <category>ç¨‹å¼è§£é¡Œ</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>Python</tag>
        <tag>Matrix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode#561 Array Partition I]]></title>
    <url>%2F2017%2F08%2F26%2FLeetCode-561-Array-Partition-I%2F</url>
    <content type="text"><![CDATA[å•é¡Œ çµ¦ä¸€å€‹é•·åº¦ç‚º 2n çš„æ•´æ•¸é™£åˆ—ï¼Œå°‡é™£åˆ—çš„å€¼å…©å…©é…å°ï¼Œä½¿å¾—æ‰€æœ‰ min(ai, bi) çš„å’Œæ„ˆå¤§æ„ˆå¥½ (0 &lt;= i &lt;= n) ã€‚ è¼¸å…¥ [1,4,3,2] è¼¸å‡º 4 æ–¹æ³• é€™é¡Œç›´è¦ºå°±æ˜¯å…ˆå°‡é™£åˆ—æ’åºä¹‹å¾Œå†å–å¥‡æ•¸é …ç›¸åŠ å°±å¥½ã€‚ sort: [1,2,3,4] min(1, 2) + min(3, 4) = 4 1234567class Solution(object): def arrayPairSum(self, nums): summation = 0 for i, v in enumerate(sorted(nums)): if i % 2 == 0: summation += v return summation åœ¨åº•ä¸‹è¨è«–ç‰ˆæ‰¾åˆ°äº†ä¸€è¡Œè§£æ±ºçš„ code ï¼Œå®Œå…¨é«”ç¾ python ç°¡ç´„çš„é¢¨æ ¼ï¼Œå¤ªç¥å•¦! 123class Solution(object): def arrayPairSum(self, nums): return sum(sorted(nums)[::2])]]></content>
      <categories>
        <category>ç¨‹å¼è§£é¡Œ</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>Python</tag>
        <tag>Sorting</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode#461 Hamming Distance]]></title>
    <url>%2F2017%2F08%2F26%2FLeetCode-461-Hamming-Distance%2F</url>
    <content type="text"><![CDATA[å•é¡Œ çµ¦å…©å€‹æ•´æ•¸xèˆ‡yï¼Œè¨ˆç®—å…©è€…çš„ Hamming Distance ã€‚ ## è¼¸å…¥ ## x = 1, y = 4 ## è¼¸å‡º ## 2 ## æ–¹æ³• ## æ‰€è¬‚çš„ Hamming Distance å°±æ˜¯å…©å€‹stringä¸­æœ‰å¹¾å€‹ç›¸ç•°çš„ä½å…ƒå€‹æ•¸ï¼Œèˆ‰å€‹ä¾‹å­ï¼š 1 = (0 0 0 1) 4 = (0 1 0 0) â†‘ â†‘ 1 è·Ÿ 4 æœ‰å…©å€‹ç›¸ç•°çš„ä½å…ƒå€‹æ•¸ï¼Œå› æ­¤ 1 è·Ÿ 4 çš„ Hamming Distance ç‚º 2 ã€‚ æˆ‘å€‘å¯ä»¥å¾ˆç›´è¦ºçš„æƒ³åˆ°å¯ä»¥ç”¨ exclusive or ç°¡å–®çš„è¨ˆç®—å‡ºä¾†ï¼Œä¹Ÿå°±æ˜¯å…©å€‹ bit ç›¸åŒæ™‚è¼¸å‡º 0 ï¼Œå…©å€‹ bit ç›¸ç•°æ™‚è¼¸å‡º 1 ï¼Œæœ€å¾Œå†è¨ˆç®—ç¸½å…±æœ‰å¹¾ä½æ•¸ç‚º 1 å³å¯ã€‚ 123class Solution(object): def hammingDistance(self, x, y): return bin(x^y).count("1")]]></content>
      <categories>
        <category>ç¨‹å¼è§£é¡Œ</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>Python</tag>
        <tag>Hamming Distance</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode#1 Two Sums]]></title>
    <url>%2F2017%2F08%2F26%2FLeetCode-1-Two-Sums%2F</url>
    <content type="text"><![CDATA[å•é¡Œ çµ¦å®šä¸€å€‹æ•´æ•¸çš„é™£åˆ—numsï¼Œå…¶ä¸­æœ‰å…©å€‹æ•¸çš„å’Œæœƒç­‰æ–¼ç›®æ¨™targetï¼Œå›å‚³ä»–å€‘çš„ä½ç½®ï¼Œä¸”åŒå€‹ä½ç½®ä¸èƒ½é‡è¤‡é¸å–ã€‚ è¼¸å…¥ nums = [2, 7, 11, 15] target = 9 è¼¸å‡º [0, 1] æ–¹æ³• Brute Force Hash Table ç¬¬ä¸€ç¨®æ–¹æ³•å°±æ˜¯ç›´æ¥ä½¿ç”¨æš´åŠ›æ³•ï¼ŒæŠŠé™£åˆ—è£¡é¢çš„å…ƒç´ éƒ½åŠ åŠ çœ‹ï¼Œä½†æ˜¯æ™‚é–“è¤‡é›œåº¦æœƒåˆ°O(n2)ã€‚ 123456class Solution(object): def twoSum(self, nums, target): for i in range(len(nums)): for j in range(i + 1, len(nums)): if nums[i] + nums[j] == target: return [i, j] å±…ç„¶åƒäº†ä¸€ç™¼TLEï¼Œæ²’æƒ³åˆ°ç¬¬ä¸€é¡Œå°±ç©çœŸçš„ï¼Œåªå¥½å¦å°‹ä»–æ³•äº†ã€‚ é€™æ™‚æƒ³åˆ°pythonä¸­ç›¸ç•¶å¥½ç”¨çš„dictionaryï¼Œç¬¬äºŒç¨®æ–¹æ³•å°±æ˜¯åˆ©ç”¨é›œæ¹Šè¡¨å°‡è®€åˆ°çš„ index åŠ value ç´€éŒ„ä¸‹ä¾†ï¼Œå³å¯åœ¨è®€å…¥ä¸€å€‹æ–°çš„æ•¸ä¹‹å¾ŒæŸ¥çœ‹é€™å€‹æ•¸çš„ complement æ˜¯å¦åœ¨ table ç•¶ä¸­ï¼Œé€™æ¨£å°±å¯ä»¥å®Œç¾æ‰¾åˆ°ä¸€çµ„è§£å•¦ã€‚ å› ç‚ºä½¿ç”¨äº†dictionaryï¼Œæ‰€ä»¥æŸ¥è¡¨çš„æ™‚é–“åªè¦O(1)ï¼Œæœ€å¾Œæ™‚é–“è¤‡é›œåº¦é™åˆ°äº†O(n)ï¼Œç•¶ç„¶ä¹Ÿå°±é †åˆ©é€šéäº†ï¼ 12345678class Solution(object): def twoSum(self, nums, target): dic = &#123;&#125; for i, n in enumerate(nums): if target - n in dic: return [dic[target - n], i] else: dic[n] = i]]></content>
      <categories>
        <category>ç¨‹å¼è§£é¡Œ</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>Hash</tag>
        <tag>Python</tag>
      </tags>
  </entry>
</search>
